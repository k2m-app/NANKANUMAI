import time
import re
import os
import json
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

# Selenium & Chrome
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# 1. è¨­å®š & å®šæ•°
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# Streamlit Secrets (æœªè¨­å®šæ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å¯¾å¿œ)
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# 2. å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    """HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"
    })
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def get_driver():
    """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®å®‰å…¨ãªèµ·å‹•"""
    ops = Options()
    ops.add_argument("--headless=new") # æœ€æ–°ã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
    ops.add_argument("--no-sandbox")
    ops.add_argument("--disable-dev-shm-usage") # ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼å›é¿
    ops.add_argument("--disable-gpu")
    ops.add_argument("--window-size=1280,1024")
    ops.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=ops)

def login_keibabook_robust(driver):
    """ç«¶é¦¬ãƒ–ãƒƒã‚¯ã¸ã®ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†"""
    try:
        driver.get("https://s.keibabook.co.jp/login/login")
        time.sleep(1)
        # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ãŒã‚ã‚Œã°ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã¨ã¿ãªã™
        if "logout" in driver.current_url or len(driver.find_elements(By.XPATH, "//a[contains(@href,'logout')]")) > 0:
            return True
            
        # ãƒ­ã‚°ã‚¤ãƒ³è©¦è¡Œ
        WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(2)
        return True
    except Exception as e:
        print(f"Login Warning: {e}")
        return False

# ==================================================
# 3. Dify APIé€£æº
# ==================================================
def run_dify_prediction(full_text):
    """Dify APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    if not DIFY_API_KEY: return "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
    
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {
        "inputs": {"text": full_text}, 
        "response_mode": "blocking", # ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚blockingæ¨å¥¨ã ãŒã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ³¨æ„
        "user": "keiba-bot"
    }
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}", 
        "Content-Type": "application/json"
    }
    
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        if resp.status_code == 200:
            data = resp.json()
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å‡ºåŠ›æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´
            outputs = data.get('data', {}).get('outputs', {})
            return outputs.get('text', str(outputs))
        else:
            return f"âš ï¸ API Error: {resp.status_code} - {resp.text[:100]}"
    except Exception as e:
        return f"âš ï¸ Connection Error: {e}"

# ==================================================
# 4. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
@st.cache_resource
def load_resources():
    """CSVãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    res = {"jockeys": [], "trainers": [], "power": {}, "power_data": {}}
    
    # é¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    for fpath, key in [(JOCKEY_FILE, "jockeys"), (TRAINER_FILE, "trainers")]:
        if os.path.exists(fpath):
            try:
                with open(fpath, "r", encoding="utf-8-sig") as f:
                    res[key] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
            except: pass
            
    # é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼èª­ã¿è¾¼ã¿
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            place_col = df.columns[0]
            for _, row in df.iterrows():
                p = str(row[place_col]).strip()
                j = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                if p and j:
                    val = row.get('é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼','-')
                    res["power"][(p, j)] = f"P:{val}"
                    res["power_data"][(p, j)] = {"power": val}
        except: pass
    return res

def normalize_name(abbrev, full_list):
    """ç•¥ç§°ã‹ã‚‰æ­£å¼åç§°ã‚’æ¤œç´¢"""
    if not abbrev or not full_list: return abbrev
    clean = re.sub(r"[ ã€€â–²â–³â˜†â—‡â˜…\d\.]+", "", abbrev)
    
    # å®Œå…¨ä¸€è‡´
    if clean in full_list: return clean
    
    # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
    candidates = []
    for full in full_list:
        if all(c in full for c in clean): # æ–‡å­—ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹
            candidates.append((len(full) - len(clean), full))
    
    if candidates:
        candidates.sort() # æ–‡å­—æ•°å·®ãŒå°ã•ã„é †
        return candidates[0][1]
        
    return clean

# ==================================================
# 5. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & è§£æã‚³ã‚¢
# ==================================================
def get_kb_url_id(year, month, day, place_code, nichi, race_num):
    return f"{year}{str(month).zfill(2)}{str(place_code).zfill(2)}{str(nichi).zfill(2)}{str(race_num).zfill(2)}{str(month).zfill(2)}{str(day).zfill(2)}"

def parse_nankankeiba_detail(html, place_name, resources):
    """å—é–¢ç«¶é¦¬ã®å‡ºé¦¬è¡¨HTMLã‚’è§£æ"""
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}
    
    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
    h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = h3.get_text(strip=True) if h3 else "ä¸æ˜"
    
    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data

    for row in table.select("tbody tr"):
        try:
            # é¦¬ç•ª
            u_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not u_tag: continue
            umaban = u_tag.get_text(strip=True)
            if not umaban.isdigit(): continue
            
            # é¦¬å
            h_link = row.select_one("td.is-col03 a.is-link")
            horse_name = h_link.get_text(strip=True) if h_link else "ä¸æ˜"
            
            # é¨æ‰‹ãƒ»èª¿æ•™å¸«
            j_full, t_full = "ä¸æ˜", "ä¸æ˜"
            jg_td = row.select_one("td.cs-g1")
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: j_full = normalize_name(links[0].get_text(strip=True), resources["jockeys"])
                if len(links) >= 2: t_full = normalize_name(links[1].get_text(strip=True), resources["trainers"])
            
            # é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼
            power_info = resources["power"].get((place_name, j_full), "P:ä¸æ˜")
            
            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            histories = []
            for i in range(1, 4):
                z = row.select_one(f"td.cs-z{i}")
                if z and z.get_text(strip=True):
                    # é †ä½ã‚„æ¡ä»¶ã ã‘æŠœã
                    rank = z.select_one(".nk23_u-text19")
                    rank_txt = rank.get_text(strip=True) if rank else "?"
                    histories.append(f"{i}èµ°å‰:{rank_txt}ç€")
            
            data["horses"][umaban] = {
                "name": horse_name,
                "jockey": j_full,
                "trainer": t_full,
                "power": power_info,
                "history": " ".join(histories)
            }
        except Exception: continue
        
    return data

# ==================================================
# 6. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼‰
# ==================================================
def run_races_iter(year, month, day, place_code, target_races):
    """
    UIã«ä¾å­˜ã›ãšã€çµæœã‚’è¾æ›¸å½¢å¼ã§yieldã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
    Returnå½¢å¼: {"type": "log"|"result"|"error", "data": ...}
    """
    driver = None
    resources = load_resources()
    
    # é–‹å‚¬åœ°ã‚³ãƒ¼ãƒ‰å¤‰æ›
    kb_place_map = {"10":"å¤§äº•", "11":"å·å´", "12":"èˆ¹æ©‹", "13":"æµ¦å’Œ"}
    nk_place_map = {"10":"20", "11":"21", "12":"19", "13":"18"}
    place_name = kb_place_map.get(place_code, "åœ°æ–¹")
    nk_code = nk_place_map.get(place_code)

    try:
        driver = get_driver()
        
        # 1. é–‹å‚¬ç‰¹å®š
        yield {"type": "log", "data": f"ğŸ“… {place_name}ã®é–‹å‚¬æ—¥ã‚’ç‰¹å®šä¸­..."}
        
        # å—é–¢ç«¶é¦¬ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰å›ãƒ»æ—¥æ¬¡ã‚’å–å¾—
        kai, nichi = None, None
        driver.get("https://www.nankankeiba.com/bangumi_menu/bangumi.do")
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        target_str = f"{int(month)}æœˆ"
        for tr in soup.find_all('tr'):
            txt = tr.get_text(" ", strip=True)
            if place_name in txt and target_str in txt:
                # ç°¡æ˜“çš„ãªåˆ¤å®šï¼ˆå®Ÿéš›ã¯æ—¥ä»˜ãƒãƒƒãƒãƒ³ã‚°ãŒå¿…è¦ã ãŒã€ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ç°¡ç•¥åŒ–ï¼‰
                # å®Ÿéš›ã«ã¯ã“ã“ã§å¯¾è±¡æ—¥ã®å›ãƒ»æ—¥æ¬¡ã‚’ç‰¹å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ãŒå…¥ã‚Šã¾ã™
                # ä»Šå›ã¯ä»®ã«è¨ˆç®—ã§ããŸã¨ã™ã‚‹ã‹ã€è©³ç´°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œ
                days_part = txt.split("æœˆ")[1]
                if str(int(day)) in days_part:
                     m_kai = re.search(r'ç¬¬(\d+)å›', txt)
                     if m_kai:
                         kai = int(m_kai.group(1))
                         # æ—¥æ¬¡ã®ç‰¹å®šã¯è¤‡é›‘ãªãŸã‚ã€ã“ã“ã§ã¯å®‰å…¨ç­–ã¨ã—ã¦ã€Œ1æ—¥ç›®ã€ã¨ä»®å®šã™ã‚‹ã‹
                         # ã¾ãŸã¯æ—¥ä»˜ãƒªã‚¹ãƒˆã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                         days_nums = re.findall(r'\d+', days_part)
                         if str(int(day)) in days_nums:
                             nichi = days_nums.index(str(int(day))) + 1
                         else: nichi = 1
                         break
        
        if not kai:
            # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯URLã‹ã‚‰æ¨æ¸¬ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆéæ¨å¥¨ã ãŒå‹•ãã‚ˆã†ã«ï¼‰
            kai, nichi = 1, 1 
            yield {"type": "log", "data": "âš ï¸ é–‹å‚¬å›ãŒç‰¹å®šã§ããªã„ãŸã‚ã€ç¬¬1å›1æ—¥ç›®ã¨ã—ã¦è©¦è¡Œã—ã¾ã™"}
        
        yield {"type": "log", "data": f"âœ… {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›® è¨­å®šå®Œäº†"}

        # 2. ãƒ­ã‚°ã‚¤ãƒ³
        login_keibabook_robust(driver)

        # 3. ãƒ¬ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ—
        r_nums = target_races if target_races else range(1, 13)
        
        for r_num in r_nums:
            yield {"type": "log", "data": f"ğŸ‡ {r_num}R ãƒ‡ãƒ¼ã‚¿åé›†ä¸­..."}
            
            try:
                # IDç”Ÿæˆ
                nk_id = f"{year}{month}{day}{nk_code}{kai:02}{nichi:02}{r_num:02}"
                
                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                driver.get(f"https://www.nankankeiba.com/uma_shosai/{nk_id}.do")
                nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)
                
                if not nk_data["horses"]:
                    yield {"type": "error", "data": f"{r_num}R: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (URL: {driver.current_url})"}
                    continue
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                lines = [f"ãƒ¬ãƒ¼ã‚¹: {r_num}R {nk_data['meta']['race_name']}"]
                for u, h in nk_data["horses"].items():
                    lines.append(f"[{u}] {h['name']} (é¨:{h['jockey']} P:{h['power']}) å±¥æ­´:{h['history']}")
                
                full_text = "\n".join(lines)
                
                # AIåˆ†æ
                yield {"type": "log", "data": f"ğŸ¤– {r_num}R AIåˆ†æå®Ÿè¡Œä¸­..."}
                ai_result = run_dify_prediction(full_text)
                
                # çµæœçµåˆ
                final_output = f"=== {r_num}R äºˆæƒ³ ===\n\n{ai_result}"
                
                # æˆåŠŸã¨ã—ã¦Yield
                yield {"type": "result", "race_num": r_num, "data": final_output}
                
                time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

            except Exception as e:
                yield {"type": "error", "data": f"{r_num}R å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"}
                
    except Exception as e:
        yield {"type": "error", "data": f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}"}
    finally:
        if driver:
            driver.quit()

# ==================================================
# 7. UI ãƒ¡ã‚¤ãƒ³å‡¦ç† (Streamlit)
# ==================================================
def main():
    st.set_page_config(page_title="NANKAN AI Robust", layout="wide")
    st.title("ğŸ‡ NANKAN AI (Stable Version)")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    st.sidebar.header("å®Ÿè¡Œè¨­å®š")
    d_now = datetime.now()
    year = st.sidebar.number_input("å¹´", value=d_now.year)
    month = st.sidebar.number_input("æœˆ", value=d_now.month)
    day = st.sidebar.number_input("æ—¥", value=d_now.day)
    place_code = st.sidebar.selectbox("é–‹å‚¬åœ°", ["10","11","12","13"], format_func=lambda x: {"10":"å¤§äº•","11":"å·å´","12":"èˆ¹æ©‹","13":"æµ¦å’Œ"}.get(x))
    
    races_str = st.sidebar.text_input("ãƒ¬ãƒ¼ã‚¹æŒ‡å®š (ä¾‹: 1,2,11 / ç©ºç™½ã§å…¨R)", "")
    
    # çµæœä¿å­˜ç”¨ï¼ˆãƒªãƒ­ãƒ¼ãƒ‰å¯¾ç­–ï¼‰
    if "results" not in st.session_state:
        st.session_state.results = {}
    
    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.sidebar.button("åˆ†æé–‹å§‹", type="primary"):
        target_races = [int(x.strip()) for x in races_str.split(",")] if races_str.strip() else []
        
        # ãƒ­ã‚°è¡¨ç¤ºã‚¨ãƒªã‚¢
        log_area = st.empty()
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿å®Ÿè¡Œ
        # ã“ã“ã§ã®å¤‰æ•° `event` ã¯è¾æ›¸å‹ {"type":..., "data":...} ãªã®ã§ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã¯èµ·ããªã„
        for event in run_races_iter(year, month, day, place_code, target_races):
            
            if event["type"] == "log":
                log_area.info(event["data"])
                
            elif event["type"] == "error":
                st.error(event["data"])
                
            elif event["type"] == "result":
                r_num = event["race_num"]
                res_text = event["data"]
                st.session_state.results[r_num] = res_text
                st.success(f"{r_num}R å®Œäº†")
                
        log_area.success("âœ¨ å…¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

    # çµæœè¡¨ç¤º
    st.divider()
    st.subheader("ğŸ“Š åˆ†æçµæœ")
    
    if st.session_state.results:
        # ãƒ¬ãƒ¼ã‚¹é †ã«ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
        for r in sorted(st.session_state.results.keys()):
            with st.expander(f"ğŸ {r}ãƒ¬ãƒ¼ã‚¹ã®çµæœ", expanded=False):
                st.text(st.session_state.results[r])
    else:
        st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
