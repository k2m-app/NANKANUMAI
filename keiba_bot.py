import time
import re
import os
import csv
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# ã€è¨­å®šã€‘ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# ==================================================
# ã€è¨­å®šã€‘ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ»API
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ / Difyé€£æº
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    sess = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def run_dify_prediction(full_text):
    """ Dify APIã«ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’é€ä¿¡ã—ã¦äºˆæƒ³ã‚’å–å¾—ã™ã‚‹ """
    if not DIFY_API_KEY: return "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
    
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {
        "inputs": {"text": full_text}, 
        "response_mode": "blocking", 
        "user": "keiba-bot"
    }
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    sess = get_http_session()
    try:
        res = sess.post(url, headers=headers, json=payload, timeout=90)
        if res.status_code != 200: return f"âš ï¸ Dify Error: {res.status_code}"
        j = res.json()
        return j.get("data", {}).get("outputs", {}).get("text", "") or str(j)
    except Exception as e: return f"âš ï¸ API Error: {e}"

# ==================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–
# ==================================================
@st.cache_resource
def load_resources():
    res = {"jockeys": [], "trainers": [], "power": {}}
    if os.path.exists(JOCKEY_FILE):
        try:
            with open(JOCKEY_FILE, "r", encoding="utf-8-sig") as f:
                res["jockeys"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    if os.path.exists(TRAINER_FILE):
        try:
            with open(TRAINER_FILE, "r", encoding="utf-8-sig") as f:
                res["trainers"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            place_col = df.columns[0]
            for _, row in df.iterrows():
                p = str(row[place_col]).strip()
                j = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                if p and j:
                    info = f"é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:{row.get('é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼','-')}(å‹ç‡{row.get('å‹ç‡','-')} è¤‡å‹ç‡{row.get('è¤‡å‹ç‡','-')})"
                    res["power"][(p, j)] = info
        except: pass
    return res

def normalize_name(abbrev, full_list):
    if not abbrev: return ""
    clean = abbrev.replace(" ","").replace("ã€€","")
    if not full_list: return clean
    if clean in full_list: return clean
    matches = [n for n in full_list if n.startswith(clean) or (len(clean)>=2 and n.startswith(clean[0]) and clean[1] in n)]
    return sorted(matches, key=len)[0] if matches else clean

# ==================================================
# nankankeiba & KeibaBook è§£æ
# ==================================================
def parse_nankankeiba_detail(html, place_name, resources):
    """ nankankeibaè©³ç´°å‡ºèµ°è¡¨ã‹ã‚‰ã€åŸºæœ¬æƒ…å ±ãƒ»ç›¸æ€§ãƒ»è¿‘èµ°3èµ°ã‚’å–å¾— """
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}

    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
    h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = h3.get_text(strip=True) if h3 else ""
    if data["meta"]["race_name"]:
        parts = re.split(r'[ ã€€]+', data["meta"]["race_name"])
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    
    cond = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    data["meta"]["course"] = f"{place_name} {cond.get_text(strip=True)}" if cond else ""

    # é¦¬ãƒ‡ãƒ¼ã‚¿
    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data

    for row in table.select("tbody tr"):
        try:
            # é¦¬ç•ªãƒ»é¦¬å
            u_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not u_tag: continue
            umaban = u_tag.get_text(strip=True)
            if not umaban.isdigit(): continue
            
            h_tag = row.select_one("td.is-col03 a.is-link")
            horse_name = h_tag.get_text(strip=True) if h_tag else ""

            # é¨æ‰‹ãƒ»èª¿æ•™å¸«
            jg_td = row.select_one("td.cs-g1")
            j_raw, t_raw = "", ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: j_raw = links[0].get_text(strip=True)
                if len(links) >= 2: t_raw = links[1].get_text(strip=True)
            
            j_full = normalize_name(j_raw, resources["jockeys"])
            t_full = normalize_name(t_raw, resources["trainers"])
            power = resources["power"].get((place_name, j_full), "é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:ä¸æ˜")

            # ç›¸æ€§ (é¨æ‰‹xèª¿æ•™å¸«)
            ai2 = row.select_one("td.cs-ai2 .graph_text_div")
            pair_stats = "ãƒ‡ãƒ¼ã‚¿ãªã—"
            if ai2 and "ãƒ‡ãƒ¼ã‚¿" not in ai2.get_text():
                r = ai2.select_one(".is-percent").get_text(strip=True)
                w = ai2.select_one(".is-number").get_text(strip=True)
                t = ai2.select_one(".is-total").get_text(strip=True)
                pair_stats = f"å‹ç‡{r}({w}å‹/{t}å›)"

            # è¿‘èµ°3èµ°
            history = []
            for i in range(1, 4):
                z = row.select_one(f"td.cs-z{i}")
                if not z or not z.get_text(strip=True): continue
                
                # æ—¥ä»˜ãƒ»å ´æ‰€ãƒ»æ¡ä»¶
                d_txt = ""
                d_spans = z.select("p.nk23_u-d-flex span.nk23_u-text10")
                if d_spans:
                    for s in d_spans:
                        if re.search(r"\d+\.\d+\.\d+", s.get_text()): d_txt = s.get_text(strip=True); break
                
                # å¹´æœˆæ—¥å¤‰æ›
                ymd = ""
                m = re.match(r"([^\d]+)(\d+)\.(\d+)\.(\d+)", d_txt)
                if m:
                    place_short = m.group(1)
                    ymd = f"20{m.group(2)}/{int(m.group(3)):02}/{int(m.group(4)):02}"
                
                cond_txt = d_spans[-1].get_text(strip=True) if len(d_spans)>=2 else ""
                dist_m = re.search(r"\d{4}", cond_txt)
                dist = dist_m.group(0) if dist_m else ""
                course_s = f"{place_short}{dist}m" if m else cond_txt

                # ãƒ¬ãƒ¼ã‚¹åãƒ»ã‚¯ãƒ©ã‚¹
                r_a = z.select_one("a.is-link")
                r_ti = r_a.get("title", "") if r_a else ""
                rp = re.split(r'[ ã€€]+', r_ti)
                r_nm = rp[0] if rp else ""
                r_cl = rp[1] if len(rp)>1 else ""

                # é¨æ‰‹ãƒ»äººæ°—ãƒ»ç€é †ãƒ»é€šéãƒ»ä¸ŠãŒã‚Š
                p_lines = z.select("p.nk23_u-text10")
                j_prev, pop, agari = "", "", ""
                rank = z.select_one(".nk23_u-text19").get_text(strip=True).replace("ç€","") if z.select_one(".nk23_u-text19") else ""
                
                pos_p = z.select_one("p.position")
                pas = "-".join([s.get_text(strip=True) for s in pos_p.find_all("span")]) if pos_p else ""

                for p in p_lines:
                    pt = p.get_text(strip=True)
                    if "äººæ°—" in pt:
                        pm = re.search(r"(\d+)äººæ°—", pt)
                        if pm: pop = f"{pm.group(1)}äººæ°—"
                        sps = p.find_all("span")
                        if len(sps)>1: j_prev = re.sub(r"[\d\.]+", "", sps[1].get_text(strip=True))
                    if "3F" in pt:
                        am = re.search(r"\(([\d]+)\)", pt)
                        if am: agari = f"ä¸ŠãŒã‚Š3F:{am.group(1)}ä½"

                h_str = f"é–‹å‚¬æ—¥ï¼š{ymd}ã€€ã‚³ãƒ¼ã‚¹ï¼š{course_s} ãƒ¬ãƒ¼ã‚¹ï¼š{r_nm}ã€€ã‚¯ãƒ©ã‚¹ï¼š{r_cl} é¨æ‰‹ï¼š{j_prev}ã€€é€šéé †{pas}({agari})â†’{rank}ç€ï¼ˆ{pop}ï¼‰"
                history.append(h_str)

            data["horses"][umaban] = {
                "name": horse_name, "jockey": j_full, "trainer": t_full,
                "power": power, "compat": pair_stats, "hist": history
            }
        except: continue
    return data

def parse_kb_danwa_cyokyo(driver, kb_rid):
    """ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‹ã‚‰è«‡è©±ã¨èª¿æ•™ã‚’ä¸€æ‹¬å–å¾— """
    d_danwa, d_cyokyo = {}, {}
    try:
        driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_rid}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        tbl = soup.find("table", class_="danwa")
        if tbl and tbl.tbody:
            cur = None
            for row in tbl.tbody.find_all("tr"):
                u = row.find("td", class_="umaban")
                if u: cur = u.get_text(strip=True); continue
                t = row.find("td", class_="danwa")
                if t and cur: d_danwa[cur] = t.get_text(strip=True); cur=None
        
        driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_rid}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for tbl in soup.find_all("table", class_="cyokyo"):
            if not tbl.tbody: continue
            rs = tbl.tbody.find_all("tr", recursive=False)
            if not rs: continue
            u_td = rs[0].find("td", class_="umaban")
            if u_td:
                u = u_td.get_text(strip=True)
                tp = rs[0].find("td", class_="tanpyo").get_text(strip=True) if rs[0].find("td", class_="tanpyo") else ""
                dt = rs[1].get_text(" ", strip=True) if len(rs)>1 else ""
                d_cyokyo[u] = f"ã€çŸ­è©•ã€‘{tp} ã€è©³ç´°ã€‘{dt}"
    except: pass
    return d_danwa, d_cyokyo

# ==================================================
# å¯¾æˆ¦è¡¨ & è©•ä¾¡è§£æ (å¾©æ´»)
# ==================================================
def _parse_grades_from_ai(text):
    """ AIã®å›ç­”ã‹ã‚‰é¦¬ã®è©•ä¾¡(S/A/B...)ã‚’æŠ½å‡ºã™ã‚‹ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯ """
    grades = {}
    # è¡Œã”ã¨ã« "â—é¦¬å" ã‚„ "S é¦¬å" ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
    lines = text.split('\n')
    for line in lines:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: [S] é¦¬å or è©•ä¾¡:S é¦¬å ãªã©
        m = re.search(r'([SABCDE])\s*[:ï¼š]?\s*([^\sã€€]+)', line)
        if m:
            grade, name = m.group(1), m.group(2)
            # é¦¬åã‹ã‚‰æ‹¬å¼§ãªã©ã‚’é™¤å»
            name = re.sub(r'[ï¼ˆ\(].*?[ï¼‰\)]', '', name).strip()
            if name: grades[name] = grade
    return grades

def _fetch_matchup_table(nankan_id, grades):
    """ nankankeibaã®å¯¾æˆ¦è¡¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã€AIè©•ä¾¡å°ã‚’ä»˜ä¸ã—ã¦æ•´å½¢ """
    url = f"https://www.nankankeiba.com/taisen/{nankan_id}.do"
    sess = get_http_session()
    try:
        res = sess.get(url, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        tbl = soup.find('table', class_='nk23_c-table08__table')
        if not tbl: return "\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—)"

        races = []
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹åå–å¾—
        if tbl.find('thead') and tbl.find('thead').find('tr'):
            for col in tbl.find('thead').find('tr').find_all(['th','td'])[2:]:
                det = col.find(class_='nk23_c-table08__detail')
                if det:
                    link = col.find('a')
                    races.append({
                        "title": det.get_text(" ", strip=True),
                        "url": "https://www.nankankeiba.com" + link.get('href','') if link else "",
                        "results": []
                    })
        
        if not races: return "\n(åˆå¯¾æˆ¦)"

        # ãƒœãƒ‡ã‚£ã‹ã‚‰ç€é †å–å¾—
        if tbl.find('tbody'):
            for tr in tbl.find('tbody').find_all('tr'):
                u_link = tr.find('a', class_='nk23_c-table08__text')
                if not u_link: continue
                
                horse_name = u_link.get_text(strip=True)
                # AIè©•ä¾¡ã‚’å–å¾— (å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´)
                grade = grades.get(horse_name, "")
                if not grade:
                    # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
                    for k, v in grades.items():
                        if k in horse_name or horse_name in k:
                            grade = v; break
                
                cells = tr.find_all(['td','th'])
                # é¦¬åã‚»ãƒ«ã®æ¬¡ã‹ã‚‰ãŒãƒ¬ãƒ¼ã‚¹çµæœ
                start_idx = -1
                for idx, c in enumerate(cells):
                    if c.find('a', class_='nk23_c-table08__text'): start_idx=idx; break
                
                if start_idx == -1: continue

                for i, cell in enumerate(cells[start_idx+1:]):
                    if i >= len(races): break
                    rank_p = cell.find('p', class_='nk23_c-table08__number')
                    rank = ""
                    if rank_p:
                        sp = rank_p.find('span')
                        rank = sp.get_text(strip=True) if sp else rank_p.get_text(strip=True).split('ï½œ')[0].strip()
                    
                    if rank and (rank.isdigit() or rank in ['é™¤å¤–','ä¸­æ­¢']):
                        races[i]["results"].append({
                            "rank": rank, "name": horse_name, "grade": grade,
                            "sort": int(rank) if rank.isdigit() else 999
                        })

        # å‡ºåŠ›ç”Ÿæˆ
        out = ["\nã€å¯¾æˆ¦è¡¨ï¼ˆAIè©•ä¾¡ä»˜ãï¼‰ã€‘"]
        has_data = False
        for r in races:
            if not r["results"]: continue
            has_data = True
            r["results"].sort(key=lambda x: x["sort"])
            # 1ç€ é¦¬å(S) / 2ç€ é¦¬å(A)...
            line_parts = []
            for x in r["results"]:
                g_str = f"[{x['grade']}]" if x['grade'] else ""
                line_parts.append(f"{x['rank']}ç€ {x['name']}{g_str}")
            
            out.append(f"â—† {r['title']}")
            out.append(" / ".join(line_parts))
            out.append(f"è©³ç´°: {r['url']}\n")
            
        return "\n".join(out) if has_data else "\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—)"

    except Exception as e: return f"(å¯¾æˆ¦è¡¨ã‚¨ãƒ©ãƒ¼: {e})"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    # 1. æº–å‚™
    resources = load_resources()
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°
    kb_place_map = {"10":"å¤§äº•", "11":"å·å´", "12":"èˆ¹æ©‹", "13":"æµ¦å’Œ"}
    nk_place_map = {"10":"20", "11":"21", "12":"19", "13":"18"} # KB -> Nankan
    
    place_name = kb_place_map.get(place_code, "åœ°æ–¹")
    nk_place_code = nk_place_map.get(place_code)

    if not nk_place_code: yield (0, "âš ï¸ å ´æ‰€ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼"); return

    # Selenium
    ops = Options()
    ops.add_argument("--headless=new")
    ops.add_argument("--no-sandbox")
    ops.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=ops)
    wait = WebDriverWait(driver, 10)

    try:
        # 2. ãƒ­ã‚°ã‚¤ãƒ³
        if ui: st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        if "logout" not in driver.current_url:
            wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
            driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
            driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
            time.sleep(1)

        # 3. ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆå–å¾— (nankankeibaã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ãŒç¢ºå®Ÿ)
        # URL: https://www.nankankeiba.com/program/YYYYMMDDpp.do
        prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nk_place_code}.do"
        if ui: st.info(f"ğŸ“… é–‹å‚¬æƒ…å ±å–å¾—: {prog_url}")
        driver.get(prog_url)
        
        # ãƒªãƒ³ã‚¯ã‹ã‚‰IDæŠ½å‡º (ID: YYYYMMDDppKkDDRR)
        soup_prog = BeautifulSoup(driver.page_source, "html.parser")
        race_list = [] # (race_num, full_nankan_id)
        
        # "race_one" ãªã©ã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        for a in soup_prog.find_all("a", href=True):
            # hrefã«æ—¥ä»˜ã¨å ´æ‰€ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
            if f"{year}{month}{day}{nk_place_code}" in a["href"] and "uma_shosai" not in a["href"]:
                # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ä¸€è¦§å†…ã®ãƒªãƒ³ã‚¯ (ä¾‹: .../2026012119100301.do)
                # ãƒ•ã‚¡ã‚¤ãƒ«åéƒ¨åˆ†ã‚’æŠ½å‡º
                fname = a["href"].split("/")[-1].replace(".do","")
                if fname.isdigit() and len(fname) == 16:
                    r_num = int(fname[14:16])
                    race_list.append((r_num, fname))
        
        # é‡è¤‡æ’é™¤ã—ã¦ã‚½ãƒ¼ãƒˆ
        race_list = sorted(list(set(race_list)))
        
        if not race_list: yield (0, "âš ï¸ ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"); return

        # 4. å„ãƒ¬ãƒ¼ã‚¹å‡¦ç†
        for r_num, nk_id in race_list:
            if target_races and r_num not in target_races: continue
            
            if ui: st.markdown(f"## {place_name} {r_num}R")
            
            try:
                # A. KeibaBook IDç”Ÿæˆ (YYYYMMDD + KB_Place + RR)
                kb_id = f"{year}{month}{day}{place_code}{str(r_num).zfill(2)}"
                
                # B. KeibaBookãƒ‡ãƒ¼ã‚¿ (è«‡è©±ãƒ»èª¿æ•™)
                danwa, cyokyo = parse_kb_danwa_cyokyo(driver, kb_id)
                
                # C. Nankanãƒ‡ãƒ¼ã‚¿ (è©³ç´°å‡ºèµ°è¡¨)
                nk_url = f"https://www.nankankeiba.com/uma_shosai/{nk_id}.do"
                driver.get(nk_url)
                nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)
                
                # D. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                header = f"ãƒ¬ãƒ¼ã‚¹å: {r_num}R {nk_data['meta'].get('race_name','')}ã€€æ ¼ä»˜ã‘:{nk_data['meta'].get('grade','')}ã€€ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                
                horse_texts = []
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h = nk_data["horses"][u]
                    
                    # å‰èµ°é¨æ‰‹
                    prev_j = ""
                    if h["hist"]:
                        m = re.search(r"é¨æ‰‹ï¼š([^ã€€\s]+)", h["hist"][0])
                        if m: prev_j = m.group(1)
                    p_info = f" (å‰èµ°:{prev_j})" if prev_j else ""
                    
                    # ãƒ–ãƒ­ãƒƒã‚¯ä½œæˆ
                    lines = [
                        f"[é¦¬ç•ª{u}] {h['name']} é¨æ‰‹:{h['jockey']}{p_info} èª¿æ•™å¸«:{h['trainer']}",
                        f"è«‡è©±: {danwa.get(u,'ãªã—')} èª¿æ•™:{cyokyo.get(u,'èª¿æ•™ãƒ‡ãƒ¼ã‚¿ãªã—')}",
                        f"ã€é¨æ‰‹ã€‘{h['power']} ç›¸æ€§:{h['compat']}"
                    ]
                    
                    cn = {0:"â‘ ", 1:"â‘¡", 2:"â‘¢"}
                    for idx, his in enumerate(h["hist"]):
                        lines.append(f"ã€è¿‘èµ°ã€‘{cn.get(idx,'')} {his}")
                    
                    horse_texts.append("\n".join(lines))
                
                full_prompt = header + "\n\n" + "\n\n".join(horse_texts)
                
                # E. Difyé€ä¿¡
                if ui: st.info("ğŸ¤– AIåˆ†æä¸­...")
                ai_output = run_dify_prediction(full_prompt)
                
                # F. å¯¾æˆ¦è¡¨ä½œæˆ (AIã®è©•ä¾¡ã‚’ä½¿ã£ã¦)
                # AIã®å›ç­”ã‹ã‚‰è©•ä¾¡(S/A/B)ã‚’æŠ½å‡º
                grades = _parse_grades_from_ai(ai_output)
                # nankankeibaã®å¯¾æˆ¦è¡¨URLã¯IDã¨åŒã˜
                matchup_text = _fetch_matchup_table(nk_id, grades)
                
                # G. æœ€çµ‚å‡ºåŠ›
                final_res = f"ğŸ“… {year}/{month}/{day} {place_name}{r_num}R\n\n=== ğŸ¤–AIäºˆæƒ³ ===\n{ai_output}\n\n{matchup_text}\n\n=== ğŸ“Šåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ(å‚è€ƒ) ===\n{full_prompt[:300]}..."
                
                if ui: st.success("âœ… å®Œäº†")
                yield (r_num, final_res)
                time.sleep(2)

            except Exception as e:
                yield (r_num, f"Error: {e}")
    
    finally:
        driver.quit()
