import time
import re
import os
import csv
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# ã€è¨­å®šã€‘ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# ==================================================
# ã€è¨­å®šã€‘ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ»API
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ / Difyé€£æº
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    sess = requests.Session()
    # ç«¶é¦¬ãƒ–ãƒƒã‚¯ãªã©ã¯ã‚¹ãƒãƒ›ã‚µã‚¤ãƒˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚UAã‚’å½è£…
    sess.headers.update({
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"
    })
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def run_dify_prediction(full_text):
    """ Dify APIã«ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’é€ä¿¡ã—ã¦äºˆæƒ³ã‚’å–å¾—ã™ã‚‹ """
    if not DIFY_API_KEY: return "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
    
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {
        "inputs": {"text": full_text}, 
        "response_mode": "blocking", 
        "user": "keiba-bot"
    }
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    sess = get_http_session()
    try:
        # æ¨è«–ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«
        res = sess.post(url, headers=headers, json=payload, timeout=120)
        if res.status_code != 200: return f"âš ï¸ Dify Error: {res.status_code} {res.text}"
        j = res.json()
        return j.get("data", {}).get("outputs", {}).get("text", "") or str(j)
    except Exception as e: return f"âš ï¸ API Error: {e}"

# ==================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–
# ==================================================
@st.cache_resource
def load_resources():
    res = {"jockeys": [], "trainers": [], "power": {}}
    
    # é¨æ‰‹ãƒªã‚¹ãƒˆ
    if os.path.exists(JOCKEY_FILE):
        try:
            with open(JOCKEY_FILE, "r", encoding="utf-8-sig") as f:
                res["jockeys"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    
    # èª¿æ•™å¸«ãƒªã‚¹ãƒˆ
    if os.path.exists(TRAINER_FILE):
        try:
            with open(TRAINER_FILE, "r", encoding="utf-8-sig") as f:
                res["trainers"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    
    # é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            place_col = df.columns[0]
            for _, row in df.iterrows():
                p = str(row[place_col]).strip()
                j = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                if p and j:
                    info = f"é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:{row.get('é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼','-')}(å‹ç‡{row.get('å‹ç‡','-')} è¤‡å‹ç‡{row.get('è¤‡å‹ç‡','-')})"
                    res["power"][(p, j)] = info
        except: pass
    return res

def normalize_name(abbrev, full_list):
    """ åå¯„ã›ãƒ­ã‚¸ãƒƒã‚¯ """
    if not abbrev: return ""
    clean = abbrev.replace(" ","").replace("ã€€","")
    if not full_list: return clean
    if clean in full_list: return clean
    # å‰æ–¹ä¸€è‡´ã§å€™è£œã‚’æ¢ã™ (ä¾‹: æœ¨é–“é¾ -> æœ¨é–“å¡šé¾é¦¬)
    matches = [n for n in full_list if n.startswith(clean) or (len(clean)>=2 and n.startswith(clean[0]) and clean[1] in n)]
    return sorted(matches, key=len)[0] if matches else clean

# ==================================================
# é–‹å‚¬å›ãƒ»æ—¥æ¬¡ ç‰¹å®šãƒ­ã‚¸ãƒƒã‚¯ (nankankeiba)
# ==================================================
def get_nankan_kai_nichi(month, day, place_name):
    """ 
    nankankeibaã®ç•ªçµ„è¡¨ã‹ã‚‰ã€Œç¬¬ã€‡å›ãƒ»ã€‡æ—¥ç›®ã€ã‚’ç‰¹å®š
    â€»ã“ã®ã€Œã€‡æ—¥ç›®(nichi)ã€ã¯ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®URLç”Ÿæˆã«ã‚‚ä½¿ç”¨ã—ã¾ã™
    """
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    sess = get_http_session()
    try:
        res = sess.get(url, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        target_m = int(month)
        target_d = int(day)
        
        for tr in soup.find_all('tr'):
            text = tr.get_text(" ", strip=True)
            if place_name not in text: continue
            
            # å›æ•°ã®ç‰¹å®š
            kai_match = re.search(r'ç¬¬\s*(\d+)\s*å›', text)
            if not kai_match: continue
            kai = int(kai_match.group(1))
            
            # æœˆã®ç‰¹å®š
            m_match = re.search(r'(\d+)\s*æœˆ', text)
            if not m_match: continue
            if int(m_match.group(1)) != target_m: continue
            
            # æ—¥ä»˜ãƒªã‚¹ãƒˆæŠ½å‡º (ä¾‹: 19, 20, 21...)
            if "æœˆ" in text:
                days_part = text.split("æœˆ")[1]
                days_match = re.findall(r'(\d+)', days_part)
                # å¦¥å½“ãªæ—¥ä»˜ã®ã¿ãƒªã‚¹ãƒˆåŒ–
                days_list = [int(d) for d in days_match if 1 <= int(d) <= 31]
                
                if target_d in days_list:
                    nichi = days_list.index(target_d) + 1
                    return kai, nichi
        return None, None
    except: return None, None

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ è§£æãƒ­ã‚¸ãƒƒã‚¯ (URLè¨ˆç®—ãƒ»HTMLè§£æ)
# ==================================================
def get_kb_url_id(year, month, day, place_code, nichi, race_num):
    """
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®URL IDã‚’è¨ˆç®—ã§ç”Ÿæˆ
    Format: YYYY(4) + MM(2) + Place(2) + Nichi(2) + Race(2) + MMDD(4)
    """
    mm = str(month).zfill(2)
    dd = str(day).zfill(2)
    p_code = str(place_code).zfill(2)
    n_code = str(nichi).zfill(2)
    r_code = str(race_num).zfill(2)
    
    return f"{year}{mm}{p_code}{n_code}{r_code}{mm}{dd}"

def parse_kb_danwa_cyokyo(driver, kb_id):
    """ 
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‹ã‚‰è«‡è©±ãƒ»èª¿æ•™ã‚’å–å¾— 
    """
    d_danwa, d_cyokyo = {}, {}
    
    # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ç¶­æŒã®ãŸã‚ã®ãƒªãƒˆãƒ©ã‚¤
    def ensure_login():
        if "login" in driver.current_url:
            try:
                driver.find_element(By.NAME, "login_id").send_keys(KEIBA_ID)
                driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
                driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
                time.sleep(1)
            except: pass

    try:
        # --- è«‡è©± (Danwa) ---
        url_danwa = f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_id}"
        driver.get(url_danwa)
        ensure_login() # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¯¾å¿œ
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        # table class="default danwa" ã‚’æ¢ã™
        for tbl in soup.select("table.danwa"):
            current_horse = None
            for tr in tbl.select("tbody tr"):
                # é¦¬ç•ªè¡Œ
                u_td = tr.select_one("td.umaban")
                if u_td:
                    current_horse = u_td.get_text(strip=True)
                    continue
                
                # è«‡è©±è¡Œ
                t_td = tr.select_one("td.danwa")
                if current_horse and t_td:
                    # <p>ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    text = t_td.get_text(strip=True)
                    d_danwa[current_horse] = text
                    current_horse = None # ãƒªã‚»ãƒƒãƒˆ

        # --- èª¿æ•™ (Cyokyo) ---
        url_cyokyo = f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_id}"
        driver.get(url_cyokyo)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # è¤‡æ•°ã® table class="default cyokyo" ãŒã‚ã‚‹
        for tbl in soup.select("table.cyokyo"):
            rows = tbl.select("tbody tr")
            if not rows: continue
            
            # 1è¡Œç›®: åŸºæœ¬æƒ…å ±
            r1 = rows[0]
            u_td = r1.select_one("td.umaban")
            if not u_td: continue
            
            uma_num = u_td.get_text(strip=True)
            tanpyo = ""
            tp_td = r1.select_one("td.tanpyo")
            if tp_td: tanpyo = tp_td.get_text(strip=True)
            
            # 2è¡Œç›®ä»¥é™: è©³ç´° (dlã‚„nested table)
            detail_text = ""
            if len(rows) > 1:
                # 2è¡Œç›®ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã¾ã‚‹ã”ã¨å–å¾—ã—ã¦æ•´å½¢
                raw_text = rows[1].get_text(" ", strip=True)
                # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                detail_text = re.sub(r'\s+', ' ', raw_text)
            
            d_cyokyo[uma_num] = f"ã€çŸ­è©•ã€‘{tanpyo} ã€è©³ç´°ã€‘{detail_text}"

    except Exception as e:
        print(f"KB Parse Error: {e}")
        
    return d_danwa, d_cyokyo

# ==================================================
# nankankeiba è©³ç´°è§£æ
# ==================================================
def parse_nankankeiba_detail(html, place_name, resources):
    """ nankankeibaè©³ç´°å‡ºèµ°è¡¨è§£æ """
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}

    h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = h3.get_text(strip=True) if h3 else ""
    if data["meta"]["race_name"]:
        parts = re.split(r'[ ã€€]+', data["meta"]["race_name"])
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    
    cond = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    data["meta"]["course"] = f"{place_name} {cond.get_text(strip=True)}" if cond else ""

    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data

    for row in table.select("tbody tr"):
        try:
            u_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not u_tag: continue
            umaban = u_tag.get_text(strip=True)
            if not umaban.isdigit(): continue
            
            h_tag = row.select_one("td.is-col03 a.is-link")
            horse_name = h_tag.get_text(strip=True) if h_tag else ""

            jg_td = row.select_one("td.cs-g1")
            j_raw, t_raw = "", ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: j_raw = links[0].get_text(strip=True)
                if len(links) >= 2: t_raw = links[1].get_text(strip=True)
            
            j_full = normalize_name(j_raw, resources["jockeys"])
            t_full = normalize_name(t_raw, resources["trainers"])
            power = resources["power"].get((place_name, j_full), "é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:ä¸æ˜")

            # ç›¸æ€§ãƒ‡ãƒ¼ã‚¿
            ai2 = row.select_one("td.cs-ai2 .graph_text_div")
            pair_stats = "ãƒ‡ãƒ¼ã‚¿ãªã—"
            if ai2 and "ãƒ‡ãƒ¼ã‚¿" not in ai2.get_text():
                r = ai2.select_one(".is-percent").get_text(strip=True)
                w = ai2.select_one(".is-number").get_text(strip=True)
                t = ai2.select_one(".is-total").get_text(strip=True)
                pair_stats = f"å‹ç‡{r}({w}å‹/{t}å›)"

            # è¿‘èµ°3èµ°
            history = []
            for i in range(1, 4):
                z = row.select_one(f"td.cs-z{i}")
                if not z or not z.get_text(strip=True): continue
                
                # æ—¥ä»˜ãƒ»å ´æ‰€
                d_txt = ""
                d_spans = z.select("p.nk23_u-d-flex span.nk23_u-text10")
                if d_spans:
                    for s in d_spans:
                        if re.search(r"\d+\.\d+\.\d+", s.get_text()): d_txt = s.get_text(strip=True); break
                
                ymd = ""
                m = re.match(r"([^\d]+)(\d+)\.(\d+)\.(\d+)", d_txt)
                if m:
                    place_short = m.group(1)
                    ymd = f"20{m.group(2)}/{int(m.group(3)):02}/{int(m.group(4)):02}"
                
                cond_txt = d_spans[-1].get_text(strip=True) if len(d_spans)>=2 else ""
                dist_m = re.search(r"\d{4}", cond_txt)
                dist = dist_m.group(0) if dist_m else ""
                course_s = f"{place_short}{dist}m" if m else cond_txt

                r_a = z.select_one("a.is-link")
                r_ti = r_a.get("title", "") if r_a else ""
                rp = re.split(r'[ ã€€]+', r_ti)
                r_nm = rp[0] if rp else ""
                r_cl = rp[1] if len(rp)>1 else ""

                p_lines = z.select("p.nk23_u-text10")
                j_prev, pop, agari = "", "", ""
                rank = z.select_one(".nk23_u-text19").get_text(strip=True).replace("ç€","") if z.select_one(".nk23_u-text19") else ""
                
                pos_p = z.select_one("p.position")
                pas = "-".join([s.get_text(strip=True) for s in pos_p.find_all("span")]) if pos_p else ""

                for p in p_lines:
                    pt = p.get_text(strip=True)
                    if "äººæ°—" in pt:
                        pm = re.search(r"(\d+)äººæ°—", pt)
                        if pm: pop = f"{pm.group(1)}äººæ°—"
                        sps = p.find_all("span")
                        if len(sps)>1: j_prev = re.sub(r"[\d\.]+", "", sps[1].get_text(strip=True))
                    if "3F" in pt:
                        am = re.search(r"\(([\d]+)\)", pt)
                        if am: agari = f"ä¸ŠãŒã‚Š3F:{am.group(1)}ä½"

                h_str = f"é–‹å‚¬æ—¥ï¼š{ymd}ã€€ã‚³ãƒ¼ã‚¹ï¼š{course_s} ãƒ¬ãƒ¼ã‚¹ï¼š{r_nm}ã€€ã‚¯ãƒ©ã‚¹ï¼š{r_cl} é¨æ‰‹ï¼š{j_prev}ã€€é€šéé †{pas}({agari})â†’{rank}ç€ï¼ˆ{pop}ï¼‰"
                history.append(h_str)

            data["horses"][umaban] = {
                "name": horse_name, "jockey": j_full, "trainer": t_full,
                "power": power, "compat": pair_stats, "hist": history
            }
        except: continue
    return data

# ==================================================
# å¯¾æˆ¦è¡¨ & è©•ä¾¡è§£æ
# ==================================================
def _parse_grades_from_ai(text):
    grades = {}
    lines = text.split('\n')
    for line in lines:
        m = re.search(r'([SABCDE])\s*[:ï¼š]?\s*([^\sã€€]+)', line)
        if m:
            grade, name = m.group(1), m.group(2)
            name = re.sub(r'[ï¼ˆ\(].*?[ï¼‰\)]', '', name).strip()
            if name: grades[name] = grade
    return grades

def _fetch_matchup_table(nankan_id, grades):
    url = f"https://www.nankankeiba.com/taisen/{nankan_id}.do"
    sess = get_http_session()
    try:
        res = sess.get(url, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        tbl = soup.find('table', class_='nk23_c-table08__table')
        if not tbl: return "\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—)"

        races = []
        if tbl.find('thead'):
            for col in tbl.find('thead').find_all(['th','td'])[2:]:
                det = col.find(class_='nk23_c-table08__detail')
                if det:
                    link = col.find('a')
                    races.append({
                        "title": det.get_text(" ", strip=True),
                        "url": "https://www.nankankeiba.com" + link.get('href','') if link else "",
                        "results": []
                    })
        
        if not races: return "\n(åˆå¯¾æˆ¦)"

        if tbl.find('tbody'):
            for tr in tbl.find('tbody').find_all('tr'):
                u_link = tr.find('a', class_='nk23_c-table08__text')
                if not u_link: continue
                h_name = u_link.get_text(strip=True)
                grade = grades.get(h_name, "")
                if not grade:
                    for k, v in grades.items():
                        if k in h_name or h_name in k: grade = v; break
                
                cells = tr.find_all(['td','th'])
                st_idx = -1
                for idx, c in enumerate(cells):
                    if c.find('a', class_='nk23_c-table08__text'): st_idx=idx; break
                if st_idx == -1: continue

                for i, cell in enumerate(cells[st_idx+1:]):
                    if i >= len(races): break
                    rp = cell.find('p', class_='nk23_c-table08__number')
                    rnk = ""
                    if rp:
                        sp = rp.find('span')
                        rnk = sp.get_text(strip=True) if sp else rp.get_text(strip=True).split('ï½œ')[0].strip()
                    
                    if rnk and (rnk.isdigit() or rnk in ['é™¤å¤–','ä¸­æ­¢']):
                        races[i]["results"].append({
                            "rank": rnk, "name": h_name, "grade": grade,
                            "sort": int(rnk) if rnk.isdigit() else 999
                        })

        out = ["\nã€å¯¾æˆ¦è¡¨ï¼ˆAIè©•ä¾¡ä»˜ãï¼‰ã€‘"]
        for r in races:
            if not r["results"]: continue
            r["results"].sort(key=lambda x: x["sort"])
            line_parts = []
            for x in r["results"]:
                g_str = f"[{x['grade']}]" if x['grade'] else ""
                line_parts.append(f"{x['rank']}ç€ {x['name']}{g_str}")
            out.append(f"â—† {r['title']}")
            out.append(" / ".join(line_parts))
            out.append(f"è©³ç´°: {r['url']}\n")
            
        return "\n".join(out)

    except: return "(å¯¾æˆ¦è¡¨ä½œæˆã‚¨ãƒ©ãƒ¼)"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    resources = load_resources()
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›(KBã‚³ãƒ¼ãƒ‰) -> Nankanã‚³ãƒ¼ãƒ‰
    # 10:å¤§äº•, 11:å·å´, 12:èˆ¹æ©‹, 13:æµ¦å’Œ
    kb_input_map = {"10":"å¤§äº•", "11":"å·å´", "12":"èˆ¹æ©‹", "13":"æµ¦å’Œ"}
    nk_code_map = {"10":"20", "11":"21", "12":"19", "13":"18"}
    
    place_name = kb_input_map.get(place_code, "åœ°æ–¹")
    nk_place_code = nk_code_map.get(place_code)

    if not nk_place_code: yield (0, "âš ï¸ å ´æ‰€ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼"); return

    # ã‚¹ãƒãƒ›ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (Selenium)
    ops = Options()
    ops.add_argument("--headless=new")
    ops.add_argument("--no-sandbox")
    ops.add_argument("--disable-dev-shm-usage")
    ops.add_argument("user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1")
    
    driver = webdriver.Chrome(options=ops)
    wait = WebDriverWait(driver, 10)

    try:
        # 1. é–‹å‚¬æƒ…å ±ç‰¹å®š (Kai, Nichi)
        if ui: st.info("ğŸ“… é–‹å‚¬æƒ…å ±ã‚’ç‰¹å®šä¸­...")
        kai, nichi = get_nankan_kai_nichi(month, day, place_name)
        if not kai or not nichi:
            yield (0, f"âš ï¸ é–‹å‚¬æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ ({month}/{day} {place_name})")
            return
        
        if ui: st.success(f"âœ… {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›®")

        # 2. ãƒ­ã‚°ã‚¤ãƒ³ (KeibaBook)
        if ui: st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­(KeibaBook)...")
        login_keibabook_robust(driver)

        # 3. ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾— (nankankeiba)
        prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nk_place_code}.do"
        driver.get(prog_url)
        soup_prog = BeautifulSoup(driver.page_source, "html.parser")
        
        race_nums = []
        for a in soup_prog.find_all("a", href=True):
            if f"{year}{month}{day}{nk_place_code}" in a["href"] and "uma_shosai" not in a["href"]:
                fname = a["href"].split("/")[-1].replace(".do","")
                if len(fname) == 16: race_nums.append(int(fname[14:16]))
        
        race_nums = sorted(list(set(race_nums)))
        if not race_nums: race_nums = range(1, 13)

        # 4. å„ãƒ¬ãƒ¼ã‚¹å‡¦ç†
        for r_num in race_nums:
            if target_races and r_num not in target_races: continue
            
            if ui: st.markdown(f"## {place_name} {r_num}R")
            
            try:
                # â˜…IDç”Ÿæˆ
                nk_id = f"{year}{month}{day}{nk_place_code}{kai:02}{nichi:02}{r_num:02}"
                # KB_ID: YYYY(4)+MM(2)+Place(2)+Nichi(2)+R(2)+MMDD(4)
                # place_codeã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã¾ã¾(10,11,12,13)
                kb_id = get_kb_url_id(year, month, day, place_code, nichi, r_num)
                
                # A. ãƒ‡ãƒ¼ã‚¿å–å¾—
                danwa, cyokyo = parse_kb_danwa_cyokyo(driver, kb_id)
                
                nk_url = f"https://www.nankankeiba.com/uma_shosai/{nk_id}.do"
                driver.get(nk_url)
                nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)
                
                if not nk_data["horses"]:
                    yield (r_num, f"âš ï¸ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {nk_url}"); continue

                # B. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                header = f"ãƒ¬ãƒ¼ã‚¹å: {r_num}R {nk_data['meta'].get('race_name','')}ã€€æ ¼ä»˜ã‘:{nk_data['meta'].get('grade','')}ã€€ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                horse_texts = []
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h = nk_data["horses"][u]
                    
                    prev_j = ""
                    if h["hist"]:
                        m = re.search(r"é¨æ‰‹ï¼š([^ã€€\s]+)", h["hist"][0])
                        if m: prev_j = m.group(1)
                    p_info = f" (å‰èµ°:{prev_j})" if prev_j else ""
                    
                    lines = [
                        f"[é¦¬ç•ª{u}] {h['name']} é¨æ‰‹:{h['jockey']}{p_info} èª¿æ•™å¸«:{h['trainer']}",
                        f"è«‡è©±: {danwa.get(u,'ãªã—')} èª¿æ•™:{cyokyo.get(u,'èª¿æ•™ãƒ‡ãƒ¼ã‚¿ãªã—')}",
                        f"ã€é¨æ‰‹ã€‘{h['power']} ç›¸æ€§:{h['compat']}"
                    ]
                    cn = {0:"â‘ ", 1:"â‘¡", 2:"â‘¢"}
                    for idx, his in enumerate(h["hist"]):
                        lines.append(f"ã€è¿‘èµ°ã€‘{cn.get(idx,'')} {his}")
                    
                    horse_texts.append("\n".join(lines))
                
                full_prompt = header + "\n\n" + "\n\n".join(horse_texts)
                
                # C. Difyé€ä¿¡
                if ui: st.info("ğŸ¤– AIåˆ†æä¸­...")
                ai_output = run_dify_prediction(full_prompt)
                
                # D. å¯¾æˆ¦è¡¨ä½œæˆ
                grades = _parse_grades_from_ai(ai_output)
                matchup_text = _fetch_matchup_table(nk_id, grades)
                
                # E. æœ€çµ‚å‡ºåŠ›
                final_res = f"ğŸ“… {year}/{month}/{day} {place_name}{r_num}R\n\n=== ğŸ¤–AIäºˆæƒ³ ===\n{ai_output}\n\n{matchup_text}\n\n=== ğŸ“Šåˆ†æãƒ‡ãƒ¼ã‚¿(æŠœç²‹) ===\n{full_prompt[:300]}..."
                
                if ui: st.success("âœ… å®Œäº†")
                yield (r_num, final_res)
                time.sleep(2)

            except Exception as e:
                yield (r_num, f"Error: {e}")
    
    finally:
        driver.quit()
