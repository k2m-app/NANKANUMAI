import time
import re
import os
import csv
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# ã€è¨­å®šã€‘ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
# ==================================================
# Streamlit Cloudãªã©ã®ç’°å¢ƒã«åˆã‚ã›ã¦ãƒ‘ã‚¹ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# ==================================================
# ã€è¨­å®šã€‘ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ»API (Secretsã‹ã‚‰èª­ã¿è¾¼ã¿)
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ / Difyé€£æº
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    sess = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def run_dify_prediction(full_text):
    """ Dify APIã«ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’é€ä¿¡ã—ã¦äºˆæƒ³ã‚’å–å¾—ã™ã‚‹ """
    if not DIFY_API_KEY: return "âš ï¸ DIFY_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {
        "inputs": {"text": full_text}, 
        "response_mode": "blocking", 
        "user": "keiba-bot"
    }
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}", 
        "Content-Type": "application/json"
    }
    
    sess = get_http_session()
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®šï¼ˆAIã®æ€è€ƒæ™‚é–“è€ƒæ…®ï¼‰
        res = sess.post(url, headers=headers, json=payload, timeout=90)
        
        if res.status_code != 200:
            return f"âš ï¸ Dify Error ({res.status_code}): {res.text}"
            
        json_data = res.json()
        # å‡ºåŠ›ã‚­ãƒ¼ã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¨­å®šã«ä¾å­˜ã—ã¾ã™ï¼ˆ'text', 'answer', 'result'ãªã©ï¼‰
        # ã“ã“ã§ã¯ä¸€èˆ¬çš„ãª 'text' ã‚’å–å¾—ã—ã€ãªã‘ã‚Œã°jsonå…¨ä½“ã‚’è¿”ã—ã¾ã™
        outputs = json_data.get("data", {}).get("outputs", {})
        return outputs.get("text") or outputs.get("result") or str(outputs)
        
    except Exception as e:
        return f"âš ï¸ API Connection Error: {e}"

# ==================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
@st.cache_resource
def load_resources():
    res = {"jockeys": [], "trainers": [], "power": {}}
    
    # 1. é¨æ‰‹ãƒªã‚¹ãƒˆ
    if os.path.exists(JOCKEY_FILE):
        try:
            with open(JOCKEY_FILE, "r", encoding="utf-8-sig") as f:
                res["jockeys"] = [line.strip().replace("ï¼Œ","").replace(",","").replace(" ","").replace("ã€€","") for line in f if line.strip()]
        except Exception as e: print(f"âš ï¸ Jockey list load error: {e}")

    # 2. èª¿æ•™å¸«ãƒªã‚¹ãƒˆ
    if os.path.exists(TRAINER_FILE):
        try:
            with open(TRAINER_FILE, "r", encoding="utf-8-sig") as f:
                res["trainers"] = [line.strip().replace("ï¼Œ","").replace(",","").replace(" ","").replace("ã€€","") for line in f if line.strip()]
        except Exception as e: print(f"âš ï¸ Trainer list load error: {e}")

    # 3. é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼CSV
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            # 1åˆ—ç›®ãŒç«¶é¦¬å ´åã¨ä»®å®š (ã‚«ãƒ©ãƒ åãŒ Unnamed: 0 ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚)
            place_col = df.columns[0]
            
            for _, row in df.iterrows():
                place = str(row[place_col]).strip()
                jockey = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                
                if place and jockey:
                    power = row.get("é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼", "-")
                    win = row.get("å‹ç‡", "-")
                    fuku = row.get("è¤‡å‹ç‡", "-")
                    
                    # æ¤œç´¢ã‚­ãƒ¼: (ç«¶é¦¬å ´, é¨æ‰‹å)
                    key = (place, jockey)
                    res["power"][key] = f"é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:{power}(å‹ç‡{win} è¤‡å‹ç‡{fuku})"
                    
            print(f"âœ… Power data loaded: {len(res['power'])} records")
        except Exception as e: print(f"âš ï¸ Power data load error: {e}")
    
    return res

def normalize_name(abbrev, full_list):
    """ ç•¥ç§° -> æ­£å¼åç§°ã¸ã®å¤‰æ› (åå¯„ã›) """
    if not abbrev: return ""
    clean = abbrev.replace(" ","").replace("ã€€","")
    if not full_list: return clean
    if clean in full_list: return clean
    
    # å‰æ–¹ä¸€è‡´æ¤œç´¢ (ä¾‹: "æœ¨é–“é¾" -> "æœ¨é–“å¡šé¾é¦¬")
    # 2æ–‡å­—ä»¥ä¸Šä¸€è‡´ã€ã‹ã¤å…ˆé ­ãŒä¸€è‡´ã™ã‚‹ã‚‚ã®
    matches = [n for n in full_list if n.startswith(clean) or (len(clean)>=2 and n.startswith(clean[0]) and clean[1] in n)]
    
    if matches:
        # æœ€ã‚‚çŸ­ã„ã‚‚ã®ï¼ˆã‚ã‚‹ã„ã¯ãƒªã‚¹ãƒˆé †ï¼‰ã‚’è¿”ã™
        return sorted(matches, key=len)[0]
    return clean

# ==================================================
# nankankeiba.com è§£æãƒ­ã‚¸ãƒƒã‚¯ (BeautifulSoup)
# ==================================================
def parse_nankankeiba_html(html, place_name, resources):
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}

    # --- 1. ãƒ¬ãƒ¼ã‚¹æƒ…å ± ---
    title_h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = title_h3.get_text(strip=True) if title_h3 else ""
    
    # æ ¼ä»˜ã‘æŠ½å‡º (ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ã®æœ«å°¾ãªã©ã‹ã‚‰æ¨æ¸¬)
    if data["meta"]["race_name"]:
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚„åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
        parts = re.split(r'[ ã€€]+', data["meta"]["race_name"])
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    
    cond_a = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    if cond_a:
        cond_text = cond_a.get_text(strip=True)
        data["meta"]["course"] = f"{place_name} {cond_text}"

    # --- 2. è©³ç´°å‡ºèµ°è¡¨è§£æ ---
    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data

    rows = table.select("tbody tr")
    for row in rows:
        try:
            # é¦¬ç•ª
            umaban_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not umaban_tag: continue
            umaban = umaban_tag.get_text(strip=True)
            if not umaban.isdigit(): continue

            # é¦¬å
            horse_tag = row.select_one("td.is-col03 a.is-link")
            horse_name = horse_tag.get_text(strip=True) if horse_tag else ""

            # é¨æ‰‹ãƒ»èª¿æ•™å¸«ï¼ˆHTMLä¸Šã®ç•¥ç§°ï¼‰
            jg_td = row.select_one("td.cs-g1")
            jockey_raw = ""
            trainer_raw = ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: jockey_raw = links[0].get_text(strip=True)
                if len(links) >= 2: trainer_raw = links[1].get_text(strip=True)
            
            # æ­£è¦åŒ– (CSVãƒªã‚¹ãƒˆã‚’ä½¿ã£ã¦ãƒ•ãƒ«ãƒãƒ¼ãƒ ã¸)
            jockey_full = normalize_name(jockey_raw, resources["jockeys"])
            trainer_full = normalize_name(trainer_raw, resources["trainers"])

            # é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼å–å¾— (å ´æ‰€åã¨é¨æ‰‹åã§æ¤œç´¢)
            # ãƒãƒƒãƒã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            power_info = resources["power"].get((place_name, jockey_full), f"é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:ä¸æ˜")

            # ç›¸æ€§ãƒ‡ãƒ¼ã‚¿ (é¨æ‰‹xèª¿æ•™å¸«: cs-ai2)
            # nankankeibaã¯ã€Œå‹ç‡ã€ã¨ã€Œå‹åˆ©æ•°/é¨ä¹—æ•°ã€ã‚’æŒã£ã¦ã„ã‚‹
            ai2_div = row.select_one("td.cs-ai2 .graph_text_div")
            stats_pair = "ãƒ‡ãƒ¼ã‚¿ãªã—"
            if ai2_div and "ãƒ‡ãƒ¼ã‚¿" not in ai2_div.get_text():
                rate = ai2_div.select_one(".is-percent").get_text(strip=True)
                win = ai2_div.select_one(".is-number").get_text(strip=True)
                total = ai2_div.select_one(".is-total").get_text(strip=True)
                stats_pair = f"å‹ç‡{rate}({win}å‹/{total}å›)"

            # --- è¿‘èµ°ãƒ‡ãƒ¼ã‚¿ (éå»3èµ°: cs-z1 ~ cs-z3) ---
            history = []
            for i in range(1, 4): # 1, 2, 3
                z_td = row.select_one(f"td.cs-z{i}")
                if not z_td or not z_td.get_text(strip=True): continue
                
                # æ—¥ä»˜ãƒ»å ´æ‰€å–å¾— (ä¾‹: æµ¦å’Œ26.1.7)
                dp_span = z_td.select("p.nk23_u-d-flex span.nk23_u-text10")
                date_place_text = ""
                if dp_span:
                    for s in dp_span:
                        txt = s.get_text(strip=True)
                        if re.search(r"\d+\.\d+\.\d+", txt): date_place_text = txt; break
                
                # æ—¥ä»˜æ•´å½¢
                date_str = ""
                m = re.match(r"([^\d]+)(\d+)\.(\d+)\.(\d+)", date_place_text)
                if m:
                    # å¹´å·26 -> 2026
                    yy, mm, dd = m.group(2), m.group(3), m.group(4)
                    date_str = f"20{yy}/{int(mm):02}/{int(dd):02}"
                
                # ã‚³ãƒ¼ã‚¹æ¡ä»¶ (ä¾‹: ç¨å¤–ãƒ€1400)
                cond_text = dp_span[-1].get_text(strip=True) if len(dp_span) >= 2 else ""
                
                # è·é›¢ã¨å ´æ‰€ã‚’çµåˆ
                dist_m = re.search(r"\d{4}", cond_text)
                dist = dist_m.group(0) if dist_m else ""
                place_m = re.match(r"^[^\d]+", date_place_text)
                place_h = place_m.group(0) if place_m else ""
                course_str = f"{place_h}{dist}m"

                # ãƒ¬ãƒ¼ã‚¹åãƒ»ã‚¯ãƒ©ã‚¹
                race_a = z_td.select_one("a.is-link")
                race_title_full = race_a.get("title", "") if race_a else ""
                r_parts = re.split(r'[ ã€€]+', race_title_full) # ç©ºç™½åŒºåˆ‡ã‚Š
                race_name = r_parts[0] if r_parts else ""
                race_class = r_parts[1] if len(r_parts) > 1 else ""

                # é¨æ‰‹ãƒ»äººæ°—ãƒ»ç€é †
                j_p_line = z_td.select("p.nk23_u-text10")
                jockey_prev = ""
                pop = ""
                for p in j_p_line:
                    ptxt = p.get_text(strip=True)
                    if "äººæ°—" in ptxt:
                        pop_m = re.search(r"(\d+)äººæ°—", ptxt)
                        if pop_m: pop = f"{pop_m.group(1)}äººæ°—"
                        # åŒã˜Pã‚¿ã‚°å†…ã€ã‚ã‚‹ã„ã¯å…„å¼Ÿè¦ç´ ã‹ã‚‰é¨æ‰‹åã‚’å–å¾—
                        spans = p.find_all("span")
                        if len(spans) > 1:
                            # æ•°å­—ã‚’é™¤å»ã—ã¦åå‰ã ã‘ã«ã™ã‚‹ (å°æ‰äº®55.0 -> å°æ‰äº®)
                            j_raw = spans[1].get_text(strip=True)
                            jockey_prev = re.sub(r"[\d\.]+", "", j_raw)

                rank_span = z_td.select_one(".nk23_u-text19")
                rank = rank_span.get_text(strip=True).replace("ç€", "") if rank_span else ""

                # é€šéé †
                pass_str = ""
                pos_p = z_td.select_one("p.position")
                if pos_p: 
                    pass_str = "-".join([s.get_text(strip=True) for s in pos_p.find_all("span")])
                
                # ä¸ŠãŒã‚Š3F
                agari_str = ""
                for p in j_p_line: # ã™ã§ã«å–å¾—ã—ãŸpãƒªã‚¹ãƒˆã‚’å†åˆ©ç”¨
                    if "3F" in p.get_text():
                        # "3F 39.9(10)" -> "(10)"ã‚’æŠ½å‡º
                        ag_m = re.search(r"\(([\d]+)\)", p.get_text())
                        if ag_m: agari_str = f"ä¸ŠãŒã‚Š3Fï¼š{ag_m.group(1)}ä½"

                # æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸æ•´å½¢
                hist_str = (f"é–‹å‚¬æ—¥ï¼š{date_str}ã€€ã‚³ãƒ¼ã‚¹ï¼š{course_str} ãƒ¬ãƒ¼ã‚¹ï¼š{race_name}ã€€"
                            f"ã‚¯ãƒ©ã‚¹ï¼š{race_class} é¨æ‰‹ï¼š{jockey_prev}ã€€"
                            f"é€šéé †{pass_str}({agari_str})â†’{rank}ç€ï¼ˆ{pop}ï¼‰")
                history.append(hist_str)

            data["horses"][umaban] = {
                "name": horse_name, 
                "jockey": jockey_full, 
                "trainer": trainer_full,
                "power": power_info, 
                "compatibility": stats_pair, 
                "history": history
            }
        except Exception: continue
            
    return data

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ è§£æãƒ­ã‚¸ãƒƒã‚¯ (è«‡è©±ãƒ»èª¿æ•™)
# ==================================================
def parse_keibabook_danwa(html):
    soup = BeautifulSoup(html, "html.parser")
    d = {}
    tbl = soup.find("table", class_="danwa")
    if tbl and tbl.tbody:
        cur = None
        for row in tbl.tbody.find_all("tr"):
            u = row.find("td", class_="umaban")
            if u: cur = u.get_text(strip=True); continue
            t = row.find("td", class_="danwa")
            if t and cur: d[cur] = t.get_text(strip=True); cur=None
    return d

def parse_keibabook_cyokyo(html):
    soup = BeautifulSoup(html, "html.parser")
    d = {}
    for tbl in soup.find_all("table", class_="cyokyo"):
        tb = tbl.find("tbody")
        if not tb: continue
        rs = tb.find_all("tr", recursive=False)
        if not rs: continue
        u_td = rs[0].find("td", class_="umaban")
        if u_td:
            u = u_td.get_text(strip=True)
            tp = rs[0].find("td", class_="tanpyo").get_text(strip=True) if rs[0].find("td", class_="tanpyo") else ""
            dt = rs[1].get_text(" ", strip=True) if len(rs)>1 else ""
            d[u] = f"ã€çŸ­è©•ã€‘{tp} ã€è©³ç´°ã€‘{dt}"
    return d

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒ‡ãƒ¼ã‚¿åé›† -> ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ -> AIé€£æº)
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    # 1. ãƒªã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
    resources = load_resources()
    
    # 2. å ´æ‰€ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (KeibaBook: 10~13 -> Nankan: 20,21,19,18)
    kb_to_nankan = {"10": "20", "11": "21", "12": "19", "13": "18"}
    nankan_place_code = kb_to_nankan.get(place_code)
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(place_code, "åœ°æ–¹")

    # 3. ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    wait = WebDriverWait(driver, 10)

    try:
        # A. ç«¶é¦¬ãƒ–ãƒƒã‚¯ã«ãƒ­ã‚°ã‚¤ãƒ³
        if ui: st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­ (KeibaBook)...")
        driver.get("https://s.keibabook.co.jp/login/login")
        if "logout" not in driver.current_url:
            wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
            driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
            driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
            time.sleep(1)

        # B. é–‹å‚¬ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆå–å¾— (KeibaBookã®æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰)
        date_str = f"{year}{month}{day}"
        driver.get(f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10")
        soup_kb = BeautifulSoup(driver.page_source, "html.parser")
        
        kb_race_ids = [] 
        for a in soup_kb.find_all("a", href=True):
            m = re.search(r"(\d{16})", a["href"])
            if m:
                rid = m.group(1)
                # å ´æ‰€ã‚³ãƒ¼ãƒ‰ãŒä¸€è‡´ã™ã‚‹ã‚‚ã®ã ã‘
                if rid[6:8] == place_code:
                    kb_race_ids.append((int(rid[14:16]), rid))
        kb_race_ids.sort()
        
        # C. ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for r_num, kb_rid in kb_race_ids:
            if target_races and r_num not in target_races: continue
            
            if ui: st.markdown(f"## {place_name} {r_num}R")
            
            try:
                # --- [Step 1] ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‹ã‚‰è«‡è©±ãƒ»èª¿æ•™ã‚’å–å¾— ---
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_rid}")
                danwa_dict = parse_keibabook_danwa(driver.page_source)
                driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_rid}")
                cyokyo_dict = parse_keibabook_cyokyo(driver.page_source)

                # --- [Step 2] nankankeiba.comã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— ---
                # URLã‚’ç‰¹å®š (nankankeibaã®IDã«ã¯ã€Œå›ãƒ»æ—¥æ¬¡ã€ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™)
                if r_num == 1 or 'nk_base_id' not in locals():
                    # å½“æ—¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ä¸€è¦§ã¸ã‚¢ã‚¯ã‚»ã‚¹
                    prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nankan_place_code}.do"
                    driver.get(prog_url)
                    # è©²å½“ãƒ¬ãƒ¼ã‚¹ç•ªå·ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã—ã¦IDã®ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†(YYYYMMDDppKkDD)ã‚’æŠ½å‡º
                    try:
                        lnk = driver.find_element(By.XPATH, f"//a[contains(@href, '{year}{month}{day}{nankan_place_code}') and contains(@href, '{str(r_num).zfill(2)}.do')]")
                        href = lnk.get_attribute('href')
                        nk_id_full = href.split("/")[-1].replace(".do", "")
                        nk_base_id = nk_id_full[:-2] # æœ«å°¾ã®ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é™¤å»
                    except:
                        yield (r_num, "âš ï¸ nankankeiba URLç‰¹å®šå¤±æ•—"); continue
                
                # è©³ç´°å‡ºèµ°è¡¨ã¸ã‚¢ã‚¯ã‚»ã‚¹
                nk_race_url = f"https://www.nankankeiba.com/uma_shosai/{nk_base_id}{str(r_num).zfill(2)}.do"
                driver.get(nk_race_url)
                
                # ãƒ‡ãƒ¼ã‚¿è§£æ
                nk_data = parse_nankankeiba_html(driver.page_source, place_name, resources)
                
                # --- [Step 3] AIç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ ---
                header = f"ãƒ¬ãƒ¼ã‚¹å: {r_num}R {nk_data['meta'].get('race_name','')}ã€€æ ¼ä»˜ã‘:{nk_data['meta'].get('grade','')}ã€€ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                horse_texts = []
                
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h = nk_data["horses"][u]
                    danwa = danwa_dict.get(u, "ãªã—")
                    cyokyo = cyokyo_dict.get(u, "èª¿æ•™ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
                    # å‰èµ°é¨æ‰‹æƒ…å ±ã®æŠ½å‡º (è¿‘èµ°å±¥æ­´ã®1è¡Œç›®ã‹ã‚‰)
                    p_jockey = ""
                    if h["history"]:
                        m = re.search(r"é¨æ‰‹ï¼š([^ã€€\s]+)", h["history"][0])
                        if m: p_jockey = m.group(1)
                    p_info = f" (å‰èµ°:{p_jockey})" if p_jockey else ""
                    
                    # ãƒ–ãƒ­ãƒƒã‚¯æ§‹ç¯‰
                    block = [
                        f"[é¦¬ç•ª{u}] {h['name']} é¨æ‰‹:{h['jockey']}{p_info} èª¿æ•™å¸«:{h['trainer']}",
                        f"è«‡è©±: {danwa} èª¿æ•™:{cyokyo}",
                        f"ã€é¨æ‰‹ã€‘{h['power']} ç›¸æ€§:{h['compatibility']}"
                    ]
                    
                    # è¿‘èµ°å±¥æ­´è¿½åŠ 
                    cn_map = {0:"â‘ ", 1:"â‘¡", 2:"â‘¢"}
                    for idx, hs in enumerate(h["history"]):
                        block.append(f"ã€è¿‘èµ°ã€‘{cn_map.get(idx,'')} {hs}")
                    
                    horse_texts.append("\n".join(block))
                
                # å®Œæˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                full_prompt = header + "\n\n" + "\n\n".join(horse_texts)
                
                # --- [Step 4] Difyã¸é€ä¿¡ & äºˆæƒ³å–å¾— ---
                if ui: st.info("ğŸ¤– AIåˆ†æä¸­ (Dify)...")
                dify_res = run_dify_prediction(full_prompt)
                
                # --- [Step 5] çµæœå‡ºåŠ› ---
                final_output = f"ğŸ“… {year}/{month}/{day} {place_name}{r_num}R\n\n=== ğŸ¤–AIäºˆæƒ³ ===\n{dify_res}\n\n=== ğŸ“Šä½¿ç”¨ãƒ‡ãƒ¼ã‚¿(æŠœç²‹) ===\n{full_prompt[:500]}..."
                
                if ui: st.success("âœ… äºˆæƒ³å®Œäº†")
                yield (r_num, final_output)
                time.sleep(2)

            except Exception as e:
                yield (r_num, f"Error: {e}")
                
    finally:
        driver.quit()
