import time
import re
import os
import json
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

# Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# HTML Parsing & Network
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# 1. è¨­å®š & å®šæ•°
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# Secrets
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# â˜… å…ˆã« normalize_name ã‚’å®šç¾©ï¼ˆload_resourcesã§ä½¿ã†ãŸã‚ï¼‰
# ==================================================
def normalize_name(abbrev, full_list, priority_set=None):
    """
    ç•¥ç§°ã‚’ãƒ•ãƒ«ãƒãƒ¼ãƒ ã«æ­£è¦åŒ–ã™ã‚‹ã€‚
    priority_setãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã“ã«å«ã¾ã‚Œã‚‹åå‰ã‚’å„ªå…ˆã™ã‚‹ï¼ˆpriority_setã¯ãƒ•ãƒ«ãƒãƒ¼ãƒ é›†åˆã§ã‚ã‚‹ã“ã¨ï¼‰ã€‚
    """
    if not abbrev:
        return ""

    # ä½™è¨ˆãªè¨˜å·ãƒ»æ•°å­—ãƒ»ç©ºç™½ãªã©é™¤å»ï¼ˆæœ€å¤§3æ–‡å­—ã§ã‚‚ã“ã“ã§æ•´ã†ï¼‰
    clean = re.sub(r"[ ã€€â–²â–³â˜†â—‡â˜…\d\.]+", "", str(abbrev))
    clean = clean.strip()
    if not clean:
        return ""

    if not full_list:
        return clean

    # å®Œå…¨ä¸€è‡´ï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ãŒæ¥ãŸã¨ãã¯ãã®ã¾ã¾ï¼‰
    if clean in full_list:
        return clean

    candidates = []
    for full in full_list:
        # 1) é€£ç¶šä¸€è‡´ï¼ˆæœ€å„ªå…ˆï¼‰
        # 2) æ–‡å­—ãŒå…¨éƒ¨å«ã¾ã‚Œã‚‹ï¼ˆæ¬¡ç‚¹ã€2ï½3æ–‡å­—ã§ã‚‚æ‹¾ãˆã‚‹ï¼‰
        if clean in full:
            diff = len(full) - len(clean)
            is_priority = 1 if (priority_set and full in priority_set) else 0
            # é€£ç¶šä¸€è‡´ã¯å¼·ãå„ªå…ˆã™ã‚‹ãŸã‚ã€contig=0 ã‚’æœ€å„ªå…ˆã«
            candidates.append((0, -is_priority, diff, full))
        elif all(c in full for c in clean):
            diff = len(full) - len(clean)
            is_priority = 1 if (priority_set and full in priority_set) else 0
            candidates.append((1, -is_priority, diff, full))

    if candidates:
        # contig(0ãŒæœ€å¼·) â†’ priority(1ãŒå¼·ã„ã®ã§-å„ªå…ˆ) â†’ diff(çŸ­ã„ã»ã©) ã§æ±ºå®š
        candidates.sort(key=lambda x: (x[0], x[1], x[2]))
        return candidates[0][3]

    return clean

# ==================================================
# 2. å…±é€šé–¢æ•°
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    })
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def get_driver():
    ops = Options()
    ops.add_argument("--headless=new")
    ops.add_argument("--no-sandbox")
    ops.add_argument("--disable-dev-shm-usage")
    ops.add_argument("--disable-gpu")
    ops.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=ops)

def login_keibabook_robust(driver):
    try:
        driver.get("https://s.keibabook.co.jp/login/login")
        time.sleep(1)
        if "logout" in driver.current_url or driver.find_elements(By.XPATH, "//a[contains(@href,'logout')]"):
            return True
        WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(2)
        return True
    except Exception:
        return False

# ==================================================
# 3. Dify API
# ==================================================
def run_dify_prediction(full_text):
    if not DIFY_API_KEY:
        return "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    sess = get_http_session()

    max_retries = 3
    for attempt in range(max_retries):
        full_response = ""
        try:
            with sess.post(url, headers=headers, json=payload, stream=True, timeout=120) as res:
                if res.status_code == 429:
                    time.sleep(60)
                    continue
                if res.status_code != 200:
                    return f"âš ï¸ Dify Error: {res.status_code}"

                for line in res.iter_lines():
                    if line:
                        decoded_line = line.decode("utf-8")
                        if decoded_line.startswith("data:"):
                            json_str = decoded_line[5:].strip()
                            if not json_str:
                                continue
                            try:
                                data = json.loads(json_str)
                                event = data.get("event")
                                if event == "workflow_finished":
                                    outputs = data.get("data", {}).get("outputs", {})
                                    if "text" in outputs:
                                        return outputs["text"]
                                elif event == "text_chunk" or event == "message":
                                    chunk = data.get("data", {}).get("text", "")
                                    full_response += chunk
                            except:
                                pass
                return full_response if full_response else "ï¼ˆå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼‰"
        except Exception:
            time.sleep(5)
    return "âš ï¸ ã‚¨ãƒ©ãƒ¼: ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã‚’è¶…ãˆã¾ã—ãŸ"

# ==================================================
# 4. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ & è§£æ (â˜…è¿‘èµ°é¨æ‰‹åãƒ•ãƒ«ãƒãƒ¼ãƒ åŒ–ãŒç¢ºå®Ÿã«å¶ã†ç‰ˆ)
# ==================================================
@st.cache_resource
def load_resources():
    res = {
        "jockeys": [],        # â˜…ãƒ•ãƒ«ãƒãƒ¼ãƒ ã®ã¿ï¼ˆJOCKEY_FILEç”±æ¥ï¼‰
        "trainers": [],
        "power_data": {},     # (å ´æ‰€, é¨æ‰‹ãƒ•ãƒ«å) -> {power, win, fuku}
        "power_jockeys": set()  # â˜…ãƒ•ãƒ«ãƒãƒ¼ãƒ é›†åˆï¼ˆpriorityç”¨ï¼‰
    }

    # ãƒ‘ã‚¹è§£æ±ºãƒ˜ãƒ«ãƒ‘ãƒ¼
    def get_valid_path(target_path):
        if os.path.exists(target_path):
            return target_path
        basename = os.path.basename(target_path)
        p2 = os.path.join(DATA_DIR, basename)
        if os.path.exists(p2):
            return p2
        if os.path.exists(basename):
            return basename
        return None

    # 1. é¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ æƒ³å®šï¼‰
    j_path = get_valid_path(JOCKEY_FILE)
    if j_path:
        for enc in ["utf-8-sig", "cp932"]:
            try:
                with open(j_path, "r", encoding=enc) as f:
                    # â˜…1è¡Œ1åã®å‰æã§ãƒ•ãƒ«ãƒãƒ¼ãƒ ã ã‘ã‚’ä½œã‚‹
                    res["jockeys"] = [
                        l.strip().replace(" ", "").replace("ã€€", "")
                        for l in f if l.strip()
                    ]
                break
            except:
                continue

    t_path = get_valid_path(TRAINER_FILE)
    if t_path:
        for enc in ["utf-8-sig", "cp932"]:
            try:
                with open(t_path, "r", encoding=enc) as f:
                    res["trainers"] = [
                        l.strip().replace(",", "").replace(" ", "").replace("ã€€", "")
                        for l in f if l.strip()
                    ]
                break
            except:
                continue

    # 2. é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼CSVèª­ã¿è¾¼ã¿
    # â˜…ã“ã“ãŒé‡è¦ï¼šPOWER_FILEå´ã®é¨æ‰‹åï¼ˆçŸ­ç¸®è¡¨è¨˜ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰ã‚’ãƒ•ãƒ«ãƒãƒ¼ãƒ ã«æ­£è¦åŒ–ã—ã¦ä¿å­˜ã™ã‚‹
    p_path = get_valid_path(POWER_FILE)
    if p_path:
        df = None
        for enc in ["utf-8-sig", "cp932"]:
            try:
                df = pd.read_csv(p_path, encoding=enc)
                break
            except:
                continue

        if df is not None:
            try:
                place_col = df.columns[0]
                has_win = "å‹ç‡" in df.columns
                has_fuku = "è¤‡å‹ç‡" in df.columns
                has_power = "é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼" in df.columns
                has_name = "é¨æ‰‹å" in df.columns

                for _, row in df.iterrows():
                    p = str(row[place_col]).strip()
                    j_raw = row["é¨æ‰‹å"] if has_name else ""
                    j_raw = str(j_raw).replace(" ", "").replace("ã€€", "").strip()

                    if not j_raw or not p:
                        continue

                    # â˜…POWERå´ã®åå‰ã‚’ã€Œãƒ•ãƒ«ãƒãƒ¼ãƒ å€™è£œã€ã«æ­£è¦åŒ–ï¼ˆã“ã“ã§æœ€å¤§3æ–‡å­—â†’ãƒ•ãƒ«åã¸ï¼‰
                    j_full = normalize_name(j_raw, res["jockeys"], priority_set=None)

                    # power_jockeys ã¯ãƒ•ãƒ«ãƒãƒ¼ãƒ é›†åˆï¼ˆpriorityç”¨ï¼‰
                    if j_full:
                        res["power_jockeys"].add(j_full)

                    val_power = str(row["é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼"]) if has_power else "-"
                    val_win = str(row["å‹ç‡"]) if has_win else "-"
                    val_fuku = str(row["è¤‡å‹ç‡"]) if has_fuku else "-"

                    # â˜…ã‚­ãƒ¼: (å ´æ‰€, é¨æ‰‹ãƒ•ãƒ«å) ã«çµ±ä¸€
                    key_t = (p, j_full if j_full else j_raw)
                    res["power_data"][key_t] = {
                        "power": val_power,
                        "win": val_win,
                        "fuku": val_fuku
                    }

                # â˜…ã“ã“ã¯å‰Šé™¤ï¼šPOWER_FILEç”±æ¥ã®ã€ŒçŸ­ã„é¨æ‰‹åã€ã‚’ jockeys ã«æ··ãœãªã„
                # ï¼ˆæ··ãœã‚‹ã¨ normalize_name ãŒçŸ­ã„æ–¹ã§ç¢ºå®šã—ã¦ã—ã¾ã„ã€ãƒ•ãƒ«ãƒãƒ¼ãƒ åŒ–ãŒå¤±æ•—ã™ã‚‹ï¼‰
                # current_jockeys = set(res["jockeys"])
                # for j in res["power_jockeys"]:
                #     if j not in current_jockeys:
                #         res["jockeys"].append(j)

            except Exception:
                pass

    return res

def parse_nankankeiba_detail(html, place_name, resources):
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}

    h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = h3.get_text(strip=True) if h3 else ""
    if data["meta"]["race_name"]:
        parts = re.split(r"[ ã€€]+", data["meta"]["race_name"])
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    cond = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    data["meta"]["course"] = f"{place_name} {cond.get_text(strip=True)}" if cond else ""

    shosai_area = soup.select_one("#shosai_aria")
    if not shosai_area:
        return data

    table = shosai_area.select_one("table.nk23_c-table22__table")
    if not table:
        return data

    PLACE_MAP = {"èˆ¹": "èˆ¹æ©‹", "å¤§": "å¤§äº•", "å·": "å·å´", "æµ¦": "æµ¦å’Œ", "é–€": "é–€åˆ¥", "ç››": "ç››å²¡", "æ°´": "æ°´æ²¢", "ç¬ ": "ç¬ æ¾", "å": "åå¤å±‹", "åœ’": "åœ’ç”°", "å§«": "å§«è·¯", "é«˜": "é«˜çŸ¥", "ä½": "ä½è³€"}
    KNOWN_PLACES = list(PLACE_MAP.values()) + ["JRA"]

    for row in table.select("tbody tr"):
        try:
            u_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not u_tag:
                continue
            umaban = u_tag.get_text(strip=True)
            if not umaban.isdigit():
                continue
            h_link = row.select_one("td.is-col03 a.is-link") or row.select_one("td.pr-umaName-textRound a.is-link")
            horse_name = h_link.get_text(strip=True) if h_link else "ä¸æ˜"

            # --- ä»Šå›ã®é¨æ‰‹ãƒ»èª¿æ•™å¸« ---
            jg_td = row.select_one("td.cs-g1")
            j_raw, t_raw = "", ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1:
                    j_raw = links[0].get_text(strip=True)
                if len(links) >= 2:
                    t_raw = links[1].get_text(strip=True)

            # æ­£è¦åŒ–
            j_full = normalize_name(j_raw, resources["jockeys"], resources["power_jockeys"])
            t_full = normalize_name(t_raw, resources["trainers"], None)

            # --- ä»Šå›ã®é¨æ‰‹ãƒ‡ãƒ¼ã‚¿ ---
            p_data_curr = resources["power_data"].get((place_name, j_full))
            curr_power_str = "P:ä¸æ˜"
            if p_data_curr:
                cp = p_data_curr["power"]
                cw = p_data_curr["win"].replace("%", "")
                cf = p_data_curr["fuku"].replace("%", "")
                curr_power_str = f"P:{cp}(å‹{cw}%è¤‡{cf}%)"

            ai2 = row.select_one("td.cs-ai2 .graph_text_div")
            pair_stats = "-"
            if ai2 and "ãƒ‡ãƒ¼ã‚¿" not in ai2.get_text():
                r = ai2.select_one(".is-percent").get_text(strip=True)
                w = ai2.select_one(".is-number").get_text(strip=True)
                t = ai2.select_one(".is-total").get_text(strip=True)
                pair_stats = f"å‹{r}({w}/{t})"

            history = []
            prev_power_val = None

            # --- è¿‘èµ°ãƒ‡ãƒ¼ã‚¿ (æœ€å¤§3èµ°) ---
            for i in range(1, 4):
                z = row.select_one(f"td.cs-z{i}")
                if not z:
                    continue
                z_full_text = z.get_text(" ", strip=True)
                if not z_full_text:
                    continue

                # 1. æ—¥ä»˜ã¨é–‹å‚¬å ´
                d_txt = ""
                place_short = ""
                d_div = z.select_one("p.nk23_u-d-flex")

                if d_div:
                    d_raw = d_div.get_text(" ", strip=True)
                    m_dt = re.search(r"(\d+\.\d+\.\d+)", d_raw)
                    if m_dt:
                        d_txt = m_dt.group(1)

                    rem_text = d_raw.replace(d_txt, "") if d_txt else d_raw
                    for kp in KNOWN_PLACES:
                        if kp in rem_text:
                            place_short = kp
                            break
                    if not place_short:
                        for k, v in PLACE_MAP.items():
                            if k in rem_text:
                                place_short = v
                                break

                if not d_txt:
                    d_txt = "ä¸æ˜"
                if not place_short:
                    place_short = place_name

                # 2. è·é›¢
                dm = re.search(r"(\d{3,4})m?", z_full_text)
                dist = dm.group(1) if dm else ""

                # ==================================================
                # â˜… 3. ç€é † (ä¿®æ­£ï¼šèƒ½è©¦ãƒ»å–æ¶ˆãƒ»é™¤å¤–ã«å¯¾å¿œ)
                # ==================================================
                rank = ""
                # é€šå¸¸ã®ç€é †ã‚¿ã‚° (ä¾‹: 1ç€, 2ç€...)
                r_tag = z.select_one(".nk23_u-text19")
                
                if r_tag:
                    # æ•°å­—ã®ã¿ã‚’å–ã‚Šå‡ºã™
                    rank = r_tag.get_text(strip=True).replace("ç€", "")
                else:
                    # ç€é †ãŒãªã„å ´åˆã€ç‰¹æ®Šã‚¿ã‚°(èƒ½è©¦ã€å–æ¶ˆã€é™¤å¤–ãªã©)ã‚’æ¢ã™
                    special_tag = z.select_one(".nk23_u-text16")
                    if special_tag:
                        # "èƒ½è©¦" ã‚„ "å–æ¶ˆ" ã¨ã„ã†æ–‡å­—ã‚’ãã®ã¾ã¾å–å¾—
                        rank = special_tag.get_text(strip=True)
                # ==================================================

                # 4. é¨æ‰‹(ç•¥ç§°)ãƒ»äººæ°—
                j_prev, pop = "", ""
                p_lines = z.select("p.nk23_u-text10")
                for p in p_lines:
                    txt = p.get_text(strip=True)
                    if "äººæ°—" in txt:
                        pm = re.search(r"(\d+)äººæ°—", txt)
                        if pm:
                            pop = f"{pm.group(1)}äºº"
                        spans = p.find_all("span")
                        if len(spans) >= 2:
                            j_cand = spans[1].get_text(strip=True)
                            j_prev = re.sub(r"[\d\.]+", "", j_cand)
                        break

                # 5. ä¸ŠãŒã‚Š3F (ã‚¿ã‚°å–å¾—ç‰ˆ)
                agari = ""
                ft_elem = z.select_one(".furlongtime")
                if ft_elem:
                    raw_agari = ft_elem.get_text(strip=True)
                    if raw_agari:
                        agari = raw_agari

                # 6. é€šéé †
                pos_p = z.select_one("p.position")
                pas = ""
                if pos_p:
                    pas_spans = [s.get_text(strip=True) for s in pos_p.find_all("span")]
                    pas = "-".join(pas_spans)

                # 7. é¨æ‰‹åã®æ­£è¦åŒ–
                j_prev_full = normalize_name(j_prev, resources["jockeys"], resources["power_jockeys"])
                if not j_prev_full and j_prev:
                    j_prev_full = j_prev

                # â˜… å‰èµ°(i=1)ã®På–å¾— â˜…
                if i == 1:
                    p_key = (place_short, j_prev_full)
                    p_data_prev = resources["power_data"].get(p_key)
                    if p_data_prev:
                        prev_power_val = p_data_prev["power"]

                # ==================================================
                # â˜… æ–‡å­—åˆ—ç”Ÿæˆ (ä¿®æ­£ï¼šç€é †ã®è¡¨ç¤ºåˆ†ã‘)
                # ==================================================
                agari_part = f"({agari})" if agari else ""
                pop_part = f"({pop})" if pop else ""
                
                # rankãŒæ•°å­—ãªã‚‰ã€Œç€ã€ã‚’ã¤ã‘ã‚‹ã€‚ãã‚Œä»¥å¤–ï¼ˆèƒ½è©¦ãƒ»å–æ¶ˆãªã©ï¼‰ãªã‚‰ãã®ã¾ã¾è¡¨ç¤ºã€‚
                if rank.isdigit():
                    rank_part = f"{rank}ç€"
                elif rank:
                    rank_part = rank  # "èƒ½è©¦", "å–æ¶ˆ" ãªã©
                else:
                    rank_part = "ç€ä¸æ˜"

                h_str = f"{d_txt} {place_short}{dist} {j_prev_full} {pas}{agari_part}â†’{rank_part}{pop_part}"
                history.append(h_str)
                # ==================================================
            # --- æœ€çµ‚è¡¨ç¤ºç”¨ ---
            if prev_power_val:
                power_line = f"ã€é¨æ‰‹ã€‘{curr_power_str}(å‰P:{prev_power_val})ã€ ç›¸æ€§:{pair_stats}"
            else:
                power_line = f"ã€é¨æ‰‹ã€‘{curr_power_str}ã€ ç›¸æ€§:{pair_stats}"

            data["horses"][umaban] = {
                "name": horse_name, "jockey": j_full, "trainer": t_full,
                "power": curr_power_str,
                "compat": pair_stats, "hist": history,
                "display_power": power_line
            }

        except Exception:
            continue
    return data
# ==================================================
# 5. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (URL, ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç­‰)
# ==================================================
def get_nankan_kai_nichi(month, day, place_name):
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    sess = get_http_session()
    try:
        res = sess.get(url, timeout=10)
        res.encoding = "cp932"
        soup = BeautifulSoup(res.text, "html.parser")
        target_m, target_d = int(month), int(day)
        for tr in soup.find_all("tr"):
            text = tr.get_text(" ", strip=True)
            if place_name not in text:
                continue
            kai_m = re.search(r"ç¬¬\s*(\d+)\s*å›", text)
            mon_m = re.search(r"(\d+)\s*æœˆ", text)
            if kai_m and mon_m and int(mon_m.group(1)) == target_m:
                days_part = text.split("æœˆ")[1]
                days_match = re.findall(r"(\d+)", days_part)
                days_list = [int(d) for d in days_match if 1 <= int(d) <= 31]
                if target_d in days_list:
                    return int(kai_m.group(1)), days_list.index(target_d) + 1
        return None, None
    except:
        return None, None

def get_kb_url_id(year, month, day, place_code, nichi, race_num):
    return f"{year}{str(month).zfill(2)}{str(place_code).zfill(2)}{str(nichi).zfill(2)}{str(race_num).zfill(2)}{str(month).zfill(2)}{str(day).zfill(2)}"

def parse_kb_danwa_cyokyo(driver, kb_id):
    d_danwa, d_cyokyo = {}, {}
    try:
        driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_id}")
        if "login" in driver.current_url:
            if login_keibabook_robust(driver):
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_id}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for tbl in soup.select("table.danwa"):
            curr = None
            for tr in tbl.select("tbody tr"):
                u = tr.select_one("td.umaban")
                if u:
                    curr = u.get_text(strip=True)
                    continue
                t = tr.select_one("td.danwa")
                if curr and t:
                    raw_text = t.get_text(" ", strip=True)
                    m = re.search(r"[â€•-]+(.*)", raw_text)
                    d_danwa[curr] = m.group(1).strip() if m else raw_text
                    curr = None

        driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_id}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for tbl in soup.select("table.cyokyo"):
            rows = tbl.select("tbody tr")
            if not rows:
                continue
            r1 = rows[0]
            u_td = r1.select_one("td.umaban")
            if not u_td:
                continue
            uma = u_td.get_text(strip=True)
            tp_txt = r1.select_one("td.tanpyo").get_text(strip=True) if r1.select_one("td.tanpyo") else ""
            dt_txt = ""
            if len(rows) > 1:
                dt_raw = rows[1].get_text(" ", strip=True)
                dt_txt = re.sub(r"\s+", " ", dt_raw)
            d_cyokyo[uma] = f"ã€çŸ­è©•ã€‘{tp_txt} ã€è©³ç´°ã€‘{dt_txt}"
    except:
        pass
    return d_danwa, d_cyokyo

def _parse_grades_from_ai(text):
    grades = {}
    for line in text.split("\n"):
        m = re.search(r"([SABCDE])\s*[:ï¼š]?\s*([^\sã€€]+)", line)
        if m:
            g, n = m.group(1), re.sub(r"[ï¼ˆ\(].*?[ï¼‰\)]", "", m.group(2)).strip()
            if n:
                grades[n] = g
    return grades

def _fetch_matchup_table_selenium(driver, nankan_id, grades):
    url = f"https://www.nankankeiba.com/taisen/{nankan_id}.do"
    try:
        driver.get(url)
        time.sleep(0.5)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        tbl = soup.find("table", class_="nk23_c-table08__table")
        if not tbl:
            return "\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—)"

        races = []
        if tbl.find("thead"):
            for col in tbl.find("thead").find_all(["th", "td"])[2:]:
                det = col.find(class_="nk23_c-table08__detail")
                if det:
                    link = col.find("a")
                    href = link.get("href", "") if link else ""
                    full_url = ""
                    if href:
                        id_match = re.search(r"(\d{10,})", href)
                        if id_match:
                            full_url = f"https://www.nankankeiba.com/result/{id_match.group(1)}.do"
                        elif href.startswith("/"):
                            full_url = "https://www.nankankeiba.com" + href
                        else:
                            full_url = href

                    races.append({
                        "title": det.get_text(" ", strip=True),
                        "url": full_url,
                        "results": []
                    })

        if not races:
            return "\n(åˆå¯¾æˆ¦)"

        if tbl.find("tbody"):
            for tr in tbl.find("tbody").find_all("tr"):
                u = tr.find("a", class_="nk23_c-table08__text")
                if not u:
                    continue
                name = u.get_text(strip=True)
                grade = grades.get(name, "")
                if not grade:
                    for k, v in grades.items():
                        if k in name or name in k:
                            grade = v
                            break
                cells = tr.find_all(["td", "th"])
                idx_st = -1
                for i, c in enumerate(cells):
                    if c.find("a", class_="nk23_c-table08__text"):
                        idx_st = i
                        break
                if idx_st == -1:
                    continue
                for i, c in enumerate(cells[idx_st + 1:]):
                    if i >= len(races):
                        break
                    rp = c.find("p", class_="nk23_c-table08__number")
                    rnk = ""
                    if rp:
                        sp = rp.find("span")
                        rnk = sp.get_text(strip=True) if sp else rp.get_text(strip=True).split("ï½œ")[0].strip()
                    if rnk and (rnk.isdigit() or rnk in ["é™¤å¤–", "ä¸­æ­¢"]):
                        races[i]["results"].append({"rank": rnk, "name": name, "grade": grade, "sort": int(rnk) if rnk.isdigit() else 999})

        out = ["\nã€å¯¾æˆ¦è¡¨ï¼ˆAIè©•ä¾¡ä»˜ãï¼‰ã€‘"]
        for r in races:
            if not r["results"]:
                continue
            r["results"].sort(key=lambda x: x["sort"])
            line_parts = []
            for x in r["results"]:
                g = f"[{x['grade']}]" if x["grade"] else ""
                line_parts.append(f"{x['rank']}ç€ {x['name']}{g}")
            out.append(f"â—† {r['title']}\n" + " / ".join(line_parts) + (f"\nLink: {r['url']}" if r["url"] else ""))

        return "\n".join(out)
    except Exception as e:
        return f"(å¯¾æˆ¦è¡¨å–å¾—ã‚¨ãƒ©ãƒ¼: {e})"

# ==================================================
# 6. ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, mode="dify", **kwargs):
    resources = load_resources()
    kb_input_map = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    nk_code_map = {"10": "20", "11": "21", "12": "19", "13": "18"}
    place_name = kb_input_map.get(place_code, "åœ°æ–¹")
    nk_place_code = nk_code_map.get(place_code)
    driver = get_driver()

    try:
        yield {"type": "status", "data": f"ğŸ“… é–‹å‚¬ç‰¹å®šä¸­ ({place_name})..."}
        kai, nichi = get_nankan_kai_nichi(month, day, place_name)
        if not kai:
            yield {"type": "error", "data": "é–‹å‚¬ç‰¹å®šå¤±æ•—"}
            return
        yield {"type": "status", "data": f"âœ… {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›®"}

        yield {"type": "status", "data": "ğŸ”‘ ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³ä¸­..."}
        login_keibabook_robust(driver)

        prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nk_place_code}.do"
        driver.get(prog_url)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        r_nums = []
        for a in soup.find_all("a", href=True):
            if f"{year}{month}{day}{nk_place_code}" in a["href"] and "uma_shosai" not in a["href"]:
                f = a["href"].split("/")[-1].replace(".do", "")
                if len(f) == 16:
                    r_nums.append(int(f[14:16]))
        r_nums = sorted(list(set(r_nums))) or range(1, 13)

        for r_num in r_nums:
            if target_races and r_num not in target_races:
                continue

            yield {"type": "status", "data": f"ğŸ‡ {r_num}R ãƒ‡ãƒ¼ã‚¿è§£æä¸­..."}

            try:
                nk_id = f"{year}{month}{day}{nk_place_code}{kai:02}{nichi:02}{r_num:02}"
                kb_id = get_kb_url_id(year, month, day, place_code, nichi, r_num)

                result_url = f"https://www.nankankeiba.com/result/{nk_id}.do"

                danwa, cyokyo = parse_kb_danwa_cyokyo(driver, kb_id)

                driver.get(f"https://www.nankankeiba.com/uma_shosai/{nk_id}.do")
                try:
                    driver.execute_script("if(typeof changeShosai === 'function'){ changeShosai('s1'); }")
                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "shosai_aria")))
                    time.sleep(1.0)
                except TimeoutException:
                    yield {"type": "error", "data": f"{r_num}R è©³ç´°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"}
                    continue

                nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)

                # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
                if not nk_data["horses"]:
                    for _ in range(2):
                        time.sleep(1)
                        driver.execute_script("if(typeof changeShosai === 'function'){ changeShosai('s1'); }")
                        time.sleep(1)
                        nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)
                        if nk_data["horses"]:
                            break

                if not nk_data["horses"]:
                    yield {"type": "error", "data": f"{r_num}R ãƒ‡ãƒ¼ã‚¿ãªã— (HTMLè§£æå¤±æ•—)"}
                    continue

                header = f"ãƒ¬ãƒ¼ã‚¹å:{r_num}R {nk_data['meta'].get('race_name','')} æ ¼:{nk_data['meta'].get('grade','')} ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                horse_texts = []
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h = nk_data["horses"][u]

                    power_line = h.get("display_power", f"ã€é¨æ‰‹ã€‘{h['power']}ã€ ç›¸æ€§:{h['compat']}")

                    block = [
                        f"[{u}]{h['name']} é¨:{h['jockey']} å¸«:{h['trainer']}",
                        f"è©±:{danwa.get(u,'ãªã—')}",
                        f"èª¿:{cyokyo.get(u,'ãƒ‡ãƒ¼ã‚¿ãªã—')}",
                        power_line,
                        "ã€è¿‘èµ°ã€‘"
                    ]
                    for idx, hs in enumerate(h["hist"]):
                        block.append(f"{hs}")
                    horse_texts.append("\n".join(block))

                full_prompt = header + "\n\n" + "\n\n".join(horse_texts)

                if mode == "raw":
                    yield {"type": "status", "data": f"ğŸ” {r_num}R å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."}
                    match_txt = _fetch_matchup_table_selenium(driver, nk_id, grades={})
                    final_text = f"{full_prompt}\n\n{match_txt}\n\nè©³ç´°ãƒªãƒ³ã‚¯: {result_url}"
                    yield {"type": "result", "race_num": r_num, "data": final_text}
                    time.sleep(1)
                    continue

                yield {"type": "status", "data": f"ğŸ¤– {r_num}R AIäºˆæ¸¬ä¸­..."}
                ai_out = run_dify_prediction(full_prompt)
                grades = _parse_grades_from_ai(ai_out)
                match_txt = _fetch_matchup_table_selenium(driver, nk_id, grades)
                ai_out_clean = re.sub(r"^\s*-{3,}\s*$", "", ai_out, flags=re.MULTILINE)
                ai_out_clean = re.sub(r"\n{3,}", "\n\n", ai_out_clean).strip()

                final_text = f"ğŸ“… {year}/{month}/{day} {place_name}{r_num}R\n\n=== ğŸ¤–AIäºˆæƒ³ ===\n{ai_out_clean}\n\n{match_txt}\n\nè©³ç´°ãƒªãƒ³ã‚¯: {result_url}"
                yield {"type": "result", "race_num": r_num, "data": final_text}
                time.sleep(15)

            except Exception as e:
                yield {"type": "error", "data": f"{r_num}R Error: {e}"}

    except Exception as e:
        yield {"type": "error", "data": f"Fatal: {e}"}
    finally:
        driver.quit()
