import time
import re
import os
import csv
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# ã€è¨­å®šã€‘ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# ==================================================
# ã€è¨­å®šã€‘ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ»API
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
@st.cache_resource
def load_resources():
    res = {"jockeys": [], "trainers": [], "power": {}}
    
    # 1. é¨æ‰‹ãƒªã‚¹ãƒˆ
    if os.path.exists(JOCKEY_FILE):
        try:
            with open(JOCKEY_FILE, "r", encoding="utf-8-sig") as f:
                res["jockeys"] = [line.strip().replace("ï¼Œ","").replace(",","").replace(" ","").replace("ã€€","") for line in f if line.strip()]
        except Exception as e: print(f"âš ï¸ Jockey load error: {e}")

    # 2. èª¿æ•™å¸«ãƒªã‚¹ãƒˆ
    if os.path.exists(TRAINER_FILE):
        try:
            with open(TRAINER_FILE, "r", encoding="utf-8-sig") as f:
                res["trainers"] = [line.strip().replace("ï¼Œ","").replace(",","").replace(" ","").replace("ã€€","") for line in f if line.strip()]
        except Exception as e: print(f"âš ï¸ Trainer load error: {e}")

    # 3. é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼ (CSV: 1åˆ—ç›®=å ´æ‰€, Cåˆ—=é¨æ‰‹å, ... é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼, å‹ç‡, è¤‡å‹ç‡)
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            # 1åˆ—ç›®ãŒå ´æ‰€(Unnamed: 0ã®å ´åˆã‚ã‚Š)ã€é¨æ‰‹åã‚«ãƒ©ãƒ ã‚’æ¢ã™
            place_col = df.columns[0]
            for _, row in df.iterrows():
                place = str(row[place_col]).strip()
                jockey = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                if place and jockey:
                    power = row.get("é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼", "")
                    win = row.get("å‹ç‡", "")
                    fuku = row.get("è¤‡å‹ç‡", "")
                    res["power"][(place, jockey)] = f"é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:{power}(å‹ç‡{win} è¤‡å‹ç‡{fuku})"
        except Exception as e: print(f"âš ï¸ Power load error: {e}")
    
    return res

def normalize_name(abbrev, full_list):
    """ ç•¥ç§°(æœ¨é–“é¾) -> æ­£å¼åç§°(æœ¨é–“å¡šé¾é¦¬) """
    if not abbrev: return ""
    clean = abbrev.replace(" ","").replace("ã€€","")
    if not full_list: return clean
    # å®Œå…¨ä¸€è‡´
    if clean in full_list: return clean
    # å‰æ–¹ä¸€è‡´æ¤œç´¢
    matches = [n for n in full_list if n.startswith(clean) or (len(clean)>=2 and n.startswith(clean[0]) and clean[1] in n)]
    if matches:
        # æœ€ã‚‚æ–‡å­—æ•°ãŒè¿‘ã„ã€ã‚ã‚‹ã„ã¯ãƒªã‚¹ãƒˆé †ã§æœ€åˆã®ã‚‚ã®ã‚’è¿”ã™
        return sorted(matches, key=len)[0]
    return clean

# ==================================================
# nankankeiba.com è§£æãƒ­ã‚¸ãƒƒã‚¯ (BeautifulSoup)
# ==================================================
def parse_nankankeiba_html(html, place_name, resources):
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}

    # --- 1. ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾— ---
    title_h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = title_h3.get_text(strip=True) if title_h3 else ""
    
    # æ ¼ä»˜ã‘ (ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡ºã€ã¾ãŸã¯åˆ¥ã®å ´æ‰€)
    # ä¾‹: "ç¬‘é–€æ¥ç¦è³ ï¼¢ï¼“(äºŒ)" -> æ ¼ä»˜ã‘ã¯ "ï¼¢ï¼“(äºŒ)"
    # ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ã—ã¦å¾Œã‚ã‚’å–å¾—ã™ã‚‹ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯
    if data["meta"]["race_name"]:
        parts = data["meta"]["race_name"].split(" ")
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    
    # ã‚³ãƒ¼ã‚¹æ¡ä»¶
    cond_a = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    if cond_a:
        # "ãƒ€1,600mï¼ˆå¤–ï¼‰" ã®ã‚ˆã†ãªå½¢å¼
        cond_text = cond_a.get_text(strip=True)
        data["meta"]["course"] = f"{place_name} {cond_text}"

    # --- 2. å„é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾— (è©³ç´°å‡ºèµ°è¡¨) ---
    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data

    rows = table.select("tbody tr")
    for row in rows:
        try:
            # é¦¬ç•ª
            umaban_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not umaban_tag: continue
            umaban = umaban_tag.get_text(strip=True)
            if not umaban.isdigit(): continue

            # é¦¬å
            horse_tag = row.select_one("td.is-col03 a.is-link")
            horse_name = horse_tag.get_text(strip=True) if horse_tag else ""

            # é¨æ‰‹ãƒ»èª¿æ•™å¸«
            jg_td = row.select_one("td.cs-g1")
            jockey_raw = ""
            trainer_raw = ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: jockey_raw = links[0].get_text(strip=True)
                if len(links) >= 2: trainer_raw = links[1].get_text(strip=True)
            
            # æ­£è¦åŒ–
            jockey_full = normalize_name(jockey_raw, resources["jockeys"])
            trainer_full = normalize_name(trainer_raw, resources["trainers"])

            # é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼å–å¾—
            power_info = resources["power"].get((place_name, jockey_full), "é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼:ä¸æ˜")

            # ç›¸æ€§ãƒ‡ãƒ¼ã‚¿ (é¨æ‰‹xèª¿æ•™å¸«)
            ai2_div = row.select_one("td.cs-ai2 .graph_text_div")
            stats_pair = "ãƒ‡ãƒ¼ã‚¿ãªã—"
            if ai2_div and "ãƒ‡ãƒ¼ã‚¿" not in ai2_div.get_text():
                rate = ai2_div.select_one(".is-percent").get_text(strip=True)
                win = ai2_div.select_one(".is-number").get_text(strip=True)
                total = ai2_div.select_one(".is-total").get_text(strip=True)
                stats_pair = f"å‹ç‡{rate}({win}å‹/{total}å›)"

            # --- è¿‘èµ°ãƒ‡ãƒ¼ã‚¿ (éå»3èµ°: cs-z1 ~ cs-z3) ---
            history = []
            for i in range(1, 4): # 1, 2, 3
                z_td = row.select_one(f"td.cs-z{i}")
                if not z_td or not z_td.get_text(strip=True): continue
                
                # é–‹å‚¬æ—¥ãƒ»å ´æ‰€ (ä¾‹: æµ¦å’Œ26.1.7)
                date_place_text = ""
                dp_span = z_td.select("p.nk23_u-d-flex span.nk23_u-text10")
                if dp_span:
                    # è¤‡æ•°ã‚ã‚‹å ´åˆã€æ—¥ä»˜ã£ã½ã„ã‚‚ã®ã‚’æ¢ã™
                    for s in dp_span:
                        txt = s.get_text(strip=True)
                        if re.search(r"\d+\.\d+\.\d+", txt):
                            date_place_text = txt
                            break
                
                # æ—¥ä»˜å¤‰æ› (æµ¦å’Œ26.1.7 -> 2026/01/07)
                # å¹´å·26ã¯2026å¹´ã¨ä»®å®š
                date_str = ""
                course_short = "" # æµ¦å’Œ1400m
                
                m = re.match(r"([^\d]+)(\d+)\.(\d+)\.(\d+)", date_place_text)
                if m:
                    place_short = m.group(1)
                    yy, mm, dd = m.group(2), m.group(3), m.group(4)
                    date_str = f"20{yy}/{int(mm):02}/{int(dd):02}"
                
                # ã‚³ãƒ¼ã‚¹ãƒ»æ¡ä»¶ (ç¨å¤–ãƒ€1400)
                cond_text = ""
                if len(dp_span) >= 2:
                    cond_text = dp_span[-1].get_text(strip=True)
                
                # è·é›¢æŠ½å‡º (1400)
                dist_m = re.search(r"\d{4}", cond_text)
                dist = dist_m.group(0) if dist_m else ""
                # å ´æ‰€åæŠ½å‡º (æ—¥ä»˜ã®é ­ã«ã¤ã„ã¦ã‚‹ã‚„ã¤)
                place_m = re.match(r"^[^\d]+", date_place_text)
                place_h = place_m.group(0) if place_m else ""
                
                course_str = f"{place_h}{dist}m"

                # ãƒ¬ãƒ¼ã‚¹åãƒ»ã‚¯ãƒ©ã‚¹
                race_a = z_td.select_one("a.is-link")
                race_title_full = race_a.get("title", "") if race_a else ""
                # "åˆå¤¢ï¼ˆã¯ã¤ã‚†ã‚ï¼‰ç‰¹åˆ¥ ï¼¢ï¼’(äºŒ)ï¼¢ï¼“(ä¸€)" -> åˆ†å‰²
                # ç©ºç™½ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
                r_parts = race_title_full.split(" ")
                race_name = r_parts[0] if r_parts else ""
                race_class = r_parts[1] if len(r_parts) > 1 else ""

                # é¨æ‰‹ (å°æ‰äº®55.0) -> åå‰ã ã‘æŠ½å‡º
                j_p_line = z_td.select("p.nk23_u-text10")
                jockey_prev = ""
                pop = ""
                rank = ""
                
                # äººæ°—ãƒ»é¨æ‰‹æƒ…å ±ã®è¡Œã‚’æ¢ã™
                for p in j_p_line:
                    ptxt = p.get_text(strip=True)
                    if "äººæ°—" in ptxt:
                        # "13é ­ 6ç•ª 9äººæ°—"
                        pop_m = re.search(r"(\d+)äººæ°—", ptxt)
                        if pop_m: pop = f"{pop_m.group(1)}äººæ°—"
                        
                        # åŒã˜è¡Œã®spanã«é¨æ‰‹ãŒã„ã‚‹å ´åˆãŒã‚ã‚‹ãŒã€æ§‹é€ ä¸Šåˆ¥ã‚¿ã‚°ã®å ´åˆã‚‚
                        # nankankeibaã¯ <p><span>äººæ°—</span><span>é¨æ‰‹</span></p>
                        spans = p.find_all("span")
                        if len(spans) > 1:
                            j_raw = spans[1].get_text(strip=True)
                            jockey_prev = re.sub(r"[\d\.]+", "", j_raw) # æ•°å­—é™¤å»

                # ç€é † (10ç€)
                rank_span = z_td.select_one(".nk23_u-text19")
                if rank_span:
                    rank = rank_span.get_text(strip=True).replace("ç€", "")

                # é€šéé †ãƒ»ä¸ŠãŒã‚Š (7-6-7-11 / 3F 39.9(10))
                pass_str = ""
                agari_str = ""
                
                pos_p = z_td.select_one("p.position")
                if pos_p:
                    pass_str = "-".join([s.get_text(strip=True) for s in pos_p.find_all("span")])
                
                time_p = z_td.select("p.nk23_u-text10")
                for p in time_p:
                    if "3F" in p.get_text():
                        # "1:30.3(1.2) 3F 39.9(10)"
                        ft = p.select_one(".furlongtime")
                        if ft:
                            ft_text = ft.get_text(strip=True)
                            # (10) ã‚’æŠ½å‡º
                            ag_m = re.search(r"\(([\d]+)\)", ft_text)
                            if ag_m:
                                agari_str = f"ä¸ŠãŒã‚Š3Fï¼š{ag_m.group(1)}ä½"

                # æ•´å½¢
                # â‘ é–‹å‚¬æ—¥ï¼š...
                hist_str = (f"é–‹å‚¬æ—¥ï¼š{date_str}ã€€ã‚³ãƒ¼ã‚¹ï¼š{course_str} ãƒ¬ãƒ¼ã‚¹ï¼š{race_name}ã€€"
                            f"ã‚¯ãƒ©ã‚¹ï¼š{race_class} é¨æ‰‹ï¼š{jockey_prev}ã€€"
                            f"é€šéé †{pass_str}({agari_str})â†’{rank}ç€ï¼ˆ{pop}ï¼‰")
                history.append(hist_str)

            data["horses"][umaban] = {
                "name": horse_name,
                "jockey": jockey_full,
                "trainer": trainer_full,
                "power": power_info,
                "compatibility": stats_pair,
                "history": history
            }

        except Exception as e:
            print(f"Parse error at row: {e}")
            continue
            
    return data

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ è§£æãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def parse_keibabook_danwa(html):
    soup = BeautifulSoup(html, "html.parser")
    d = {}
    tbl = soup.find("table", class_="danwa")
    if tbl and tbl.tbody:
        cur = None
        for row in tbl.tbody.find_all("tr"):
            u = row.find("td", class_="umaban")
            if u: cur = u.get_text(strip=True); continue
            t = row.find("td", class_="danwa")
            if t and cur: d[cur] = t.get_text(strip=True); cur=None
    return d

def parse_keibabook_cyokyo(html):
    soup = BeautifulSoup(html, "html.parser")
    d = {}
    for tbl in soup.find_all("table", class_="cyokyo"):
        tb = tbl.find("tbody")
        if not tb: continue
        rs = tb.find_all("tr", recursive=False)
        if not rs: continue
        u_td = rs[0].find("td", class_="umaban")
        if u_td:
            u = u_td.get_text(strip=True)
            tp = rs[0].find("td", class_="tanpyo").get_text(strip=True) if rs[0].find("td", class_="tanpyo") else ""
            dt = rs[1].get_text(" ", strip=True) if len(rs)>1 else ""
            d[u] = f"ã€çŸ­è©•ã€‘{tp} ã€è©³ç´°ã€‘{dt}"
    return d

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    resources = load_resources()
    
    # å ´æ‰€ã‚³ãƒ¼ãƒ‰å¤‰æ› (KeibaBook -> Nankan)
    # æµ¦å’Œ:18, èˆ¹æ©‹:19, å¤§äº•:20, å·å´:21
    # KB: 10=å¤§äº•, 11=å·å´, 12=èˆ¹æ©‹, 13=æµ¦å’Œ
    kb_to_nankan = {"10": "20", "11": "21", "12": "19", "13": "18"}
    nankan_place_code = kb_to_nankan.get(place_code)
    
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(place_code, "åœ°æ–¹")

    # Seleniumèµ·å‹•
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    wait = WebDriverWait(driver, 10)

    try:
        # 1. ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ­ã‚°ã‚¤ãƒ³
        if ui: st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        if "logout" not in driver.current_url:
            wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
            driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
            driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
            time.sleep(1)

        # 2. é–‹å‚¬å›ãƒ»æ—¥æ¬¡å–å¾— (nankankeiba URLç”Ÿæˆç”¨)
        # ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¦IDã‚’è§£æã™ã‚‹ã®ãŒç¢ºå®Ÿ
        date_str = f"{year}{month}{day}"
        kb_url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10"
        driver.get(kb_url)
        
        # ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—
        soup_kb = BeautifulSoup(driver.page_source, "html.parser")
        kb_race_ids = [] # (race_num, kb_id)
        for a in soup_kb.find_all("a", href=True):
            m = re.search(r"(\d{16})", a["href"])
            if m:
                rid = m.group(1)
                # å ´æ‰€ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if rid[6:8] == place_code:
                    r_num = int(rid[14:16])
                    kb_race_ids.append((r_num, rid))
        
        kb_race_ids.sort()
        
        # nankankeibaã®é–‹å‚¬å›ãƒ»æ—¥æ¬¡ç‰¹å®š (ç°¡æ˜“çš„ã«nankankeibaã®æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‚’è¦‹ã‚‹ã‹ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—)
        # ã“ã“ã§ã¯nankankeibaã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ç­‰ã‹ã‚‰å½“æ—¥ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™ãƒ­ã‚¸ãƒƒã‚¯ãŒè¤‡é›‘ãªãŸã‚ã€
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›æƒ…å ±ã«ã‚ã‚‹ã€Œ_get_kai_nichi_from_webã€ç›¸å½“ã®å‡¦ç†ãŒå¿…è¦ã§ã™ãŒã€
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€URLç”Ÿæˆã«å¿…è¦ãªã€Œå›ãƒ»æ—¥ã€ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—ã—ã¾ã™ã€‚
        
        # 3. ãƒ¬ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ—
        for r_num, kb_rid in kb_race_ids:
            if target_races and r_num not in target_races: continue
            
            if ui: st.markdown(f"## {place_name} {r_num}R")
            
            try:
                # --- A. ç«¶é¦¬ãƒ–ãƒƒã‚¯æƒ…å ±å–å¾— ---
                # è«‡è©±
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_rid}")
                danwa_dict = parse_keibabook_danwa(driver.page_source)
                # èª¿æ•™
                driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_rid}")
                cyokyo_dict = parse_keibabook_cyokyo(driver.page_source)

                # --- B. nankankeibaæƒ…å ±å–å¾— ---
                # URLã‚’ç‰¹å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®IDã«ã¯å›ãƒ»æ—¥ãŒå«ã¾ã‚Œãªã„(YYYYMMDDppRR00)
                # nankankeibaã¯ YYYYMMDDppKkDDRR (Kk=å›, DD=æ—¥)
                # é–‹å‚¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹
                if r_num == 1 or 'nk_base_url' not in locals():
                    # 1Rã®æ™‚ã«é–‹å‚¬æƒ…å ±ã‚’å–å¾—ã—ã€ãƒ™ãƒ¼ã‚¹URLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç‰¹å®š
                    nk_sched_url = "https://www.nankankeiba.com/calendar/000000.do"
                    driver.get(nk_sched_url)
                    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰å½“æ—¥ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™...ã®ã¯å¤§å¤‰ãªã®ã§
                    # ç›´æ¥å½“æ—¥ã®å‡ºèµ°è¡¨ä¸€è¦§ã¸ã‚¢ã‚¯ã‚»ã‚¹ (YYYYMMDDpp.do)
                    nk_prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nankan_place_code}.do"
                    driver.get(nk_prog_url)
                    # è©²å½“ãƒ¬ãƒ¼ã‚¹ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    lnk = driver.find_element(By.XPATH, f"//a[contains(@href, '{year}{month}{day}{nankan_place_code}') and contains(@href, '{str(r_num).zfill(2)}.do')]")
                    href = lnk.get_attribute('href')
                    # href = .../2026012119100301.do -> IDæŠ½å‡º
                    nk_id_full = href.split("/")[-1].replace(".do", "")
                    # ãƒ™ãƒ¼ã‚¹éƒ¨åˆ† (YYYYMMDDppKkDD)
                    nk_base_id = nk_id_full[:-2]
                
                # å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã®URL
                nk_race_url = f"https://www.nankankeiba.com/uma_shosai/{nk_base_id}{str(r_num).zfill(2)}.do"
                driver.get(nk_race_url)
                
                # è©³ç´°ãƒ‡ãƒ¼ã‚¿è§£æ
                nk_data = parse_nankankeiba_html(driver.page_source, place_name, resources)
                
                # --- C. ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»å‡ºåŠ›ç”Ÿæˆ ---
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼å‡ºåŠ›
                header_text = f"ãƒ¬ãƒ¼ã‚¹å: {r_num}R {nk_data['meta'].get('race_name','')}ã€€æ ¼ä»˜ã‘:{nk_data['meta'].get('grade','')}ã€€ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                output_lines = [header_text, ""]
                
                # é¦¬ã”ã¨ã®å‡ºåŠ›
                # nk_data['horses'] ã¯ umaban(str) ãŒã‚­ãƒ¼
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h_data = nk_data["horses"][u]
                    
                    # ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
                    danwa = danwa_dict.get(u, "ãªã—")
                    cyokyo = cyokyo_dict.get(u, "èª¿æ•™ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
                    # å‰èµ°é¨æ‰‹ã‚’å–å¾— (Historyã®1ç•ªç›®ã‹ã‚‰æŠ½å‡º)
                    prev_jockey = ""
                    if h_data["history"]:
                        # "é¨æ‰‹ï¼šå°æ‰äº®" ã‚’æ¢ã™
                        m = re.search(r"é¨æ‰‹ï¼š([^ã€€\s]+)", h_data["history"][0])
                        if m: prev_jockey = m.group(1)
                    
                    # å‰èµ°æƒ…å ±æ–‡å­—åˆ— (å‰èµ°:å°æ‰äº®)
                    prev_info = f" (å‰èµ°:{prev_jockey})" if prev_jockey else ""
                    
                    # åŸºæœ¬æƒ…å ±è¡Œ
                    line1 = f"[é¦¬ç•ª{u}] {h_data['name']} é¨æ‰‹:{h_data['jockey']}{prev_info} èª¿æ•™å¸«:{h_data['trainer']}"
                    
                    # è«‡è©±ãƒ»èª¿æ•™è¡Œ
                    line2 = f"è«‡è©±: {danwa} èª¿æ•™:{cyokyo}"
                    
                    # é¨æ‰‹ãƒ‡ãƒ¼ã‚¿è¡Œ
                    line3 = f"ã€é¨æ‰‹ã€‘{h_data['power']} ç›¸æ€§:{h_data['compatibility']}"
                    
                    # è¿‘èµ°ãƒ‡ãƒ¼ã‚¿è¡Œ
                    hist_lines = []
                    cn_map = {0:"â‘ ", 1:"â‘¡", 2:"â‘¢"}
                    for idx, h_str in enumerate(h_data["history"]):
                        hist_lines.append(f"ã€è¿‘èµ°ã€‘{cn_map.get(idx,'')} {h_str}")
                    
                    # çµåˆ
                    block = "\n".join([line1, line2, line3] + hist_lines)
                    output_lines.append(block + "\n") # ç©ºè¡ŒåŒºåˆ‡ã‚Š
                
                final_output = "\n".join(output_lines)
                
                if ui: st.text_area(f"{r_num}R å‡ºåŠ›çµæœ", final_output, height=300)
                yield (r_num, final_output)
                
                time.sleep(2)

            except Exception as e:
                yield (r_num, f"Error: {e}")
                
    finally:
        driver.quit()
