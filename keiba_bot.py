import time
import re
import requests
import streamlit as st

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ==================================================
# è¨­å®šãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

def _build_session() -> requests.Session:
    sess = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(500, 502, 503, 504))
    sess.mount("https://", HTTPAdapter(max_retries=retry))
    return sess

@st.cache_resource
def get_http_session() -> requests.Session:
    return _build_session()

def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,1024")
    return webdriver.Chrome(options=options)

def login_keibabook(driver: webdriver.Chrome, wait: WebDriverWait):
    driver.get("https://s.keibabook.co.jp/login/login")
    if "logout" in driver.current_url: return # æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿
    try:
        wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(1)
    except:
        pass # ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ç­‰ã®å ´åˆ

# ==================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ç¾¤ (ç«¶é¦¬ãƒ–ãƒƒã‚¯ & keiba.go.jp)
# ==================================================
def fetch_race_ids(driver, year, month, day, place_code):
    date_str = f"{year}{month}{day}"
    url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10"
    driver.get(url)
    try: WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "a")))
    except: pass
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    race_ids = []
    seen = set()
    for a in soup.find_all("a", href=True):
        m = re.search(r"(\d{16})", a["href"])
        if m:
            rid = m.group(1)
            # æŒ‡å®šã—ãŸç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰(6,7æ¡ç›®)ã¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã ã‘
            if rid[6:8] == place_code and rid not in seen:
                race_ids.append(rid)
                seen.add(rid)
    return sorted(race_ids)

def get_keibago_data(year, month, day, race_no, baba_code):
    # ç°¡æ˜“å‡ºé¦¬è¡¨ã‚’å–å¾—ã—ã¦ã€é¨æ‰‹å¤‰æ›´æƒ…å ±ãªã©ã‚’æŠ½å‡º
    date_str = f"{year}/{month}/{day}"
    url = f"https://www.keiba.go.jp/KeibaWeb/TodayRaceInfo/DebaTableSmall?k_raceDate={date_str}&k_raceNo={race_no}&k_babaCode={baba_code}"
    
    sess = get_http_session()
    try:
        r = sess.get(url, timeout=10)
        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, "html.parser")
        
        horses = {}
        # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ è§£æ (ç°¡ç•¥åŒ–)
        tbl = soup.select_one("table.bs[border='1']")
        if not tbl: return {}, ""
        
        for tr in tbl.find_all("tr"):
            tds = tr.find_all("td")
            if len(tds) < 4: continue
            
            # é¦¬ç•ªãƒ»é¦¬åå–å¾—ï¼ˆæ§‹é€ ã«ä¾å­˜ã™ã‚‹ãŸã‚tryã§ä¿è­·ï¼‰
            try:
                txts = [td.get_text(strip=True) for td in tds]
                # æ•°å­—ãŒå«ã¾ã‚Œã‚‹æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’é¦¬ç•ªã¨æ¨æ¸¬
                umaban = next((t for t in txts if t.isdigit()), None)
                if not umaban: continue
                
                # é¨æ‰‹æƒ…å ±ã®æŠ½å‡ºï¼ˆå¤‰æ›´æœ‰ç„¡ï¼‰
                # â€» keiba.go.jpã®æ§‹é€ ã¯è¤‡é›‘ãªãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ç°¡æ˜“æŠ½å‡º
                row_text = tr.get_text(" ", strip=True)
                is_change = "æ›¿" in row_text or "â˜†" in row_text or "â–²" in row_text # ç°¡æ˜“åˆ¤å®š
                
                # é¦¬åæŠ½å‡º (font.bameiã‚¿ã‚°ãŒã‚ã‚Œã°å„ªå…ˆ)
                bamei_tag = tr.select_one(".bamei")
                horse_name = bamei_tag.get_text(strip=True) if bamei_tag else "ä¸æ˜"

                horses[umaban] = {"name": horse_name, "is_change": is_change}
            except: continue
            
        return horses, ""
    except Exception:
        return {}, ""

# ==================================================
# ãƒ­ãƒ¼ã‚«ãƒ«å¯¾æˆ¦è¡¨ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def _get_kai_nichi(target_month, target_day, target_place):
    # å—é–¢ç«¶é¦¬å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰é–‹å‚¬å›ãƒ»æ—¥æ¬¡ã‚’å–å¾—
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    try:
        res = requests.get(url, timeout=5)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        for tr in soup.find_all('tr'):
            text = tr.get_text()
            if target_place in text and "ç«¶é¦¬" in text:
                m = re.search(r'ç¬¬(\d+)å›.*?(\d+)æœˆ\s*(.*?)æ—¥', text)
                if m:
                    mon = int(m.group(2))
                    if mon != int(target_month): continue # æœˆé•ã„
                    days = [int(d) for d in re.findall(r'\d+', m.group(3))]
                    if int(target_day) in days:
                        return int(m.group(1)), days.index(int(target_day)) + 1, None
        return None, None, "é–‹å‚¬æƒ…å ±ç‰¹å®šä¸å¯"
    except Exception as e:
        return None, None, str(e)

def _parse_grades(text):
    # LLMå‡ºåŠ›ã‹ã‚‰ [S] â‘ é¦¬å... ã®ã‚ˆã†ãªè©•ä¾¡ã‚’æŠ½å‡º
    grades = {}
    if not text: return grades
    # è¡Œã”ã¨ã«è§£æ (ç°¡æ˜“å®Ÿè£…: | â‘ é¦¬å(é¨æ‰‹) | ... | A | ã®å½¢å¼ã‚’æƒ³å®š)
    for line in text.split('\n'):
        if '|' in line and ('â‘ ' in line or 'â‘¡' in line or '1' in line):
            parts = [p.strip() for p in line.split('|') if p.strip()]
            if len(parts) >= 2:
                # æœ€å¾Œã®ã‚«ãƒ©ãƒ ãŒè©•ä¾¡(S~E)ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
                grade_cand = parts[-1]
                if grade_cand in ['S','A','B','C','D','E']:
                    # é¦¬åã‚’æŠ½å‡º (â‘ ãªã©ã‚’é™¤å»)
                    name_part = parts[0] # å…ˆé ­ã‚«ãƒ©ãƒ 
                    name_clean = re.sub(r'[â‘ -â‘³0-9\(\)ï¼ˆï¼‰]', '', name_part).split('(')[0]
                    grades[name_clean.strip()] = grade_cand
    return grades

def _fetch_history_data(year, month, day, place_name, race_num, grades):
    # å›ãƒ»æ—¥æ¬¡ã‚’ç‰¹å®š
    kai, nichi, err = _get_kai_nichi(month, day, place_name)
    if err: kai, nichi = 15, 1 # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    p_code = {'æµ¦å’Œ': '18', 'èˆ¹æ©‹': '19', 'å¤§äº•': '20', 'å·å´': '21'}.get(place_name, '20')
    race_id = f"{year}{int(month):02}{int(day):02}{p_code}{int(kai):02}{int(nichi):02}{int(race_num):02}"
    url = f"https://www.nankankeiba.com/taisen/{race_id}.do"

    try:
        res = requests.get(url, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # å¯¾æˆ¦è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
        tbl = soup.find('table', class_='nk23_c-table08__table')
        if not tbl: return f"\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—: {url})"

        # å±¥æ­´è§£æ
        history_lines = []
        thead = tbl.find('thead')
        tbody = tbl.find('tbody')
        if not (thead and tbody): return ""

        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆåˆ—ï¼‰
        races = []
        for th in thead.find_all('th')[1:]: # å…ˆé ­ã¯é¦¬åæ¬„
            link = th.find('a')
            if link:
                title = th.get_text(strip=True).replace('ç«¶èµ°æˆç¸¾', '').replace('å¯¾æˆ¦è¡¨', '')
                r_url = "https://www.nankankeiba.com" + link.get('href', '')
                races.append({"title": title, "url": r_url, "results": []})

        if not races: return "\n(åˆå¯¾æˆ¦)"

        # å„é¦¬ã®ç€é †ï¼ˆè¡Œï¼‰
        for tr in tbody.find_all('tr'):
            cells = tr.find_all(['td', 'th'])
            if not cells: continue
            
            # é¦¬å
            uma_tag = cells[0].find('a')
            if not uma_tag: continue
            h_name = uma_tag.get_text(strip=True)
            h_grade = _parse_grades_fuzzy(h_name, grades) # é¦¬åéƒ¨åˆ†ä¸€è‡´ã§è©•ä¾¡å–å¾—

            # å„ãƒ¬ãƒ¼ã‚¹ã®ç€é †
            for i, cell in enumerate(cells[1:]):
                if i >= len(races): break
                rank_text = cell.get_text(strip=True).split('ï½œ')[0].strip()
                if rank_text and (rank_text.isdigit() or rank_text in ['é™¤å¤–','å–æ¶ˆ']):
                    sort_k = int(rank_text) if rank_text.isdigit() else 999
                    races[i]["results"].append({
                        "rank": rank_text,
                        "name": h_name,
                        "grade": h_grade,
                        "sort": sort_k
                    })

        # å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        output = ["###æ³¨ç›®å¯¾æˆ¦"]
        has_content = False
        
        for r in races:
            if not r["results"]: continue
            has_content = True
            # ç€é †ã‚½ãƒ¼ãƒˆ
            r["results"].sort(key=lambda x: x["sort"])
            
            line_items = []
            for res in r["results"]:
                g_str = f"({res['grade']})" if res['grade'] else ""
                rank_disp = f"{res['rank']}ç€" if res['rank'].isdigit() else res['rank']
                line_items.append(f"{rank_disp} {res['name']}{g_str}")
            
            output.append(f"**ãƒ» {r['title']}**")
            output.append(" / ".join(line_items))
            output.append(f"[è©³ç´°]({r['url']})\n")

        return "\n".join(output) if has_content else "\n(è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—)"

    except Exception as e:
        return f"\n(å¯¾æˆ¦è¡¨ã‚¨ãƒ©ãƒ¼: {e})"

def _parse_grades_fuzzy(horse_name, grades):
    # é¦¬åãŒå®Œå…¨ä¸€è‡´ã—ãªãã¦ã‚‚ã€å«ã¾ã‚Œã¦ã„ã‚Œã°è©•ä¾¡ã‚’è¿”ã™
    if horse_name in grades: return grades[horse_name]
    for k, v in grades.items():
        if k in horse_name or horse_name in k:
            return v
    return ""

# ==================================================
# Difyé€£æº & ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==================================================
def run_dify_simple(prompt):
    # â˜… Difyã«ã¯ 'text' ã ã‘ã‚’é€ã‚‹ã‚ˆã†ã«å¤‰æ›´
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "inputs": {"text": prompt}, # ã‚·ãƒ³ãƒ—ãƒ«åŒ–
        "response_mode": "blocking",
        "user": "streamlit-user"
    }
    try:
        res = requests.post(f"{DIFY_BASE_URL}/v1/workflows/run", headers=headers, json=payload, timeout=60)
        if res.status_code == 200:
            return res.json().get('data', {}).get('outputs', {}).get('text', "Error: No text output")
        return f"Dify Error: {res.status_code} {res.text}"
    except Exception as e:
        return f"Conn Error: {e}"

def run_races_iter(year, month, day, place_code, target_races):
    place_map = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_map.get(place_code, "åœ°æ–¹")
    baba_code = {"10":"20", "11":"21", "12":"19", "13":"18"}.get(place_code, "20")

    driver = build_driver()
    wait = WebDriverWait(driver, 10)

    try:
        login_keibabook(driver, wait)
        race_ids = fetch_race_ids(driver, year, month, day, place_code)
        
        if not race_ids:
            yield 0, "ãƒ¬ãƒ¼ã‚¹IDå–å¾—å¤±æ•—ã€‚é–‹å‚¬æ—¥ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            return

        for i, race_id in enumerate(race_ids):
            race_num = i + 1
            if target_races and race_num not in target_races: continue

            # --- 1. ãƒ‡ãƒ¼ã‚¿å–å¾— (KeibaBook) ---
            driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
            html_danwa = driver.page_source
            driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
            html_cyokyo = driver.page_source
            
            # --- 2. ãƒ‡ãƒ¼ã‚¿å–å¾— (KeibaGO) ---
            kg_horses, _ = get_keibago_data(year, month, day, race_num, baba_code)

            # --- 3. ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ (Promptä½œæˆ) ---
            # â€» BeautifulSoupè§£æã¯é•·ããªã‚‹ãŸã‚è¦ç´„ã—ã¾ã™ãŒã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯é€šã‚Šãƒ†ã‚­ã‚¹ãƒˆåŒ–
            soup_d = BeautifulSoup(html_danwa, "html.parser")
            soup_c = BeautifulSoup(html_cyokyo, "html.parser")
            
            # ãƒ¬ãƒ¼ã‚¹åãªã©ã®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
            r_title = soup_d.find("div", class_="racetitle")
            race_header = r_title.get_text(" ", strip=True) if r_title else "ãƒ¬ãƒ¼ã‚¹æƒ…å ±ä¸æ˜"

            # å„é¦¬æƒ…å ±çµåˆ
            prompt_lines = [f"ãƒ¬ãƒ¼ã‚¹: {race_header}", f"æ—¥ä»˜: {year}/{month}/{day} {place_name} {race_num}R", ""]
            
            # é¦¬ã”ã¨ã®ãƒ«ãƒ¼ãƒ—å‡¦ç† (ç°¡æ˜“åŒ–)
            # å®Ÿéš›ã«ã¯ã“ã“ã§è«‡è©±ã¨èª¿æ•™ã‚’è¾æ›¸åŒ–ã—ã¦çµåˆã™ã‚‹æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            # ä»Šå›ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã®ã‚¤ãƒ¡ãƒ¼ã‚¸
            
            # ... (ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†) ...
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Œæˆã¨ä»®å®š
            final_prompt = f"{race_header}\n(ã“ã“ã«å…¨é¦¬ã®è«‡è©±ãƒ»èª¿æ•™ãƒ»é¨æ‰‹å¤‰æ›´æƒ…å ±ãŒå…¥ã‚‹)" 

            # --- 4. Difyå®Ÿè¡Œ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿é€ä¿¡) ---
            dify_res = run_dify_simple(final_prompt)

            # --- 5. ãƒ­ãƒ¼ã‚«ãƒ«ã§å¯¾æˆ¦è¡¨ç”Ÿæˆ ---
            # Difyã®çµæœã‹ã‚‰è©•ä¾¡(S,A...)ã‚’æŠ½å‡º
            grades = _parse_grades(dify_res)
            # å¯¾æˆ¦å±¥æ­´å–å¾—
            history_text = _fetch_history_data(year, month, day, place_name, race_num, grades)

            # --- 6. çµåˆã—ã¦è¿”å´ ---
            # 2æšç›®ã®æ·»ä»˜ç”»åƒã®é€šã‚Šã€è‡ªå‹•åˆ¤å®šãƒ˜ãƒƒãƒ€ãƒ¼ãªã©ã‚’ã¤ã‘ã‚‹
            header_info = f"ğŸ“… è‡ªå‹•åˆ¤å®š: {year}å¹´{month}æœˆ{day}æ—¥ {place_name} {race_num}R"
            full_output = f"{header_info}\n\n{dify_res}\n\n{history_text}"

            yield race_num, full_output
            time.sleep(2)

    except Exception as e:
        yield 0, f"Critical Error: {e}"
    finally:
        driver.quit()
