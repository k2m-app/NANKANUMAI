import time
import re
import os
import csv
import json
import requests
import streamlit as st
import pandas as pd
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==================================================
# ã€è¨­å®šã€‘ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
# ==================================================
DATA_DIR = "2025data"
JOCKEY_FILE = os.path.join(DATA_DIR, "2025_NARJockey.csv")
TRAINER_FILE = os.path.join(DATA_DIR, "2025_NankanTrainer.csv")
POWER_FILE = os.path.join(DATA_DIR, "2025_é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼.csv")

# ==================================================
# ã€è¨­å®šã€‘ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ»API
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# å…±é€šé–¢æ•°ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (å¤‰æ›´ãªã—)
# ==================================================
@st.cache_resource
def get_http_session() -> requests.Session:
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"
    })
    retry = Retry(total=3, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("https://", adapter)
    sess.mount("http://", adapter)
    return sess

def login_keibabook_robust(driver):
    try:
        driver.get("https://s.keibabook.co.jp/login/login")
        if "logout" not in driver.current_url:
            WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
            driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
            driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
            time.sleep(1)
    except Exception as e:
        print(f"Login Warning: {e}")

def run_dify_prediction(full_text):
    if not DIFY_API_KEY: return "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
    url = f"{(DIFY_BASE_URL or '').strip().rstrip('/')}/v1/workflows/run"
    payload = {
        "inputs": {"text": full_text}, 
        "response_mode": "streaming", 
        "user": "keiba-bot"
    }
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}", 
        "Content-Type": "application/json"
    }
    sess = get_http_session()
    full_response = ""
    try:
        with sess.post(url, headers=headers, json=payload, stream=True, timeout=120) as res:
            if res.status_code != 200:
                return f"âš ï¸ Dify Error ({res.status_code}): {res.text[:200]}..."
            for line in res.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith('data:'):
                        json_str = decoded_line[5:].strip()
                        if not json_str: continue
                        try:
                            data = json.loads(json_str)
                            event = data.get('event')
                            if event == 'workflow_finished':
                                outputs = data.get('data', {}).get('outputs', {})
                                if 'text' in outputs: return outputs['text']
                            elif event == 'text_chunk' or event == 'message':
                                chunk = data.get('data', {}).get('text', '')
                                full_response += chunk
                        except: pass
        return full_response if full_response else "ï¼ˆå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼‰"
    except Exception as e:
        return f"âš ï¸ API Connection Error: {e}"

@st.cache_resource
def load_resources():
    res = {"jockeys": [], "trainers": [], "power": {}, "power_data": {}}
    if os.path.exists(JOCKEY_FILE):
        try:
            with open(JOCKEY_FILE, "r", encoding="utf-8-sig") as f:
                res["jockeys"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    if os.path.exists(TRAINER_FILE):
        try:
            with open(TRAINER_FILE, "r", encoding="utf-8-sig") as f:
                res["trainers"] = [l.strip().replace(",","").replace(" ","").replace("ã€€","") for l in f if l.strip()]
        except: pass
    if os.path.exists(POWER_FILE):
        try:
            df = pd.read_csv(POWER_FILE, encoding="utf-8-sig")
            place_col = df.columns[0]
            for _, row in df.iterrows():
                p = str(row[place_col]).strip()
                j = str(row.get("é¨æ‰‹å", "")).replace(" ","").replace("ã€€","")
                if p and j:
                    power_val = row.get('é¨æ‰‹ãƒ‘ãƒ¯ãƒ¼','-')
                    win = row.get('å‹ç‡','-')
                    fuku = row.get('è¤‡å‹ç‡','-')
                    info = f"P:{power_val}(å‹{win} è¤‡{fuku})" 
                    key = (p, j)
                    res["power"][key] = info
                    res["power_data"][key] = {"power": power_val, "win": win, "fuku": fuku}
        except: pass
    return res

def normalize_name(abbrev, full_list):
    if not abbrev: return ""
    clean = re.sub(r"[ ã€€â–²â–³â˜†â—‡â˜…\d\.]+", "", abbrev)
    if not clean: return ""
    if not full_list: return clean
    if clean in full_list: return clean
    candidates = []
    for full in full_list:
        it = iter(full)
        if all(char in it for char in clean):
            diff = len(full) - len(clean)
            candidates.append((diff, full))
    if candidates:
        candidates.sort(key=lambda x: x[0])
        return candidates[0][1]
    return clean

# ==================================================
# é–‹å‚¬ç‰¹å®š & URLç”Ÿæˆ (å¤‰æ›´ãªã—)
# ==================================================
def get_nankan_kai_nichi(month, day, place_name):
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    sess = get_http_session()
    try:
        res = sess.get(url, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        target_m, target_d = int(month), int(day)
        for tr in soup.find_all('tr'):
            text = tr.get_text(" ", strip=True)
            if place_name not in text: continue
            kai_m = re.search(r'ç¬¬\s*(\d+)\s*å›', text)
            if not kai_m: continue
            kai = int(kai_m.group(1))
            mon_m = re.search(r'(\d+)\s*æœˆ', text)
            if not mon_m: continue
            if int(mon_m.group(1)) != target_m: continue
            days_part = text.split("æœˆ")[1]
            days_match = re.findall(r'(\d+)', days_part)
            days_list = [int(d) for d in days_match if 1 <= int(d) <= 31]
            if target_d in days_list:
                return kai, days_list.index(target_d) + 1
        return None, None
    except: return None, None

def get_kb_url_id(year, month, day, place_code, nichi, race_num):
    mm, dd = str(month).zfill(2), str(day).zfill(2)
    p, n, r = str(place_code).zfill(2), str(nichi).zfill(2), str(race_num).zfill(2)
    return f"{year}{mm}{p}{n}{r}{mm}{dd}"

# ==================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
# ==================================================
def parse_kb_danwa_cyokyo(driver, kb_id):
    d_danwa, d_cyokyo = {}, {}
    try:
        driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_id}")
        if "login" in driver.current_url:
            login_keibabook_robust(driver)
            driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{kb_id}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for tbl in soup.select("table.danwa"):
            curr = None
            for tr in tbl.select("tbody tr"):
                u = tr.select_one("td.umaban")
                if u: curr = u.get_text(strip=True); continue
                t = tr.select_one("td.danwa")
                if curr and t: 
                    raw_text = t.get_text(" ", strip=True)
                    m = re.search(r'[â€•-]+(.*)', raw_text)
                    if m: d_danwa[curr] = m.group(1).strip()
                    else: d_danwa[curr] = raw_text 
                    curr = None
        driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{kb_id}")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for tbl in soup.select("table.cyokyo"):
            rows = tbl.select("tbody tr")
            if not rows: continue
            r1 = rows[0]
            u_td = r1.select_one("td.umaban")
            if not u_td: continue
            uma = u_td.get_text(strip=True)
            tp_txt = r1.select_one("td.tanpyo").get_text(strip=True) if r1.select_one("td.tanpyo") else ""
            dt_txt = ""
            if len(rows) > 1:
                dt_raw = rows[1].get_text(" ", strip=True)
                dt_txt = re.sub(r'\s+', ' ', dt_raw)
            d_cyokyo[uma] = f"ã€çŸ­è©•ã€‘{tp_txt} ã€è©³ç´°ã€‘{dt_txt}"
    except: pass
    return d_danwa, d_cyokyo

def parse_nankankeiba_detail(html, place_name, resources):
    soup = BeautifulSoup(html, "html.parser")
    data = {"meta": {}, "horses": {}}
    h3 = soup.find("h3", class_="nk23_c-tab1__title")
    data["meta"]["race_name"] = h3.get_text(strip=True) if h3 else ""
    if data["meta"]["race_name"]:
        parts = re.split(r'[ ã€€]+', data["meta"]["race_name"])
        data["meta"]["grade"] = parts[-1] if len(parts) > 1 else ""
    cond = soup.select_one("a.nk23_c-tab1__subtitle__text.is-blue")
    data["meta"]["course"] = f"{place_name} {cond.get_text(strip=True)}" if cond else ""
    table = soup.select_one("#shosai_aria table.nk23_c-table22__table")
    if not table: return data
    for row in table.select("tbody tr"):
        try:
            u_tag = row.select_one("td.umaban") or row.select_one("td.is-col02")
            if not u_tag: continue
            umaban = u_tag.get_text(strip=True)
            if not umaban.isdigit(): continue
            horse_name = "ä¸æ˜"
            h_link = row.select_one("td.is-col03 a.is-link") or row.select_one("td.pr-umaName-textRound a.is-link")
            if h_link: horse_name = h_link.get_text(strip=True)
            else:
                td3 = row.select_one("td.is-col03")
                if td3:
                    txt = td3.get_text(" ", strip=True)
                    m_nm = re.search(r'[ã‚¡-ãƒ³ãƒ¼]{2,9}', txt)
                    if m_nm: horse_name = m_nm.group(0)
            jg_td = row.select_one("td.cs-g1")
            j_raw, t_raw = "", ""
            if jg_td:
                links = jg_td.select("a")
                if len(links) >= 1: j_raw = links[0].get_text(strip=True)
                if len(links) >= 2: t_raw = links[1].get_text(strip=True)
            j_full = normalize_name(j_raw, resources["jockeys"])
            t_full = normalize_name(t_raw, resources["trainers"])
            power_info = resources["power"].get((place_name, j_full), "P:ä¸æ˜")
            ai2 = row.select_one("td.cs-ai2 .graph_text_div")
            pair_stats = "-"
            if ai2 and "ãƒ‡ãƒ¼ã‚¿" not in ai2.get_text():
                r = ai2.select_one(".is-percent").get_text(strip=True)
                w = ai2.select_one(".is-number").get_text(strip=True)
                t = ai2.select_one(".is-total").get_text(strip=True)
                pair_stats = f"å‹{r}({w}/{t})" 
            history = []
            prev_power_info = ""
            for i in range(1, 4):
                z = row.select_one(f"td.cs-z{i}")
                if not z or not z.get_text(strip=True): continue
                d_spans = z.select("p.nk23_u-d-flex span.nk23_u-text10")
                d_txt = ""
                if d_spans:
                    for s in d_spans:
                        if re.search(r"\d+\.\d+\.\d+", s.get_text()): d_txt = s.get_text(strip=True); break
                ymd, place_short = "", ""
                m = re.match(r"([^\d]+)(\d+)\.(\d+)\.(\d+)", d_txt)
                if m:
                    place_short = m.group(1)
                    ymd = f"{m.group(2)}/{int(m.group(3))}/{int(m.group(4))}"
                cond_txt = d_spans[-1].get_text(strip=True) if len(d_spans)>=2 else ""
                dist_m = re.search(r"\d{4}", cond_txt)
                dist = dist_m.group(0) if dist_m else ""
                course_s = f"{place_short}{dist}" if m else cond_txt
                r_a = z.select_one("a.is-link")
                r_ti = r_a.get("title", "") if r_a else ""
                rp = re.split(r'[ ã€€]+', r_ti)
                r_cl = rp[1] if len(rp)>1 else ""
                p_lines = z.select("p.nk23_u-text10")
                j_prev, pop, agari = "", "", ""
                rank = z.select_one(".nk23_u-text19").get_text(strip=True).replace("ç€","") if z.select_one(".nk23_u-text19") else ""
                pos_p = z.select_one("p.position")
                pas = "-".join([s.get_text(strip=True) for s in pos_p.find_all("span")]) if pos_p else ""
                for p in p_lines:
                    pt = p.get_text(strip=True)
                    if "äººæ°—" in pt:
                        pm = re.search(r"(\d+)äººæ°—", pt)
                        if pm: pop = f"{pm.group(1)}äºº"
                        sps = p.find_all("span")
                        if len(sps) > 1: j_prev = sps[1].get_text(strip=True)
                    if "3F" in pt:
                        am = re.search(r"\(([\d]+)\)", pt)
                        if am: agari = f"3F{am.group(1)}ä½"
                j_prev_full = normalize_name(j_prev, resources["jockeys"])
                if i == 1:
                    p_data = resources["power_data"].get((place_short, j_prev_full))
                    if p_data: prev_power_info = f"å‰P:{p_data['power']}"
                h_str = f"{ymd} {course_s} {r_cl} {j_prev_full} {pas}({agari})â†’{rank}ç€({pop})"
                history.append(h_str)
            data["horses"][umaban] = {
                "name": horse_name, "jockey": j_full, "trainer": t_full,
                "power": power_info, "prev_power": prev_power_info,
                "compat": pair_stats, "hist": history, 
                "prev_jockey_name": history[0].split(" ")[3] if history else ""
            }
        except Exception: continue
    return data

def _parse_grades_from_ai(text):
    grades = {}
    for line in text.split('\n'):
        m = re.search(r'([SABCDE])\s*[:ï¼š]?\s*([^\sã€€]+)', line)
        if m:
            g, n = m.group(1), re.sub(r'[ï¼ˆ\(].*?[ï¼‰\)]', '', m.group(2)).strip()
            if n: grades[n] = g
    return grades

def _fetch_matchup_table(nankan_id, grades):
    url = f"https://www.nankankeiba.com/taisen/{nankan_id}.do"
    sess = get_http_session()
    try:
        soup = BeautifulSoup(sess.get(url, timeout=10).content, 'html.parser', from_encoding='cp932')
        tbl = soup.find('table', class_='nk23_c-table08__table')
        if not tbl: return "\n(å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—)"
        races = []
        if tbl.find('thead'):
            for col in tbl.find('thead').find_all(['th','td'])[2:]:
                det = col.find(class_='nk23_c-table08__detail')
                if det:
                    link = col.find('a')
                    races.append({
                        "title": det.get_text(" ", strip=True),
                        "url": "https://www.nankankeiba.com" + link.get('href','') if link else "",
                        "results": []
                    })
        if not races: return "\n(åˆå¯¾æˆ¦)"
        if tbl.find('tbody'):
            for tr in tbl.find('tbody').find_all('tr'):
                u = tr.find('a', class_='nk23_c-table08__text')
                if not u: continue
                name = u.get_text(strip=True)
                grade = grades.get(name, "")
                if not grade:
                    for k,v in grades.items():
                        if k in name or name in k: grade = v; break
                cells = tr.find_all(['td','th'])
                idx_st = -1
                for i, c in enumerate(cells):
                    if c.find('a', class_='nk23_c-table08__text'): idx_st=i; break
                if idx_st == -1: continue
                for i, c in enumerate(cells[idx_st+1:]):
                    if i >= len(races): break
                    rp = c.find('p', class_='nk23_c-table08__number')
                    rnk = ""
                    if rp:
                        sp = rp.find('span')
                        rnk = sp.get_text(strip=True) if sp else rp.get_text(strip=True).split('ï½œ')[0].strip()
                    if rnk and (rnk.isdigit() or rnk in ['é™¤å¤–','ä¸­æ­¢']):
                        races[i]["results"].append({"rank":rnk, "name":name, "grade":grade, "sort":int(rnk) if rnk.isdigit() else 999})
        out = ["\nã€å¯¾æˆ¦è¡¨ï¼ˆAIè©•ä¾¡ä»˜ãï¼‰ã€‘"]
        for r in races:
            if not r["results"]: continue
            r["results"].sort(key=lambda x:x["sort"])
            line_parts = []
            for x in r["results"]:
                g = f"[{x['grade']}]" if x['grade'] else ""
                line_parts.append(f"{x['rank']}ç€ {x['name']}{g}")
            out.append(f"â—† {r['title']}\n" + " / ".join(line_parts) + f"\nè©³ç´°: {r['url']}\n")
        return "\n".join(out)
    except: return "(å¯¾æˆ¦è¡¨ã‚¨ãƒ©ãƒ¼)"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (å¤‰æ›´éƒ¨åˆ†: Generator)
# ==================================================
def run_races_iter(year, month, day, place_code, target_races):
    """
    UIä¾å­˜ã‚’æ’é™¤ã—ã€ç´”ç²‹ã«ãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ³ã¨çµæœã‚’Yieldã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
    yield (type, content)
    type: 'status', 'error', 'result'
    """
    resources = load_resources()
    kb_input_map = {"10":"å¤§äº•", "11":"å·å´", "12":"èˆ¹æ©‹", "13":"æµ¦å’Œ"}
    nk_code_map = {"10":"20", "11":"21", "12":"19", "13":"18"}
    place_name = kb_input_map.get(place_code, "åœ°æ–¹")
    nk_place_code = nk_code_map.get(place_code)

    ops = Options()
    ops.add_argument("--headless=new")
    ops.add_argument("--no-sandbox")
    ops.add_argument("--disable-dev-shm-usage")
    ops.add_argument("user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1")
    
    driver = webdriver.Chrome(options=ops)
    
    try:
        yield ("status", f"ğŸ“… {place_name} é–‹å‚¬ç‰¹å®šä¸­...")
        kai, nichi = get_nankan_kai_nichi(month, day, place_name)
        if not kai: 
            yield ("error", f"é–‹å‚¬ç‰¹å®šå¤±æ•—: {month}/{day} {place_name}")
            return
        
        yield ("status", f"âœ… {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›® - ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook_robust(driver)

        prog_url = f"https://www.nankankeiba.com/program/{year}{month}{day}{nk_place_code}.do"
        driver.get(prog_url)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        r_nums = []
        for a in soup.find_all("a", href=True):
            if f"{year}{month}{day}{nk_place_code}" in a["href"] and "uma_shosai" not in a["href"]:
                f = a["href"].split("/")[-1].replace(".do","")
                if len(f)==16: r_nums.append(int(f[14:16]))
        r_nums = sorted(list(set(r_nums))) or range(1, 13)

        for r_num in r_nums:
            if target_races and r_num not in target_races: continue
            
            yield ("status", f"ğŸ‡ {r_num}R ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
            
            try:
                nk_id = f"{year}{month}{day}{nk_place_code}{kai:02}{nichi:02}{r_num:02}"
                kb_id = get_kb_url_id(year, month, day, place_code, nichi, r_num)
                
                danwa, cyokyo = parse_kb_danwa_cyokyo(driver, kb_id)
                driver.get(f"https://www.nankankeiba.com/uma_shosai/{nk_id}.do")
                nk_data = parse_nankankeiba_detail(driver.page_source, place_name, resources)
                
                if not nk_data["horses"]: 
                    yield ("result", (r_num, f"âš ï¸ {r_num}R ãƒ‡ãƒ¼ã‚¿ãªã—"))
                    continue

                header = f"ãƒ¬ãƒ¼ã‚¹å:{r_num}R {nk_data['meta'].get('race_name','')} æ ¼:{nk_data['meta'].get('grade','')} ã‚³ãƒ¼ã‚¹:{nk_data['meta'].get('course','')}"
                horse_texts = []
                
                for u in sorted(nk_data["horses"].keys(), key=int):
                    h = nk_data["horses"][u]
                    p_jockey = h.get("prev_jockey_name", "")
                    p_info = f"(å‰:{p_jockey})" if p_jockey else ""
                    power_line = f"ã€é¨æ‰‹ã€‘{h['power']}ã€{h['prev_power']} ç›¸æ€§:{h['compat']}"
                    block = [
                        f"[{u}]{h['name']} é¨:{h['jockey']}{p_info} å¸«:{h['trainer']}",
                        f"è©±:{danwa.get(u,'ãªã—')}", 
                        f"èª¿:{cyokyo.get(u,'ãƒ‡ãƒ¼ã‚¿ãªã—')}",
                        power_line,
                        "ã€è¿‘èµ°ã€‘"
                    ]
                    cn_map = {0:"[å‰èµ°]", 1:"[2èµ°å‰]", 2:"[3èµ°å‰]"}
                    for idx, hs in enumerate(h["hist"]):
                        prefix = cn_map.get(idx, f"[{idx+1}èµ°]")
                        block.append(f"{prefix} {hs}")
                    horse_texts.append("\n".join(block))
                
                full_prompt = header + "\n\n" + "\n\n".join(horse_texts)
                
                yield ("status", f"ğŸ¤– {r_num}R AIåˆ†æä¸­...")
                ai_out = run_dify_prediction(full_prompt)
                
                grades = _parse_grades_from_ai(ai_out)
                match_txt = _fetch_matchup_table(nk_id, grades)
                
                ai_out_clean = re.sub(r'^\s*-{3,}\s*$', '', ai_out, flags=re.MULTILINE)
                ai_out_clean = re.sub(r'\n{3,}', '\n\n', ai_out_clean).strip()

                final = f"ğŸ“… {year}/{month}/{day} {place_name}{r_num}R\n\n=== ğŸ¤–AIäºˆæƒ³ ===\n{ai_out_clean}\n\n{match_txt}"
                
                yield ("result", (r_num, final))
                time.sleep(1)

            except Exception as e:
                yield ("result", (r_num, f"Error {r_num}R: {e}"))
    except Exception as e:
        yield ("error", f"Global Error: {e}")
    finally:
        driver.quit()

# ==================================================
# Streamlit UI (ã“ã“ãŒé‡è¦ï¼šStateç®¡ç†ç‰ˆ)
# ==================================================
def main():
    st.set_page_config(page_title="NANKAN AI Master", layout="wide")
    st.title("ğŸ‡ NANKAN AI Prediction System")

    # --- Sidebar ---
    st.sidebar.header("è¨­å®š")
    today = datetime.now()
    col1, col2 = st.sidebar.columns(2)
    s_year = col1.number_input("å¹´", value=today.year)
    s_month = col2.number_input("æœˆ", value=today.month)
    s_day = st.sidebar.number_input("æ—¥", value=today.day)
    s_place = st.sidebar.selectbox("å ´æ‰€", 
        options=["10","11","12","13"], 
        format_func=lambda x: {"10":"å¤§äº•","11":"å·å´","12":"èˆ¹æ©‹","13":"æµ¦å’Œ"}.get(x)
    )
    
    # å¯¾è±¡ãƒ¬ãƒ¼ã‚¹é¸æŠ
    target_race_input = st.sidebar.text_input("ãƒ¬ãƒ¼ã‚¹æŒ‡å®š (ä¾‹: 1,2,11 / ç©ºç™½ã§å…¨R)", "")
    
    # --- Session State åˆæœŸåŒ– (ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã®ãŸã‚) ---
    if "results" not in st.session_state:
        st.session_state.results = {}  # {r_num: text}
    if "logs" not in st.session_state:
        st.session_state.logs = []

    # --- å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
    if st.sidebar.button("åˆ†æé–‹å§‹", type="primary"):
        target_races = [int(x.strip()) for x in target_race_input.split(",")] if target_race_input.strip() else []
        
        # ãƒ­ã‚°ã‚¯ãƒªã‚¢ã¯ä»»æ„
        st.session_state.logs = []
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠ
        status_container = st.status("ğŸš€ å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...", expanded=True)
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’å®Ÿè¡Œ
        for type_, content in run_races_iter(s_year, s_month, s_day, s_place, target_races):
            if type_ == "status":
                status_container.write(content)
            elif type_ == "error":
                status_container.error(content)
            elif type_ == "result":
                r_num, text = content
                # ã€é‡è¦ã€‘Stateã«ä¿å­˜ã™ã‚‹
                st.session_state.results[r_num] = text
                status_container.write(f"âœ… {r_num}R å®Œäº†")
        
        status_container.update(label="âœ¨ å…¨å‡¦ç†å®Œäº†ï¼", state="complete", expanded=False)

    # --- çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ (Stateã‹ã‚‰èª­ã¿è¾¼ã‚€ãŸã‚ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚‚æ¶ˆãˆãªã„) ---
    st.divider()
    st.subheader(f"ğŸ“Š åˆ†æçµæœä¸€è¦§ ({len(st.session_state.results)}ãƒ¬ãƒ¼ã‚¹æ¸ˆ)")

    # 1Rã€œ12Rã¾ã§é †ç•ªã«è¡¨ç¤º
    # ã‚¿ãƒ–ã§åˆ†ã‘ã‚‹ã‹ã€Expanderã§åˆ†ã‘ã‚‹ã‹ã¯ãŠå¥½ã¿ã§ã€‚ã“ã“ã§ã¯Expanderæ¨å¥¨
    if not st.session_state.results:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„")
    else:
        # çµæœãŒã‚ã‚‹ã‚‚ã®ã ã‘ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
        sorted_keys = sorted(st.session_state.results.keys())
        for r in sorted_keys:
            txt = st.session_state.results[r]
            # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«Expanderã‚’ä½œæˆ
            with st.expander(f"ğŸ {r}R ã®äºˆæƒ³çµæœã‚’è¦‹ã‚‹", expanded=False):
                st.code(txt, language="text") # ã‚³ãƒ”ãƒ¼ã—ã‚„ã™ã„ã‚ˆã†ã«codeãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨

    # å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.sidebar.button("çµæœã‚¯ãƒªã‚¢"):
        st.session_state.results = {}
        st.rerun()

if __name__ == "__main__":
    main()
