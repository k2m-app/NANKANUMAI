import time
import json
import re
import requests
import streamlit as st

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ==================================================
# è¨­å®š
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# å…±é€šãƒ„ãƒ¼ãƒ«
# ==================================================
def get_http_session():
    sess = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])
    sess.mount("https://", HTTPAdapter(max_retries=retry))
    return sess

def build_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    return webdriver.Chrome(options=options)

def login_keibabook(driver, wait):
    driver.get("https://s.keibabook.co.jp/login/login")
    wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
    driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
    driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
    time.sleep(1)

# ==================================================
# â˜…Streamlitå´ã§å¯¾æˆ¦è¡¨ã‚’ä½œã‚‹é–¢æ•° (BeautifulSoupç‰ˆ)
# ==================================================
def generate_battle_table_local(llm_text, year, month, day, place_name, race_num):
    """
    Difyã‹ã‚‰è¿”ã£ã¦ããŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å¯¾æˆ¦è¡¨ã‚’ä½œæˆã—ã¦ãã£ã¤ã‘ã‚‹é–¢æ•°
    """
    
    # 1. å›ãƒ»æ—¥ç›®ã®è‡ªå‹•å–å¾—
    kai, nichi, error_msg = _get_kai_nichi(month, day, place_name)
    
    header_info = ""
    if error_msg:
        header_info = f"âš ï¸ é–‹å‚¬æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}\n"
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆURLç”Ÿæˆç”¨ï¼‰ã‚’è¨­å®šã—ã¦ç¶šè¡Œã‚’è©¦ã¿ã‚‹ã‹ã€ã“ã“ã§æ­¢ã‚ã‚‹ã‹
        # ã“ã“ã§ã¯ã¨ã‚Šã‚ãˆãšURLç”Ÿæˆã‚’è©¦ã¿ã‚‹
        if not kai: kai = 15
        if not nichi: nichi = 1
    else:
        header_info = f"ğŸ“… è‡ªå‹•åˆ¤å®š: {year}å¹´{month}æœˆ{day}æ—¥ {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›®\n"

    # 2. LLMãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©•ä¾¡(S,A...)ã‚’èª­ã¿å–ã‚‹
    grade_map = _parse_grades(llm_text)

    # 3. å—é–¢ã‚µã‚¤ãƒˆã‹ã‚‰å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ã‚’å–ã£ã¦ãã‚‹
    history_text = _fetch_history_data(year, month, day, place_name, kai, nichi, race_num, grade_map)

    # 4. å…¨éƒ¨åˆä½“ã•ã›ã¦è¿”ã™
    return f"{header_info}\n{llm_text}\n\n{history_text}"

# --- ä»¥ä¸‹ã€å¯¾æˆ¦è¡¨ä½œæˆã®ãŸã‚ã®è£æ–¹æ©Ÿèƒ½ ---

def _clean_text_strict(text):
    """æ”¹è¡Œã‚„ç©ºç™½ã‚’å®Œå…¨ã«é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹"""
    if not text: return ""
    # HTMLã‚¿ã‚°é™¤å»
    text = re.sub(r'<[^>]+>', ' ', text)
    # å®Ÿä½“å‚ç…§å¤‰æ›
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
    # æ”¹è¡Œãƒ»ã‚¿ãƒ–ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    text = re.sub(r'[\r\n\t]+', ' ', text)
    # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    return re.sub(r'\s+', ' ', text).strip()

def _get_kai_nichi(target_month, target_day, target_place):
    """
    å—é–¢ç«¶é¦¬ã®ç•ªçµ„è¡¨ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã€æ—¥ä»˜ã‹ã‚‰ã€Œå›ãƒ»æ—¥ç›®ã€ã‚’ç‰¹å®šã™ã‚‹
    """
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'cp932' # å—é–¢ã¯CP932(Shift_JIS)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        target_row = None
        # ã€Œå¤§äº•ç«¶é¦¬ã€ãªã©ã®æ–‡å­—ãŒå«ã¾ã‚Œã‚‹è¡Œã‚’æ¢ã™
        for tr in soup.find_all('tr'):
            text = tr.get_text()
            if target_place in text and "ç«¶é¦¬" in text:
                target_row = tr
                break
        
        if not target_row:
            return None, None, f"{target_place}ã®é–‹å‚¬æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

        # ãƒªãƒ³ã‚¯ã‚„ç”»åƒaltã®ä¸­ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’æ¢ã™
        info_text = ""
        link = target_row.find('a')
        if link:
            # ã‚¿ã‚°ã‚’å«ã‚ãŸç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ”¹è¡Œã‚’é™¤å»ã—ã¦å–å¾—
            info_text = _clean_text_strict(link.get_text())
        else:
            # ãƒªãƒ³ã‚¯ãŒãªã„å ´åˆï¼ˆé–‹å‚¬æœŸé–“å¤–ãªã©ï¼‰
            info_text = _clean_text_strict(target_row.get_text())

        # æ­£è¦è¡¨ç¾ã§ã€Œç¬¬15å› 1æœˆ 12, 13...ã€ã‚’è§£æ
        # \s* ã§æ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¨±å®¹
        match = re.search(r'ç¬¬(\d+)å›.*?(\d+)æœˆ\s*(.*?)æ—¥', info_text)
        if not match:
            return None, None, f"é–‹å‚¬ãƒ†ã‚­ã‚¹ãƒˆè§£æå¤±æ•—: {info_text}"

        kai_val = int(match.group(1))
        # æœˆã®ç¢ºèª
        mon_val = int(match.group(2))
        if int(target_month) != mon_val:
             return None, None, f"é–‹å‚¬æœˆä¸ä¸€è‡´(Web:{mon_val}æœˆ, æŒ‡å®š:{target_month}æœˆ)"

        # æ—¥ä»˜ãƒªã‚¹ãƒˆä½œæˆ "12, 13, 14" -> [12, 13, 14]
        days_str = match.group(3)
        days_clean = re.sub(r'[^\d,]', '', days_str.replace('ï¼Œ', ','))
        days_list = [int(d) for d in days_clean.split(',') if d]

        target_day_int = int(target_day)
        if target_day_int in days_list:
            nichi_val = days_list.index(target_day_int) + 1
            return kai_val, nichi_val, None
        else:
            return None, None, f"æŒ‡å®šæ—¥({target_day})ãŒæœŸé–“å¤–ã§ã™(é–‹å‚¬äºˆå®š:{days_list})"

    except Exception as e:
        return None, None, str(e)

def _parse_grades(text):
    """
    LLMã®å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é¦¬åã¨è©•ä¾¡(S,A...)ã‚’è¾æ›¸åŒ–ã™ã‚‹
    """
    grades = {}
    if not text: return grades
    
    for line in text.split('\n'):
        if '|' in line and '---' not in line:
            parts = [p.strip() for p in line.split('|')]
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ—æ•°ã«åˆã‚ã›ã¦èª¿æ•´
            if len(parts) >= 4:
                raw_name = parts[1]
                raw_grade = parts[-2] # æœ€å¾ŒãŒç©ºæ–‡å­—ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ -2 ã‚’æ¨å¥¨
                
                # â‘ é¦¬å(é¨æ‰‹) ã®å½¢å¼ã‹ã‚‰é¦¬åã®ã¿æŠ½å‡º
                match = re.search(r'[â‘ -â‘³]?\s*([^(\s]+)', raw_name)
                if match:
                    horse_name = match.group(1)
                    grade = raw_grade.strip()
                    if grade in ['S', 'A', 'B', 'C', 'D']:
                        grades[horse_name] = grade
    return grades

def _fetch_history_data(year, month, day, place_name, kai, nichi, race_num, grade_map):
    """
    BeautifulSoupã‚’ä½¿ã£ã¦å—é–¢ã®å¯¾æˆ¦è¡¨ã‚’æ­£ç¢ºã«å–å¾—ã™ã‚‹
    """
    place_codes = {'æµ¦å’Œ': '18', 'èˆ¹æ©‹': '19', 'å¤§äº•': '20', 'å·å´': '21'}
    p_code = place_codes.get(place_name, '20')
    
    # IDç”Ÿæˆ
    race_id = f"{year}{int(month):02}{int(day):02}{p_code}{int(kai):02}{int(nichi):02}{int(race_num):02}"
    url = f"https://www.nankankeiba.com/taisen/{race_id}.do"
    
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # 1. ãƒ†ãƒ¼ãƒ–ãƒ«ç‰¹å®š (æ–°ã—ã„ã‚¯ãƒ©ã‚¹åå„ªå…ˆ)
        target_table = soup.find('table', class_='nk23_c-table08__table')
        if not target_table:
            # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆresultãƒªãƒ³ã‚¯ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™ï¼‰
            for tbl in soup.find_all('table'):
                if tbl.find('a', href=re.compile(r'/result/\d+')):
                    target_table = tbl
                    break
        
        if not target_table:
            return f"\n(å¯¾æˆ¦è¡¨ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {url})"

        # 2. ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆéå»ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®æŠ½å‡ºï¼‰
        past_races = []
        thead = target_table.find('thead')
        if not thead: return "\n(ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚¨ãƒ©ãƒ¼: theadãªã—)"
        
        header_row = thead.find('tr')
        header_cells = header_row.find_all(['th', 'td'])
        
        for i, cell in enumerate(header_cells):
            link = cell.find('a', href=re.compile(r'/result/\d+'))
            if link:
                # è©³ç´°ãƒ†ã‚­ã‚¹ãƒˆå–å¾— (ã‚¯ãƒ©ã‚¹å nk23_c-table08__detail ãŒã‚ã‚Œã°ãã“ã‹ã‚‰)
                detail_tag = cell.find(class_='nk23_c-table08__detail')
                raw_info = detail_tag.get_text(strip=True) if detail_tag else cell.get_text(strip=True)
                
                # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤ã—ã¦æ•´å½¢
                info_text = raw_info.replace('ç«¶èµ°æˆç¸¾', '').replace('å¯¾æˆ¦è¡¨', '')
                info_text = _clean_text_strict(info_text)
                
                full_url = "https://www.nankankeiba.com" + link['href']
                
                past_races.append({
                    'info': info_text, 
                    'url': full_url, 
                    'results': [], 
                    'max_score': 0,
                    'grades': []
                })

        if not past_races:
            return "\n(éå»ã®å¯¾æˆ¦å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“)"

        # 3. ãƒ‡ãƒ¼ã‚¿è¡Œè§£æ
        tbody = target_table.find('tbody')
        data_rows = tbody.find_all('tr')
        
        rank_score = {'S': 5, 'A': 4, 'B': 3, 'C': 2, 'D': 1}
        
        for row in data_rows:
            # é¦¬åã‚»ãƒ«ã‚’æ¢ã™
            uma_link = row.find('a', href=re.compile(r'/uma_info/'))
            if not uma_link: continue
            
            horse_name = uma_link.get_text(strip=True)
            
            # è©•ä¾¡ã®ãƒãƒƒãƒãƒ³ã‚° (å®Œå…¨ä¸€è‡´å„ªå…ˆã€ãªã‘ã‚Œã°éƒ¨åˆ†ä¸€è‡´)
            grade = grade_map.get(horse_name)
            if not grade:
                for k, v in grade_map.items():
                    if k in horse_name or horse_name in k:
                        grade = v
                        break
            
            # è¡Œå†…ã®å…¨ã‚»ãƒ«ã‚’å–å¾—
            cells = row.find_all(['td', 'th'])
            
            # é¦¬åã‚»ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç‰¹å®š
            h_idx = -1
            for idx, c in enumerate(cells):
                if c.find('a', href=re.compile(r'/uma_info/')):
                    h_idx = idx
                    break
            
            if h_idx == -1: continue
            
            # çµæœã‚»ãƒ«ã¯é¦¬åã®æ¬¡ã‹ã‚‰
            result_cells = cells[h_idx+1:]
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã§å–å¾—ã—ãŸ past_races ã®ä¸¦ã³é †ã¨ç…§åˆ
            for col_idx, race_obj in enumerate(past_races):
                if col_idx < len(result_cells):
                    cell = result_cells[col_idx]
                    
                    # ç€é †æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ (BS4ãªã‚‰ã‚¯ãƒ©ã‚¹æŒ‡å®šã§æ­£ç¢ºã«å–ã‚Œã‚‹)
                    rank = ""
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: class="nk23_c-table08__number" å†…ã® span
                    num_tag = cell.find(class_='nk23_c-table08__number')
                    if num_tag:
                        span = num_tag.find('span')
                        if span:
                            rank = span.get_text(strip=True)
                        else:
                            # ãƒ‘ã‚¤ãƒ—ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆ
                            txt = num_tag.get_text(strip=True)
                            rank = txt.split('ï½œ')[0].strip()
                    else:
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚»ãƒ«ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆ
                        txt = cell.get_text(strip=True)
                        if txt:
                            first_part = txt.split('ï½œ')[0].split('|')[0].strip()
                            if first_part.isdigit() or first_part in ['é™¤å¤–', 'ä¸­æ­¢', 'å–æ¶ˆ']:
                                rank = first_part
                    
                    if rank:
                        mark = f"ã€{grade}ã€‘" if grade else ""
                        race_obj['results'].append(f"{rank}ç€ {mark}{horse_name}")
                        
                        if grade:
                            s = rank_score.get(grade, 0)
                            if s > race_obj['max_score']:
                                race_obj['max_score'] = s
                            race_obj['grades'].append(grade)

        # 4. ã‚½ãƒ¼ãƒˆã¨å‡ºåŠ›ç”Ÿæˆ
        # é‡è¦åº¦(max_score)ãŒé«˜ã„é † > ãã®è©•ä¾¡é¦¬ã®æ•°ãŒå¤šã„é †
        past_races.sort(key=lambda x: (x['max_score'], len(x['grades'])), reverse=True)
        
        output = ["### ğŸ“Š æ³¨ç›®å¯¾æˆ¦ (Streamlitè‡ªå‹•ç”Ÿæˆ)"]
        has_data = False
        
        for race in past_races:
            if race['results']:
                has_data = True
                # é‡è¦åº¦ã‚¢ã‚¤ã‚³ãƒ³
                icon = "ğŸ”¥" if race['max_score'] >= 5 else ("âœ¨" if race['max_score'] >= 4 else "ğŸ”¹")
                # å‹•ç”»ãƒªãƒ³ã‚¯å¤‰æ›
                liveon_url = race['url'].replace('result', 'liveon')
                
                output.append(f"**{icon} {race['info']}**")
                output.append(" / ".join(race['results']))
                output.append(f"[æ˜ åƒãƒ»è©³ç´°]({liveon_url})\n")
        
        if not has_data:
            return "\n(è©²å½“ã™ã‚‹å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ)"

        return "\n".join(output)

    except Exception as e:
        return f"\n(å¯¾æˆ¦è¡¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)})"

# ==================================================
# Difyé€£æº (å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ)
# ==================================================
def run_dify(inputs_dict):
    """
    Difyã«è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’é€ã£ã¦ã€äºˆæƒ³ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
    inputs_dict: {"text": "...", "date": "...", ...}
    """
    url = f"{DIFY_BASE_URL}/v1/workflows/run"
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "inputs": inputs_dict, # è¾æ›¸ã‚’ãã®ã¾ã¾æ¸¡ã™
        "response_mode": "blocking",
        "user": "streamlit-user"
    }
    
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if res.status_code == 200:
            data = res.json().get('data', {})
            outputs = data.get('outputs', {})
            
            # çµæœã‚’æ¢ã—ã¦è¿”ã™
            for v in outputs.values():
                if isinstance(v, str) and len(v) > 10:
                    return v
            return "âš ï¸ Difyã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ"
        else:
            # ã‚¨ãƒ©ãƒ¼æ™‚
            return f"âš ï¸ Dify Error: {res.status_code} {res.text}"
            
    except Exception as e:
        return f"âš ï¸ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿)
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(place_code, "åœ°æ–¹")
    
    # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•
    driver = build_driver()
    wait = WebDriverWait(driver, 10)
    
    try:
        # 1. ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ­ã‚°ã‚¤ãƒ³
        login_keibabook(driver, wait)
        
        # 2. ãƒ¬ãƒ¼ã‚¹IDå–å¾—
        # ã“ã“ã§ã¯æœ¬æ¥ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°(fetch_race_ids_from_schedule)ã‚’å‘¼ã³å‡ºã™
        # â€»å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦ã“ã®è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„
        from keiba_bot import fetch_race_ids_from_schedule, fetch_keibago_debatable_small, parse_race_info, parse_danwa_comments, parse_cyokyo
        race_ids = fetch_race_ids_from_schedule(driver, year, month, day, place_code, ui=ui)
        
        if not race_ids:
            yield 0, "âš ï¸ ãƒ¬ãƒ¼ã‚¹IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
            return

        baba_map = {"10": "20", "11": "21", "12": "19", "13": "18"}
        baba_code = baba_map.get(place_code, "20")

        for i, race_id in enumerate(race_ids):
            race_num = i + 1
            if target_races is not None and race_num not in target_races:
                continue
            
            # --- 3. é¦¬ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (çµ±åˆ) ---
            # keiba.go.jp
            header, keibago_dict, keibago_url, nar_race_level = fetch_keibago_debatable_small(
                year=str(year), month=str(month), day=str(day),
                race_no=race_num, baba_code=str(baba_code)
            )
            
            # è«‡è©±
            driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
            html_danwa = driver.page_source
            race_meta = parse_race_info(html_danwa)
            danwa_dict = parse_danwa_comments(html_danwa)
            
            # èª¿æ•™
            driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
            cyokyo_dict = parse_cyokyo(driver.page_source)
            
            # ãƒ‡ãƒ¼ã‚¿çµåˆ
            all_uma = sorted(
                set(danwa_dict.keys()) | set(cyokyo_dict.keys()) | set(keibago_dict.keys()),
                key=lambda x: int(x) if str(x).isdigit() else 999,
            )
            
            merged_text = []
            for uma in all_uma:
                kg = keibago_dict.get(uma, {})
                d = danwa_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                c = cyokyo_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                
                info = f"â–¼[é¦¬ç•ª{uma}] {kg.get('horse','')} é¨æ‰‹:{kg.get('jockey','')} èª¿æ•™å¸«:{kg.get('trainer','')}"
                if kg.get('is_change'): info += " ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘"
                
                merged_text.append(f"{info}\nè«‡è©±: {d}\nèª¿æ•™: {c}")
                
            prompt = (
                f"ãƒ¬ãƒ¼ã‚¹å: {race_meta.get('race_name','')}\n"
                f"æ¡ä»¶: {race_meta.get('cond','')}\n\n"
                + "\n".join(merged_text)
            )
            
            # --- 4. Difyå®Ÿè¡Œ (äºˆæƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ) ---
            # â˜…ä¿®æ­£: å…¨ã¦ã®å¤‰æ•°ã‚’è¾æ›¸ã§æ¸¡ã™
            dify_inputs = {
                "text": prompt,
                "date": f"{year}/{month}/{day}",
                "place": place_name,
                "race_no": str(race_num),
                "year": str(year),
                "month": str(month),
                "day": str(day)
            }
            
            dify_res = run_dify(dify_inputs)
            
            # --- 5. Streamlitå´ã§å¯¾æˆ¦è¡¨ã‚’ä½œæˆï¼†çµåˆ ---
            final_output = generate_battle_table_local(
                dify_res, year, month, day, place_name, race_num
            )
            
            yield race_num, final_output
            
            time.sleep(2) # è² è·è»½æ¸›

    finally:
        driver.quit()
