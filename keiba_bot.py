import time
import json
import re
import requests
import streamlit as st

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ==================================================
# è¨­å®š
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
DIFY_BASE_URL = st.secrets.get("DIFY_BASE_URL", "https://api.dify.ai")

# ==================================================
# å…±é€šãƒ„ãƒ¼ãƒ«
# ==================================================
def get_http_session():
    sess = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])
    sess.mount("https://", HTTPAdapter(max_retries=retry))
    return sess

def build_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    return webdriver.Chrome(options=options)

def login_keibabook(driver, wait):
    driver.get("https://s.keibabook.co.jp/login/login")
    wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
    driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
    driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
    time.sleep(1)

# ==================================================
# â˜…æ–°æ©Ÿèƒ½: Streamlitå´ã§å¯¾æˆ¦è¡¨ã‚’ä½œã‚‹é–¢æ•° (BeautifulSoupç‰ˆ)
# ==================================================
def generate_battle_table_local(llm_text, year, month, day, place_name, race_num):
    """
    Difyã‹ã‚‰è¿”ã£ã¦ããŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å¯¾æˆ¦è¡¨ã‚’ä½œæˆã—ã¦ãã£ã¤ã‘ã‚‹é–¢æ•°
    """
    
    # 1. å›ãƒ»æ—¥ç›®ã®è‡ªå‹•å–å¾—
    kai, nichi, error_msg = _get_kai_nichi(month, day, place_name)
    
    header_info = ""
    if error_msg:
        header_info = f"âš ï¸ é–‹å‚¬æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}\n"
    else:
        header_info = f"ğŸ“… è‡ªå‹•åˆ¤å®š: {year}å¹´{month}æœˆ{day}æ—¥ {place_name} ç¬¬{kai}å› {nichi}æ—¥ç›®\n"

    # 2. LLMãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©•ä¾¡(S,A...)ã‚’èª­ã¿å–ã‚‹
    grade_map = _parse_grades(llm_text)

    # 3. å—é–¢ã‚µã‚¤ãƒˆã‹ã‚‰å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ã‚’å–ã£ã¦ãã‚‹
    history_text = _fetch_history_data(year, month, day, place_name, kai, nichi, race_num, grade_map)

    # 4. å…¨éƒ¨åˆä½“ã•ã›ã¦è¿”ã™
    return f"{header_info}\n{llm_text}\n\n{history_text}"

# --- ä»¥ä¸‹ã€å¯¾æˆ¦è¡¨ä½œæˆã®ãŸã‚ã®è£æ–¹æ©Ÿèƒ½ ---

def _get_kai_nichi(target_month, target_day, target_place):
    """
    å—é–¢ç«¶é¦¬ã®ç•ªçµ„è¡¨ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã€æ—¥ä»˜ã‹ã‚‰ã€Œå›ãƒ»æ—¥ç›®ã€ã‚’ç‰¹å®šã™ã‚‹
    """
    url = "https://www.nankankeiba.com/bangumi_menu/bangumi.do"
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'cp932' # å—é–¢ã¯CP932(Shift_JIS)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        target_row = None
        # ã€Œå¤§äº•ç«¶é¦¬ã€ãªã©ã®æ–‡å­—ãŒå«ã¾ã‚Œã‚‹è¡Œã‚’æ¢ã™
        for tr in soup.find_all('tr'):
            text = tr.get_text()
            if target_place in text and "ç«¶é¦¬" in text:
                target_row = tr
                break
        
        if not target_row:
            return 15, 4, f"{target_place}ã®é–‹å‚¬æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ(ä»®å€¤ã§ç¶šè¡Œ)"

        # ãƒªãƒ³ã‚¯ã‚„ç”»åƒaltã®ä¸­ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’æ¢ã™
        info_text = ""
        link = target_row.find('a')
        if link:
            info_text = link.get_text(strip=True)
        
        # æ­£è¦è¡¨ç¾ã§ã€Œç¬¬15å› 1æœˆ 12, 13...ã€ã‚’è§£æ
        match = re.search(r'ç¬¬(\d+)å›.*?(\d+)æœˆ\s*(.*?)æ—¥', info_text)
        if not match:
            return 15, 4, f"é–‹å‚¬ãƒ†ã‚­ã‚¹ãƒˆè§£æå¤±æ•—: {info_text}"

        kai_val = int(match.group(1))
        # æ—¥ä»˜ãƒªã‚¹ãƒˆä½œæˆ "12, 13, 14" -> [12, 13, 14]
        days_str = match.group(3)
        days_clean = re.sub(r'[^\d,]', '', days_str.replace('ï¼Œ', ','))
        days_list = [int(d) for d in days_clean.split(',') if d]

        target_day_int = int(target_day)
        if target_day_int in days_list:
            nichi_val = days_list.index(target_day_int) + 1
            return kai_val, nichi_val, None
        else:
            return 15, 4, f"æŒ‡å®šæ—¥({target_day})ãŒæœŸé–“å¤–ã§ã™"

    except Exception as e:
        return 15, 4, str(e)

def _parse_grades(text):
    """
    LLMã®å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é¦¬åã¨è©•ä¾¡(S,A...)ã‚’è¾æ›¸åŒ–ã™ã‚‹
    """
    grades = {}
    if not text: return grades
    
    for line in text.split('\n'):
        if '|' in line and '---' not in line:
            parts = [p.strip() for p in line.split('|')]
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ—æ•°ã«åˆã‚ã›ã¦èª¿æ•´ï¼ˆé¦¬åãŒ2åˆ—ç›®ã€è©•ä¾¡ãŒæœ€å¾Œã‹ã‚‰2åˆ—ç›®ã¨ä»®å®šï¼‰
            if len(parts) >= 4:
                raw_name = parts[1]
                raw_grade = parts[-2] # æœ€å¾ŒãŒç©ºæ–‡å­—ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ -2 ã‚’æ¨å¥¨
                
                # â‘ é¦¬å(é¨æ‰‹) ã®å½¢å¼ã‹ã‚‰é¦¬åã®ã¿æŠ½å‡º
                match = re.search(r'[â‘ -â‘³]?\s*([^(\s]+)', raw_name)
                if match:
                    horse_name = match.group(1)
                    grade = raw_grade.strip()
                    if grade in ['S', 'A', 'B', 'C', 'D']:
                        grades[horse_name] = grade
    return grades

def _fetch_history_data(year, month, day, place_name, kai, nichi, race_num, grade_map):
    """
    BeautifulSoupã‚’ä½¿ã£ã¦å—é–¢ã®å¯¾æˆ¦è¡¨ã‚’æ­£ç¢ºã«å–å¾—ã™ã‚‹
    """
    place_codes = {'æµ¦å’Œ': '18', 'èˆ¹æ©‹': '19', 'å¤§äº•': '20', 'å·å´': '21'}
    p_code = place_codes.get(place_name, '20')
    
    # IDç”Ÿæˆ
    race_id = f"{year}{int(month):02}{int(day):02}{p_code}{int(kai):02}{int(nichi):02}{int(race_num):02}"
    url = f"https://www.nankankeiba.com/taisen/{race_id}.do"
    
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'cp932'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # 1. ãƒ†ãƒ¼ãƒ–ãƒ«ç‰¹å®š (æ–°ã—ã„ã‚¯ãƒ©ã‚¹åå„ªå…ˆ)
        target_table = soup.find('table', class_='nk23_c-table08__table')
        if not target_table:
            # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆresultãƒªãƒ³ã‚¯ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™ï¼‰
            for tbl in soup.find_all('table'):
                if tbl.find('a', href=re.compile(r'/result/\d+')):
                    target_table = tbl
                    break
        
        if not target_table:
            return f"\n(å¯¾æˆ¦è¡¨ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {url})"

        # 2. ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆéå»ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®æŠ½å‡ºï¼‰
        past_races = []
        thead = target_table.find('thead')
        if not thead: return "\n(ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚¨ãƒ©ãƒ¼: theadãªã—)"
        
        header_row = thead.find('tr')
        header_cells = header_row.find_all(['th', 'td'])
        
        for i, cell in enumerate(header_cells):
            link = cell.find('a', href=re.compile(r'/result/\d+'))
            if link:
                # è©³ç´°ãƒ†ã‚­ã‚¹ãƒˆå–å¾— (ã‚¯ãƒ©ã‚¹å nk23_c-table08__detail ãŒã‚ã‚Œã°ãã“ã‹ã‚‰)
                detail_tag = cell.find(class_='nk23_c-table08__detail')
                raw_info = detail_tag.get_text(strip=True) if detail_tag else cell.get_text(strip=True)
                
                # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤ã—ã¦æ•´å½¢
                info_text = raw_info.replace('ç«¶èµ°æˆç¸¾', '').replace('å¯¾æˆ¦è¡¨', '')
                info_text = re.sub(r'\s+', ' ', info_text).strip()
                
                full_url = "https://www.nankankeiba.com" + link['href']
                
                past_races.append({
                    'info': info_text, 
                    'url': full_url, 
                    'results': [], 
                    'max_score': 0,
                    'grades': []
                })

        if not past_races:
            return "\n(éå»ã®å¯¾æˆ¦å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“)"

        # 3. ãƒ‡ãƒ¼ã‚¿è¡Œè§£æ
        tbody = target_table.find('tbody')
        data_rows = tbody.find_all('tr')
        
        rank_score = {'S': 5, 'A': 4, 'B': 3, 'C': 2, 'D': 1}
        
        for row in data_rows:
            # é¦¬åã‚»ãƒ«ã‚’æ¢ã™
            uma_link = row.find('a', href=re.compile(r'/uma_info/'))
            if not uma_link: continue
            
            horse_name = uma_link.get_text(strip=True)
            
            # è©•ä¾¡ã®ãƒãƒƒãƒãƒ³ã‚° (å®Œå…¨ä¸€è‡´å„ªå…ˆã€ãªã‘ã‚Œã°éƒ¨åˆ†ä¸€è‡´)
            grade = grade_map.get(horse_name)
            if not grade:
                for k, v in grade_map.items():
                    if k in horse_name or horse_name in k:
                        grade = v
                        break
            
            # è¡Œå†…ã®å…¨ã‚»ãƒ«ã‚’å–å¾—
            cells = row.find_all(['td', 'th'])
            
            # é¦¬åã‚»ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç‰¹å®š
            h_idx = -1
            for idx, c in enumerate(cells):
                if c.find('a', href=re.compile(r'/uma_info/')):
                    h_idx = idx
                    break
            
            if h_idx == -1: continue
            
            # çµæœã‚»ãƒ«ã¯é¦¬åã®æ¬¡ã‹ã‚‰
            result_cells = cells[h_idx+1:]
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã§å–å¾—ã—ãŸ past_races ã®ä¸¦ã³é †ã¨ç…§åˆ
            for col_idx, race_obj in enumerate(past_races):
                if col_idx < len(result_cells):
                    cell = result_cells[col_idx]
                    
                    # ç€é †æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ (BS4ãªã‚‰ã‚¯ãƒ©ã‚¹æŒ‡å®šã§æ­£ç¢ºã«å–ã‚Œã‚‹)
                    rank = ""
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: class="nk23_c-table08__number" å†…ã® span
                    num_tag = cell.find(class_='nk23_c-table08__number')
                    if num_tag:
                        span = num_tag.find('span')
                        if span:
                            rank = span.get_text(strip=True)
                        else:
                            # ãƒ‘ã‚¤ãƒ—ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆ
                            txt = num_tag.get_text(strip=True)
                            rank = txt.split('ï½œ')[0].strip()
                    else:
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚»ãƒ«ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆ
                        txt = cell.get_text(strip=True)
                        if txt:
                            first_part = txt.split('ï½œ')[0].split('|')[0].strip()
                            if first_part.isdigit() or first_part in ['é™¤å¤–', 'ä¸­æ­¢', 'å–æ¶ˆ']:
                                rank = first_part
                    
                    if rank:
                        mark = f"ã€{grade}ã€‘" if grade else ""
                        race_obj['results'].append(f"{rank}ç€ {mark}{horse_name}")
                        
                        if grade:
                            s = rank_score.get(grade, 0)
                            if s > race_obj['max_score']:
                                race_obj['max_score'] = s
                            race_obj['grades'].append(grade)

        # 4. ã‚½ãƒ¼ãƒˆã¨å‡ºåŠ›ç”Ÿæˆ
        # é‡è¦åº¦(max_score)ãŒé«˜ã„é † > ãã®è©•ä¾¡é¦¬ã®æ•°ãŒå¤šã„é †
        past_races.sort(key=lambda x: (x['max_score'], len(x['grades'])), reverse=True)
        
        output = ["### ğŸ“Š æ³¨ç›®å¯¾æˆ¦ (Streamlitè‡ªå‹•ç”Ÿæˆ)"]
        has_data = False
        
        for race in past_races:
            if race['results']:
                has_data = True
                # é‡è¦åº¦ã‚¢ã‚¤ã‚³ãƒ³
                icon = "ğŸ”¥" if race['max_score'] >= 5 else ("âœ¨" if race['max_score'] >= 4 else "ğŸ”¹")
                # å‹•ç”»ãƒªãƒ³ã‚¯å¤‰æ›
                liveon_url = race['url'].replace('result', 'liveon')
                
                output.append(f"**{icon} {race['info']}**")
                output.append(" / ".join(race['results']))
                output.append(f"[æ˜ åƒãƒ»è©³ç´°]({liveon_url})\n")
        
        if not has_data:
            return "\n(è©²å½“ã™ã‚‹å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ)"

        return "\n".join(output)

    except Exception as e:
        return f"\n(å¯¾æˆ¦è¡¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)})"

# ==================================================
# Difyé€£æº (ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ)
# ==================================================
def run_dify(inputs):
    """
    Difyã«ãƒ†ã‚­ã‚¹ãƒˆã‚’é€ã£ã¦ã€äºˆæƒ³ã‚³ãƒ¡ãƒ³ãƒˆã ã‘ã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
    """
    url = f"{DIFY_BASE_URL}/v1/workflows/run"
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json"
    }
    # Difyå´ã¯ 'text' å¤‰æ•°ã‚’å—ã‘å–ã‚‹è¨­å®šã«ãªã£ã¦ã„ã‚‹æƒ³å®š
    payload = {
        "inputs": inputs,
        "response_mode": "blocking",
        "user": "streamlit-user"
    }
    
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if res.status_code == 200:
            data = res.json().get('data', {})
            outputs = data.get('outputs', {})
            
            # çµæœã‚’æ¢ã—ã¦è¿”ã™
            for v in outputs.values():
                if isinstance(v, str) and len(v) > 10:
                    return v
            return "âš ï¸ Difyã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ"
        else:
            # ã‚¨ãƒ©ãƒ¼æ™‚
            return f"âš ï¸ Dify Error: {res.status_code} {res.text}"
            
    except Exception as e:
        return f"âš ï¸ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿)
# ==================================================
def run_races_iter(year, month, day, place_code, target_races, ui=False):
    # (æ³¨æ„) ã“ã“ã§ã¯Scrapingéƒ¨åˆ†ã¯çœç•¥ã›ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç’°å¢ƒã«åˆã‚ã›ã¦
    #       fetch_race_ids_from_schedule ãªã©ã‚’å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    #       ä»¥ä¸‹ã®å®Ÿè£…ã¯ã€ŒDifyé€£æºã¨å¯¾æˆ¦è¡¨çµåˆã€ã®æµã‚Œã‚’ç¤ºã™ã‚‚ã®ã§ã™ã€‚
    
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(place_code, "åœ°æ–¹")
    
    # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•
    driver = build_driver()
    wait = WebDriverWait(driver, 10)
    
    try:
        # 1. ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ­ã‚°ã‚¤ãƒ³
        # _ui_info(ui, "ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...") 
        login_keibabook(driver, wait)
        
        # 2. ãƒ¬ãƒ¼ã‚¹IDå–å¾— (æ—¢å­˜ã®é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹æƒ³å®š)
        # race_ids = fetch_race_ids_from_schedule(driver, year, month, day, place_code, ui=ui)
        # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã«ãƒ€ãƒŸãƒ¼IDãƒªã‚¹ãƒˆã‚’ä½¿ã„ã¾ã™ãŒã€å®Ÿéš›ã¯ä¸Šã®è¡Œã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„
        # ---------------------------------------------------------------
        # â˜…â˜…â˜… ã“ã“ã«å…ƒã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯(fetch_race_ids...)ã‚’å…¥ã‚Œã¦ãã ã•ã„ â˜…â˜…â˜…
        # ---------------------------------------------------------------
        
        # ä»®: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸãƒ¬ãƒ¼ã‚¹ç•ªå·ã ã‘å›ã™ãƒ«ãƒ¼ãƒ—
        for race_num in sorted(list(target_races)):
            
            # 3. é¦¬ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (çœç•¥ãƒ»æ—¢å­˜ã‚³ãƒ¼ãƒ‰åˆ©ç”¨)
            # prompt = "..." 
            prompt = f"ï¼ˆ{place_name}{race_num}R ã®é¦¬ãƒ‡ãƒ¼ã‚¿ãŒã“ã“ã«å…¥ã‚Šã¾ã™ï¼‰" # ãƒ€ãƒŸãƒ¼
            
            # 4. Difyå®Ÿè¡Œ (äºˆæƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ)
            dify_res = run_dify({"text": prompt})
            
            # 5. â˜…Streamlitå´ã§å¯¾æˆ¦è¡¨ã‚’ä½œæˆï¼†çµåˆ (BeautifulSoupç‰ˆ)
            final_output = generate_battle_table_local(
                dify_res, year, month, day, place_name, race_num
            )
            
            yield race_num, final_output
            
            time.sleep(1)

    finally:
        driver.quit()
